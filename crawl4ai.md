# Crawl4AI Custom LLM Context
Generated on: 2025-06-29T01:18:09.958Z
Total files: 14

---

## Core Functionality - Memory
Source: crawl4ai_core_memory_content.llm.md

Okay, I have read the objective and instructions. I will now generate the detailed Markdown outline for a "Foundational Memory" document for the `core` component of `crawl4ai`, using the provided `code_analysis_output.md` (the concatenated code snippets from the previous prompt) as the primary source of truth for API details.

```markdown
# Detailed Outline for crawl4ai - core Component

**Target Document Type:** memory
**Target Output Filename Suggestion:** `llm_memory_core.md`
**Library Version Context:** 0.6.3
**Outline Generation Date:** 2025-05-24
---

## 1. Introduction to Core Components
    * 1.1. Purpose: Provides the foundational classes, configurations, and data models for web crawling and scraping operations within the `crawl4ai` library.
    * 1.2. Key Functionalities:
        *   Orchestration of asynchronous web crawling (`AsyncWebCrawler`).
        *   Configuration of browser behavior and specific crawl runs (`BrowserConfig`, `CrawlerRunConfig`).
        *   Standardized data structures for crawl results and associated data (`CrawlResult`, `Media`, `Links`, etc.).
        *   Strategies for fetching web content (`AsyncPlaywrightCrawlerStrategy`, `AsyncHTTPCrawlerStrategy`).
        *   Management of browser instances and sessions (`BrowserManager`, `ManagedBrowser`).
        *   Asynchronous logging (`AsyncLogger`).
    * 1.3. Relationship with other `crawl4ai` components:
        *   The `core` component serves as the foundation upon which specialized strategies (e.g., PDF processing, Markdown generation, content extraction, chunking, content filtering) are built and integrated.

## 2. Main Class: `AsyncWebCrawler`
    * 2.1. Purpose: The primary class for orchestrating asynchronous web crawling operations. It manages browser instances (via a `BrowserManager`), applies crawling strategies, and processes HTML content to produce structured results.
    * 2.2. Initialization (`__init__`)
        * 2.2.1. Signature:
            ```python
            def __init__(
                self,
                crawler_strategy: Optional[AsyncCrawlerStrategy] = None,
                config: Optional[BrowserConfig] = None,
                base_directory: str = str(os.getenv("CRAWL4_AI_BASE_DIRECTORY", Path.home())),
                thread_safe: bool = False,
                logger: Optional[AsyncLoggerBase] = None,
                **kwargs,
            ):
            ```
        * 2.2.2. Parameters:
            * `crawler_strategy (Optional[AsyncCrawlerStrategy])`: The strategy to use for fetching web content. If `None`, defaults to `AsyncPlaywrightCrawlerStrategy` initialized with `config` and `logger`.
            * `config (Optional[BrowserConfig])`: Configuration object for browser settings. If `None`, a default `BrowserConfig()` is created.
            * `base_directory (str)`: The base directory for storing crawl4ai related files, such as cache and logs. Defaults to `os.getenv("CRAWL4_AI_BASE_DIRECTORY", Path.home())`.
            * `thread_safe (bool)`: If `True`, uses an `asyncio.Lock` for thread-safe operations, particularly relevant for `arun_many`. Default: `False`.
            * `logger (Optional[AsyncLoggerBase])`: An instance of a logger. If `None`, a default `AsyncLogger` is initialized using `base_directory` and `config.verbose`.
            * `**kwargs`: Additional keyword arguments, primarily for backward compatibility, passed to the `AsyncPlaywrightCrawlerStrategy` if `crawler_strategy` is not provided.
    * 2.3. Key Public Attributes/Properties:
        * `browser_config (BrowserConfig)`: Read-only. The browser configuration object used by the crawler.
        * `crawler_strategy (AsyncCrawlerStrategy)`: Read-only. The active crawling strategy instance.
        * `logger (AsyncLoggerBase)`: Read-only. The logger instance used by the crawler.
        * `ready (bool)`: Read-only. `True` if the crawler has been started and is ready to perform crawl operations, `False` otherwise.
    * 2.4. Lifecycle Methods:
        * 2.4.1. `async start() -> AsyncWebCrawler`:
            * Purpose: Asynchronously initializes the crawler strategy (e.g., launches the browser). This must be called before `arun` or `arun_many` if the crawler is not used as an asynchronous context manager.
            * Returns: The `AsyncWebCrawler` instance (`self`).
        * 2.4.2. `async close() -> None`:
            * Purpose: Asynchronously closes the crawler strategy and cleans up resources (e.g., closes the browser). This should be called if `start()` was used explicitly.
        * 2.4.3. `async __aenter__() -> AsyncWebCrawler`:
            * Purpose: Entry point for asynchronous context management. Calls `self.start()`.
            * Returns: The `AsyncWebCrawler` instance (`self`).
        * 2.4.4. `async __aexit__(exc_type, exc_val, exc_tb) -> None`:
            * Purpose: Exit point for asynchronous context management. Calls `self.close()`.
    * 2.5. Primary Crawl Methods:
        * 2.5.1. `async arun(url: str, config: Optional[CrawlerRunConfig] = None, **kwargs) -> RunManyReturn`:
            * Purpose: Performs a single crawl operation for the given URL or raw HTML content.
            * Parameters:
                * `url (str)`: The URL to crawl (e.g., "http://example.com", "file:///path/to/file.html") or raw HTML content prefixed with "raw:" (e.g., "raw:<html>...</html>").
                * `config (Optional[CrawlerRunConfig])`: Configuration for this specific crawl run. If `None`, a default `CrawlerRunConfig()` is used.
                * `**kwargs`: Additional parameters passed to the underlying `aprocess_html` method, can be used to override settings in `config`.
            * Returns: `RunManyReturn` (which resolves to `CrawlResultContainer` containing a single `CrawlResult`).
        * 2.5.2. `async arun_many(urls: List[str], config: Optional[CrawlerRunConfig] = None, dispatcher: Optional[BaseDispatcher] = None, **kwargs) -> RunManyReturn`:
            * Purpose: Crawls multiple URLs concurrently using a specified or default dispatcher strategy.
            * Parameters:
                * `urls (List[str])`: A list of URLs to crawl.
                * `config (Optional[CrawlerRunConfig])`: Configuration applied to all crawl runs in this batch.
                * `dispatcher (Optional[BaseDispatcher])`: The dispatcher strategy to manage concurrent crawls. Defaults to `MemoryAdaptiveDispatcher`.
                * `**kwargs`: Additional parameters passed to the underlying `arun` method for each URL.
            * Returns: `RunManyReturn`. If `config.stream` is `True`, returns an `AsyncGenerator[CrawlResult, None]`. Otherwise, returns a `CrawlResultContainer` (list-like) of `CrawlResult` objects.
    * 2.6. Internal Processing Method (User-Facing Effects):
        * 2.6.1. `async aprocess_html(url: str, html: str, extracted_content: Optional[str], config: CrawlerRunConfig, screenshot_data: Optional[str], pdf_data: Optional[bytes], verbose: bool, **kwargs) -> CrawlResult`:
            * Purpose: Processes the fetched HTML content. This method is called internally by `arun` after content is fetched (either from a live crawl or cache). It applies scraping strategies, content filtering, and Markdown generation based on the `config`.
            * Parameters:
                * `url (str)`: The URL of the content being processed.
                * `html (str)`: The raw HTML content.
                * `extracted_content (Optional[str])`: Pre-extracted content from a previous step or cache.
                * `config (CrawlerRunConfig)`: Configuration for this processing run.
                * `screenshot_data (Optional[str])`: Base64 encoded screenshot data, if available.
                * `pdf_data (Optional[bytes])`: PDF data, if available.
                * `verbose (bool)`: Verbosity setting for logging during processing.
                * `**kwargs`: Additional parameters, including `is_raw_html` and `redirected_url`.
            * Returns: A `CrawlResult` object containing the processed data.

## 3. Core Configuration Objects

    * 3.1. Class `BrowserConfig` (from `crawl4ai.async_configs`)
        * 3.1.1. Purpose: Configures the browser instance launched by Playwright, including its type, mode, display settings, proxy, user agent, and persistent storage options.
        * 3.1.2. Initialization (`__init__`)
            * Signature:
            ```python
            def __init__(
                self,
                browser_type: str = "chromium",
                headless: bool = True,
                browser_mode: str = "dedicated",
                use_managed_browser: bool = False,
                cdp_url: Optional[str] = None,
                use_persistent_context: bool = False,
                user_data_dir: Optional[str] = None,
                channel: Optional[str] = "chromium", # Note: 'channel' from code, outline had 'chrome_channel'
                proxy: Optional[str] = None, # Note: 'proxy' from code, outline had 'proxy_config' for this level
                proxy_config: Optional[Union[ProxyConfig, dict, None]] = None,
                viewport_width: int = 1080,
                viewport_height: int = 600,
                viewport: Optional[dict] = None,
                accept_downloads: bool = False,
                downloads_path: Optional[str] = None,
                storage_state: Optional[Union[str, dict, None]] = None,
                ignore_https_errors: bool = True,
                java_script_enabled: bool = True,
                sleep_on_close: bool = False,
                verbose: bool = True,
                cookies: Optional[list] = None,
                headers: Optional[dict] = None,
                user_agent: str = "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/116.0.0.0 Safari/537.36",
                user_agent_mode: str = "",
                user_agent_generator_config: Optional[dict] = None, # Note: 'user_agent_generator_config' from code
                text_mode: bool = False,
                light_mode: bool = False,
                extra_args: Optional[list] = None,
                debugging_port: int = 9222,
                host: str = "localhost",
            ):
            ```
            * Key Parameters:
                * `browser_type (str)`: Type of browser to launch ("chromium", "firefox", "webkit"). Default: "chromium".
                * `headless (bool)`: Whether to run the browser in headless mode. Default: `True`.
                * `browser_mode (str)`: How the browser should be initialized ("builtin", "dedicated", "cdp", "docker"). Default: "dedicated".
                * `use_managed_browser (bool)`: Whether to launch the browser using a managed approach (e.g., via CDP). Default: `False`.
                * `cdp_url (Optional[str])`: URL for Chrome DevTools Protocol endpoint. Default: `None`.
                * `use_persistent_context (bool)`: Use a persistent browser context (profile). Default: `False`.
                * `user_data_dir (Optional[str])`: Path to user data directory for persistent sessions. Default: `None`.
                * `channel (Optional[str])`: Browser channel (e.g., "chromium", "chrome", "msedge"). Default: "chromium".
                * `proxy (Optional[str])`: Simple proxy server URL string.
                * `proxy_config (Optional[Union[ProxyConfig, dict, None]])`: Detailed proxy configuration object or dictionary. Takes precedence over `proxy`.
                * `viewport_width (int)`: Default viewport width. Default: `1080`.
                * `viewport_height (int)`: Default viewport height. Default: `600`.
                * `viewport (Optional[dict])`: Dictionary to set viewport dimensions, overrides `viewport_width` and `viewport_height` if set (e.g., `{"width": 1920, "height": 1080}`). Default: `None`.
                * `accept_downloads (bool)`: Whether to allow file downloads. Default: `False`.
                * `downloads_path (Optional[str])`: Directory to store downloaded files. Default: `None`.
                * `storage_state (Optional[Union[str, dict, None]])`: Path to a file or a dictionary containing browser storage state (cookies, localStorage). Default: `None`.
                * `ignore_https_errors (bool)`: Ignore HTTPS certificate errors. Default: `True`.
                * `java_script_enabled (bool)`: Enable JavaScript execution. Default: `True`.
                * `user_agent (str)`: Custom User-Agent string. Default: "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/116.0.0.0 Safari/537.36".
                * `user_agent_mode (str)`: Mode for generating User-Agent (e.g., "random"). Default: `""` (uses provided `user_agent`).
                * `user_agent_generator_config (Optional[dict])`: Configuration for User-Agent generation if `user_agent_mode` is active. Default: `{}`.
                * `text_mode (bool)`: If `True`, disables images and rich content for faster loading. Default: `False`.
                * `light_mode (bool)`: Disables certain background features for performance. Default: `False`.
                * `extra_args (Optional[list])`: Additional command-line arguments for the browser. Default: `None` (resolves to `[]`).
                * `debugging_port (int)`: Port for browser debugging protocol. Default: `9222`.
                * `host (str)`: Host for browser debugging protocol. Default: "localhost".
        * 3.1.3. Key Public Methods:
            * `clone(**kwargs) -> BrowserConfig`: Creates a new `BrowserConfig` instance as a copy of the current one, with specified keyword arguments overriding existing values.
            * `to_dict() -> dict`: Returns a dictionary representation of the configuration object's attributes.
            * `dump() -> dict`: Serializes the configuration object to a JSON-serializable dictionary, including nested objects.
            * `static load(data: dict) -> BrowserConfig`: Deserializes a `BrowserConfig` instance from a dictionary (previously created by `dump`).
            * `static from_kwargs(kwargs: dict) -> BrowserConfig`: Creates a `BrowserConfig` instance directly from a dictionary of keyword arguments.

    * 3.2. Class `CrawlerRunConfig` (from `crawl4ai.async_configs`)
        * 3.2.1. Purpose: Specifies settings for an individual crawl operation initiated by `arun()` or `arun_many()`. These settings can override or augment the global `BrowserConfig`.
        * 3.2.2. Initialization (`__init__`)
            * Signature:
            ```python
            def __init__(
                self,
                # Content Processing Parameters
                word_count_threshold: int = MIN_WORD_THRESHOLD,
                extraction_strategy: Optional[ExtractionStrategy] = None,
                chunking_strategy: ChunkingStrategy = RegexChunking(),
                markdown_generator: MarkdownGenerationStrategy = DefaultMarkdownGenerator(),
                only_text: bool = False,
                css_selector: Optional[str] = None,
                target_elements: Optional[List[str]] = None,
                excluded_tags: Optional[list] = None,
                excluded_selector: Optional[str] = None,
                keep_data_attributes: bool = False,
                keep_attrs: Optional[list] = None,
                remove_forms: bool = False,
                prettify: bool = False,
                parser_type: str = "lxml",
                scraping_strategy: ContentScrapingStrategy = None, # Will default to WebScrapingStrategy
                proxy_config: Optional[Union[ProxyConfig, dict, None]] = None,
                proxy_rotation_strategy: Optional[ProxyRotationStrategy] = None,
                # Browser Location and Identity Parameters
                locale: Optional[str] = None,
                timezone_id: Optional[str] = None,
                geolocation: Optional[GeolocationConfig] = None,
                # SSL Parameters
                fetch_ssl_certificate: bool = False,
                # Caching Parameters
                cache_mode: CacheMode = CacheMode.BYPASS,
                session_id: Optional[str] = None,
                bypass_cache: bool = False, # Legacy
                disable_cache: bool = False, # Legacy
                no_cache_read: bool = False, # Legacy
                no_cache_write: bool = False, # Legacy
                shared_data: Optional[dict] = None,
                # Page Navigation and Timing Parameters
                wait_until: str = "domcontentloaded",
                page_timeout: int = PAGE_TIMEOUT,
                wait_for: Optional[str] = None,
                wait_for_timeout: Optional[int] = None,
                wait_for_images: bool = False,
                delay_before_return_html: float = 0.1,
                mean_delay: float = 0.1,
                max_range: float = 0.3,
                semaphore_count: int = 5,
                # Page Interaction Parameters
                js_code: Optional[Union[str, List[str]]] = None,
                js_only: bool = False,
                ignore_body_visibility: bool = True,
                scan_full_page: bool = False,
                scroll_delay: float = 0.2,
                process_iframes: bool = False,
                remove_overlay_elements: bool = False,
                simulate_user: bool = False,
                override_navigator: bool = False,
                magic: bool = False,
                adjust_viewport_to_content: bool = False,
                # Media Handling Parameters
                screenshot: bool = False,
                screenshot_wait_for: Optional[float] = None,
                screenshot_height_threshold: int = SCREENSHOT_HEIGHT_THRESHOLD,
                pdf: bool = False,
                capture_mhtml: bool = False,
                image_description_min_word_threshold: int = IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD,
                image_score_threshold: int = IMAGE_SCORE_THRESHOLD,
                table_score_threshold: int = 7,
                exclude_external_images: bool = False,
                exclude_all_images: bool = False,
                # Link and Domain Handling Parameters
                exclude_social_media_domains: Optional[list] = None, # Note: 'exclude_social_media_domains' from code
                exclude_external_links: bool = False,
                exclude_social_media_links: bool = False,
                exclude_domains: Optional[list] = None,
                exclude_internal_links: bool = False,
                # Debugging and Logging Parameters
                verbose: bool = True,
                log_console: bool = False,
                # Network and Console Capturing Parameters
                capture_network_requests: bool = False,
                capture_console_messages: bool = False,
                # Connection Parameters (for HTTPCrawlerStrategy)
                method: str = "GET",
                stream: bool = False,
                url: Optional[str] = None,
                # Robots.txt Handling
                check_robots_txt: bool = False,
                # User Agent Parameters
                user_agent: Optional[str] = None,
                user_agent_mode: Optional[str] = None,
                user_agent_generator_config: Optional[dict] = None, # Note: 'user_agent_generator_config' from code
                # Deep Crawl Parameters
                deep_crawl_strategy: Optional[DeepCrawlStrategy] = None,
                # Experimental Parameters
                experimental: Optional[Dict[str, Any]] = None,
            ):
            ```
            * Key Parameters:
                * `word_count_threshold (int)`: Minimum word count for a content block to be considered. Default: `MIN_WORD_THRESHOLD` (200).
                * `extraction_strategy (Optional[ExtractionStrategy])`: Strategy for structured data extraction (e.g., `LLMExtractionStrategy`, `JsonCssExtractionStrategy`). Default: `None` (falls back to `NoExtractionStrategy`).
                * `chunking_strategy (ChunkingStrategy)`: Strategy for splitting content into chunks before extraction. Default: `RegexChunking()`.
                * `markdown_generator (MarkdownGenerationStrategy)`: Strategy for converting HTML to Markdown. Default: `DefaultMarkdownGenerator()`.
                * `cache_mode (CacheMode)`: Caching behavior for this run. Default: `CacheMode.BYPASS`.
                * `session_id (Optional[str])`: ID for session persistence (reusing browser tabs/contexts). Default: `None`.
                * `js_code (Optional[Union[str, List[str]]])`: JavaScript code snippets to execute on the page. Default: `None`.
                * `wait_for (Optional[str])`: CSS selector or JS condition (prefixed with "js:") to wait for before proceeding. Default: `None`.
                * `page_timeout (int)`: Timeout for page operations (e.g., navigation) in milliseconds. Default: `PAGE_TIMEOUT` (60000ms).
                * `screenshot (bool)`: If `True`, capture a screenshot of the page. Default: `False`.
                * `pdf (bool)`: If `True`, generate a PDF of the page. Default: `False`.
                * `capture_mhtml (bool)`: If `True`, capture an MHTML snapshot of the page. Default: `False`.
                * `exclude_external_links (bool)`: If `True`, exclude external links from results. Default: `False`.
                * `stream (bool)`: If `True` (used with `arun_many`), results are yielded as an `AsyncGenerator`. Default: `False`.
                * `check_robots_txt (bool)`: If `True`, crawler will check and respect `robots.txt` rules. Default: `False`.
                * `user_agent (Optional[str])`: Override the browser's User-Agent for this specific run.
        * 3.2.3. Key Public Methods:
            * `clone(**kwargs) -> CrawlerRunConfig`: Creates a new `CrawlerRunConfig` instance as a copy of the current one, with specified keyword arguments overriding existing values.
            * `to_dict() -> dict`: Returns a dictionary representation of the configuration object's attributes.
            * `dump() -> dict`: Serializes the configuration object to a JSON-serializable dictionary, including nested objects.
            * `static load(data: dict) -> CrawlerRunConfig`: Deserializes a `CrawlerRunConfig` instance from a dictionary (previously created by `dump`).
            * `static from_kwargs(kwargs: dict) -> CrawlerRunConfig`: Creates a `CrawlerRunConfig` instance directly from a dictionary of keyword arguments.

    * 3.3. Supporting Configuration Objects (from `crawl4ai.async_configs`)
        * 3.3.1. Class `GeolocationConfig`
            * Purpose: Defines geolocation (latitude, longitude, accuracy) to be emulated by the browser.
            * Initialization (`__init__`):
                ```python
                def __init__(
                    self,
                    latitude: float,
                    longitude: float,
                    accuracy: Optional[float] = 0.0
                ):
                ```
            * Parameters:
                * `latitude (float)`: Latitude coordinate (e.g., 37.7749).
                * `longitude (float)`: Longitude coordinate (e.g., -122.4194).
                * `accuracy (Optional[float])`: Accuracy in meters. Default: `0.0`.
            * Methods:
                * `static from_dict(geo_dict: Dict) -> GeolocationConfig`: Creates an instance from a dictionary.
                * `to_dict() -> Dict`: Converts the instance to a dictionary.
                * `clone(**kwargs) -> GeolocationConfig`: Creates a copy with updated values.
        * 3.3.2. Class `ProxyConfig`
            * Purpose: Defines the settings for a single proxy server, including server address, authentication credentials, and optional IP.
            * Initialization (`__init__`):
                ```python
                def __init__(
                    self,
                    server: str,
                    username: Optional[str] = None,
                    password: Optional[str] = None,
                    ip: Optional[str] = None,
                ):
                ```
            * Parameters:
                * `server (str)`: Proxy server URL (e.g., "http://127.0.0.1:8080", "socks5://user:pass@host:port").
                * `username (Optional[str])`: Username for proxy authentication.
                * `password (Optional[str])`: Password for proxy authentication.
                * `ip (Optional[str])`: Optional IP address associated with the proxy for verification.
            * Methods:
                * `static from_string(proxy_str: str) -> ProxyConfig`: Creates an instance from a string (e.g., "ip:port:username:password" or "ip:port").
                * `static from_dict(proxy_dict: Dict) -> ProxyConfig`: Creates an instance from a dictionary.
                * `static from_env(env_var: str = "PROXIES") -> List[ProxyConfig]`: Loads a list of proxies from a comma-separated environment variable.
                * `to_dict() -> Dict`: Converts the instance to a dictionary.
                * `clone(**kwargs) -> ProxyConfig`: Creates a copy with updated values.
        * 3.3.3. Class `HTTPCrawlerConfig`
            * Purpose: Configuration for the `AsyncHTTPCrawlerStrategy`, specifying HTTP method, headers, data/JSON payload, and redirect/SSL verification behavior.
            * Initialization (`__init__`):
                ```python
                def __init__(
                    self,
                    method: str = "GET",
                    headers: Optional[Dict[str, str]] = None,
                    data: Optional[Dict[str, Any]] = None,
                    json: Optional[Dict[str, Any]] = None,
                    follow_redirects: bool = True,
                    verify_ssl: bool = True,
                ):
                ```
            * Parameters:
                * `method (str)`: HTTP method (e.g., "GET", "POST"). Default: "GET".
                * `headers (Optional[Dict[str, str]])`: Dictionary of HTTP request headers. Default: `None`.
                * `data (Optional[Dict[str, Any]])`: Dictionary of form data to send in the request body. Default: `None`.
                * `json (Optional[Dict[str, Any]])`: JSON data to send in the request body. Default: `None`.
                * `follow_redirects (bool)`: Whether to automatically follow HTTP redirects. Default: `True`.
                * `verify_ssl (bool)`: Whether to verify SSL certificates. Default: `True`.
            * Methods:
                * `static from_kwargs(kwargs: dict) -> HTTPCrawlerConfig`: Creates an instance from keyword arguments.
                * `to_dict() -> dict`: Converts config to a dictionary.
                * `clone(**kwargs) -> HTTPCrawlerConfig`: Creates a copy with updated values.
                * `dump() -> dict`: Serializes the config to a dictionary.
                * `static load(data: dict) -> HTTPCrawlerConfig`: Deserializes from a dictionary.
        * 3.3.4. Class `LLMConfig`
            * Purpose: Configures settings for interacting with Large Language Models, including provider choice, API credentials, and generation parameters.
            * Initialization (`__init__`):
                ```python
                def __init__(
                    self,
                    provider: str = DEFAULT_PROVIDER,
                    api_token: Optional[str] = None,
                    base_url: Optional[str] = None,
                    temperature: Optional[float] = None,
                    max_tokens: Optional[int] = None,
                    top_p: Optional[float] = None,
                    frequency_penalty: Optional[float] = None,
                    presence_penalty: Optional[float] = None,
                    stop: Optional[List[str]] = None,
                    n: Optional[int] = None,
                ):
                ```
            * Key Parameters:
                * `provider (str)`: Name of the LLM provider (e.g., "openai/gpt-4o", "ollama/llama3.3", "groq/llama3-8b-8192"). Default: `DEFAULT_PROVIDER` (from `crawl4ai.config`).
                * `api_token (Optional[str])`: API token for the LLM provider. If prefixed with "env:", it reads from the specified environment variable (e.g., "env:OPENAI_API_KEY"). If not provided, it attempts to load from default environment variables based on the provider.
                * `base_url (Optional[str])`: Custom base URL for the LLM API endpoint.
                * `temperature (Optional[float])`: Sampling temperature for generation.
                * `max_tokens (Optional[int])`: Maximum number of tokens to generate.
                * `top_p (Optional[float])`: Nucleus sampling parameter.
                * `frequency_penalty (Optional[float])`: Penalty for token frequency.
                * `presence_penalty (Optional[float])`: Penalty for token presence.
                * `stop (Optional[List[str]])`: List of stop sequences for generation.
                * `n (Optional[int])`: Number of completions to generate.
            * Methods:
                * `static from_kwargs(kwargs: dict) -> LLMConfig`: Creates an instance from keyword arguments.
                * `to_dict() -> dict`: Converts config to a dictionary.
                * `clone(**kwargs) -> LLMConfig`: Creates a copy with updated values.

## 4. Core Data Models (Results & Payloads from `crawl4ai.models`)

    * 4.1. Class `CrawlResult(BaseModel)`
        * Purpose: A Pydantic model representing the comprehensive result of a single crawl and processing operation.
        * Key Fields:
            * `url (str)`: The final URL that was crawled (after any redirects).
            * `html (str)`: The raw HTML content fetched from the URL.
            * `success (bool)`: `True` if the crawl operation (fetching and initial processing) was successful, `False` otherwise.
            * `cleaned_html (Optional[str])`: HTML content after sanitization and removal of unwanted tags/attributes as per configuration. Default: `None`.
            * `_markdown (Optional[MarkdownGenerationResult])`: (Private Attribute) Holds the `MarkdownGenerationResult` object if Markdown generation was performed. Use the `markdown` property to access. Default: `None`.
            * `markdown (Optional[Union[str, MarkdownGenerationResult]])`: (Property) Provides access to Markdown content. Behaves as a string (raw markdown) by default but allows access to `MarkdownGenerationResult` attributes (e.g., `result.markdown.fit_markdown`).
            * `extracted_content (Optional[str])`: JSON string representation of structured data extracted by an `ExtractionStrategy`. Default: `None`.
            * `media (Media)`: An object containing lists of `MediaItem` for images, videos, audio, and extracted tables. Default: `Media()`.
            * `links (Links)`: An object containing lists of `Link` for internal and external hyperlinks found on the page. Default: `Links()`.
            * `downloaded_files (Optional[List[str]])`: A list of file paths if any files were downloaded during the crawl. Default: `None`.
            * `js_execution_result (Optional[Dict[str, Any]])`: The result of any JavaScript code executed on the page. Default: `None`.
            * `screenshot (Optional[str])`: Base64 encoded string of the page screenshot, if `screenshot=True` was set. Default: `None`.
            * `pdf (Optional[bytes])`: Raw bytes of the PDF generated from the page, if `pdf=True` was set. Default: `None`.
            * `mhtml (Optional[str])`: MHTML snapshot of the page, if `capture_mhtml=True` was set. Default: `None`.
            * `metadata (Optional[dict])`: Dictionary of metadata extracted from the page (e.g., title, description, OpenGraph tags, Twitter card data). Default: `None`.
            * `error_message (Optional[str])`: A message describing the error if `success` is `False`. Default: `None`.
            * `session_id (Optional[str])`: The session ID used for this crawl, if applicable. Default: `None`.
            * `response_headers (Optional[dict])`: HTTP response headers from the server. Default: `None`.
            * `status_code (Optional[int])`: HTTP status code of the response. Default: `None`.
            * `ssl_certificate (Optional[SSLCertificate])`: Information about the SSL certificate if `fetch_ssl_certificate=True`. Default: `None`.
            * `dispatch_result (Optional[DispatchResult])`: Metadata about the task execution from the dispatcher (e.g., timings, memory usage). Default: `None`.
            * `redirected_url (Optional[str])`: The original URL if the request was redirected. Default: `None`.
            * `network_requests (Optional[List[Dict[str, Any]]])`: List of captured network requests if `capture_network_requests=True`. Default: `None`.
            * `console_messages (Optional[List[Dict[str, Any]]])`: List of captured browser console messages if `capture_console_messages=True`. Default: `None`.
        * Methods:
            * `model_dump(*args, **kwargs)`: Serializes the `CrawlResult` model to a dictionary, ensuring the `_markdown` private attribute is correctly handled and included as "markdown" in the output if present.

    * 4.2. Class `MarkdownGenerationResult(BaseModel)`
        * Purpose: A Pydantic model that holds various forms of Markdown generated from HTML content.
        * Fields:
            * `raw_markdown (str)`: The basic, direct conversion of HTML to Markdown.
            * `markdown_with_citations (str)`: Markdown content with inline citations (e.g., [^1^]) and a references section.
            * `references_markdown (str)`: The Markdown content for the "References" section, listing all cited links.
            * `fit_markdown (Optional[str])`: Markdown generated specifically from content deemed "relevant" by a content filter (like `PruningContentFilter` or `LLMContentFilter`), if such a filter was applied. Default: `None`.
            * `fit_html (Optional[str])`: The filtered HTML content that was used to generate `fit_markdown`. Default: `None`.
        * Methods:
            * `__str__(self) -> str`: Returns `self.raw_markdown` when the object is cast to a string.

    * 4.3. Class `ScrapingResult(BaseModel)`
        * Purpose: A Pydantic model representing a standardized output from content scraping strategies.
        * Fields:
            * `cleaned_html (str)`: The primary sanitized and processed HTML content.
            * `success (bool)`: Indicates if the scraping operation was successful.
            * `media (Media)`: A `Media` object containing extracted images, videos, audio, and tables.
            * `links (Links)`: A `Links` object containing extracted internal and external links.
            * `metadata (Dict[str, Any])`: A dictionary of metadata extracted from the page (e.g., title, description).

    * 4.4. Class `MediaItem(BaseModel)`
        * Purpose: A Pydantic model representing a generic media item like an image, video, or audio file.
        * Fields:
            * `src (Optional[str])`: The source URL of the media item. Default: `""`.
            * `data (Optional[str])`: Base64 encoded data for inline media. Default: `""`.
            * `alt (Optional[str])`: Alternative text for the media item (e.g., image alt text). Default: `""`.
            * `desc (Optional[str])`: A description or surrounding text related to the media item. Default: `""`.
            * `score (Optional[int])`: A relevance or importance score, if calculated by a strategy. Default: `0`.
            * `type (str)`: The type of media (e.g., "image", "video", "audio"). Default: "image".
            * `group_id (Optional[int])`: An identifier to group related media variants (e.g., different resolutions of the same image from a srcset). Default: `0`.
            * `format (Optional[str])`: The detected file format (e.g., "jpeg", "png", "mp4"). Default: `None`.
            * `width (Optional[int])`: The width of the media item in pixels, if available. Default: `None`.

    * 4.5. Class `Link(BaseModel)`
        * Purpose: A Pydantic model representing an extracted hyperlink.
        * Fields:
            * `href (Optional[str])`: The URL (href attribute) of the link. Default: `""`.
            * `text (Optional[str])`: The anchor text of the link. Default: `""`.
            * `title (Optional[str])`: The title attribute of the link, if present. Default: `""`.
            * `base_domain (Optional[str])`: The base domain extracted from the `href`. Default: `""`.

    * 4.6. Class `Media(BaseModel)`
        * Purpose: A Pydantic model that acts as a container for lists of different types of media items found on a page.
        * Fields:
            * `images (List[MediaItem])`: A list of `MediaItem` objects representing images. Default: `[]`.
            * `videos (List[MediaItem])`: A list of `MediaItem` objects representing videos. Default: `[]`.
            * `audios (List[MediaItem])`: A list of `MediaItem` objects representing audio files. Default: `[]`.
            * `tables (List[Dict])`: A list of dictionaries, where each dictionary represents an extracted HTML table with keys like "headers", "rows", "caption", "summary". Default: `[]`.

    * 4.7. Class `Links(BaseModel)`
        * Purpose: A Pydantic model that acts as a container for lists of internal and external links.
        * Fields:
            * `internal (List[Link])`: A list of `Link` objects considered internal to the crawled site. Default: `[]`.
            * `external (List[Link])`: A list of `Link` objects pointing to external sites. Default: `[]`.

    * 4.8. Class `AsyncCrawlResponse(BaseModel)`
        * Purpose: A Pydantic model representing the raw response from a crawler strategy's `crawl` method. This data is then processed further to create a `CrawlResult`.
        * Fields:
            * `html (str)`: The raw HTML content of the page.
            * `response_headers (Dict[str, str])`: A dictionary of HTTP response headers.
            * `js_execution_result (Optional[Dict[str, Any]])`: The result from any JavaScript code executed on the page. Default: `None`.
            * `status_code (int)`: The HTTP status code of the response.
            * `screenshot (Optional[str])`: Base64 encoded screenshot data, if captured. Default: `None`.
            * `pdf_data (Optional[bytes])`: Raw PDF data, if captured. Default: `None`.
            * `mhtml_data (Optional[str])`: MHTML snapshot data, if captured. Default: `None`.
            * `downloaded_files (Optional[List[str]])`: A list of local file paths for any files downloaded during the crawl. Default: `None`.
            * `ssl_certificate (Optional[SSLCertificate])`: SSL certificate information for the site. Default: `None`.
            * `redirected_url (Optional[str])`: The original URL requested if the final URL is a result of redirection. Default: `None`.
            * `network_requests (Optional[List[Dict[str, Any]]])`: Captured network requests if enabled. Default: `None`.
            * `console_messages (Optional[List[Dict[str, Any]]])`: Captured console messages if enabled. Default: `None`.

    * 4.9. Class `TokenUsage(BaseModel)`
        * Purpose: A Pydantic model to track token usage statistics for interactions with Large Language Models.
        * Fields:
            * `completion_tokens (int)`: Number of tokens used for the LLM's completion/response. Default: `0`.
            * `prompt_tokens (int)`: Number of tokens used for the input prompt to the LLM. Default: `0`.
            * `total_tokens (int)`: Total number of tokens used (prompt + completion). Default: `0`.
            * `completion_tokens_details (Optional[dict])`: Provider-specific detailed breakdown of completion tokens. Default: `None`.
            * `prompt_tokens_details (Optional[dict])`: Provider-specific detailed breakdown of prompt tokens. Default: `None`.

    * 4.10. Class `SSLCertificate(dict)` (from `crawl4ai.ssl_certificate`)
        * Purpose: Represents an SSL certificate's information, behaving like a dictionary for direct JSON serialization and easy access to its fields.
        * Key Fields (accessed as dictionary keys):
            * `subject (dict)`: Dictionary of subject fields (e.g., `{"CN": "example.com", "O": "Example Inc."}`).
            * `issuer (dict)`: Dictionary of issuer fields.
            * `version (int)`: Certificate version number.
            * `serial_number (str)`: Certificate serial number (hexadecimal string).
            * `not_before (str)`: Validity start date and time (ASN.1/UTC format string, e.g., "YYYYMMDDHHMMSSZ").
            * `not_after (str)`: Validity end date and time (ASN.1/UTC format string).
            * `fingerprint (str)`: SHA-256 fingerprint of the certificate (lowercase hex string).
            * `signature_algorithm (str)`: The algorithm used to sign the certificate (e.g., "sha256WithRSAEncryption").
            * `raw_cert (str)`: Base64 encoded string of the raw DER-encoded certificate.
            * `extensions (List[dict])`: A list of dictionaries, each representing a certificate extension with "name" and "value" keys.
        * Static Methods:
            * `from_url(url: str, timeout: int = 10) -> Optional[SSLCertificate]`: Fetches the SSL certificate from the given URL and returns an `SSLCertificate` instance, or `None` on failure.
        * Instance Methods:
            * `to_json(filepath: Optional[str] = None) -> Optional[str]`: Exports the certificate information as a JSON string. If `filepath` is provided, writes to the file and returns `None`.
            * `to_pem(filepath: Optional[str] = None) -> Optional[str]`: Exports the certificate in PEM format as a string. If `filepath` is provided, writes to the file and returns `None`.
            * `to_der(filepath: Optional[str] = None) -> Optional[bytes]`: Exports the raw certificate in DER format as bytes. If `filepath` is provided, writes to the file and returns `None`.
        * Example:
            ```python
            # Assuming 'cert' is an SSLCertificate instance
            # print(cert["subject"]["CN"])
            # cert.to_pem("my_cert.pem")
            ```

    * 4.11. Class `DispatchResult(BaseModel)`
        * Purpose: Contains metadata about a task's execution when processed by a dispatcher (e.g., in `arun_many`).
        * Fields:
            * `task_id (str)`: A unique identifier for the dispatched task.
            * `memory_usage (float)`: Memory usage (in MB) recorded during the task's execution.
            * `peak_memory (float)`: Peak memory usage (in MB) recorded during the task's execution.
            * `start_time (Union[datetime, float])`: The start time of the task (can be a `datetime` object or a Unix timestamp float).
            * `end_time (Union[datetime, float])`: The end time of the task.
            * `error_message (str)`: Any error message if the task failed during dispatch or execution. Default: `""`.

    * 4.12. `CrawlResultContainer(Generic[CrawlResultT])`
        * Purpose: A generic container for `CrawlResult` objects, primarily used as the return type for `arun_many` when `stream=False`. It behaves like a list, allowing iteration, indexing, and length checking.
        * Methods:
            * `__iter__(self)`: Allows iteration over the contained `CrawlResult` objects.
            * `__getitem__(self, index)`: Allows accessing `CrawlResult` objects by index.
            * `__len__(self)`: Returns the number of `CrawlResult` objects contained.
            * `__repr__(self)`: Provides a string representation of the container.
        * Attribute:
            * `_results (List[CrawlResultT])`: The internal list holding the `CrawlResult` objects.

    * 4.13. `RunManyReturn` (Type Alias from `crawl4ai.models`)
        * Purpose: A type alias defining the possible return types for the `arun_many` method of `AsyncWebCrawler`.
        * Definition: `Union[CrawlResultContainer[CrawlResult], AsyncGenerator[CrawlResult, None]]`
            * This means `arun_many` will return a `CrawlResultContainer` (a list-like object of all `CrawlResult` instances) if `CrawlerRunConfig.stream` is `False` (the default).
            * It will return an `AsyncGenerator` yielding individual `CrawlResult` instances if `CrawlerRunConfig.stream` is `True`.

## 5. Core Crawler Strategies (from `crawl4ai.async_crawler_strategy`)

    * 5.1. Abstract Base Class `AsyncCrawlerStrategy(ABC)`
        * Purpose: Defines the common interface that all asynchronous crawler strategies must implement. This allows `AsyncWebCrawler` to use different fetching mechanisms (e.g., Playwright, HTTP requests) interchangeably.
        * Initialization (`__init__`):
            ```python
            def __init__(self, browser_config: BrowserConfig, logger: AsyncLoggerBase):
            ```
            * Parameters:
                * `browser_config (BrowserConfig)`: The browser configuration to be used by the strategy.
                * `logger (AsyncLoggerBase)`: The logger instance for logging strategy-specific events.
        * Key Abstract Methods (must be implemented by concrete subclasses):
            * `async crawl(self, url: str, config: CrawlerRunConfig) -> AsyncCrawlResponse`:
                * Purpose: Fetches the content from the given URL according to the `config`.
                * Returns: An `AsyncCrawlResponse` object containing the raw fetched data.
            * `async __aenter__(self)`:
                * Purpose: Asynchronous context manager entry, typically for initializing resources (e.g., launching a browser).
            * `async __aexit__(self, exc_type, exc_val, exc_tb)`:
                * Purpose: Asynchronous context manager exit, for cleaning up resources.
        * Key Concrete Methods (available to all strategies):
            * `set_custom_headers(self, headers: dict) -> None`:
                * Purpose: Sets custom HTTP headers to be used by the strategy for subsequent requests.
            * `update_user_agent(self, user_agent: str) -> None`:
                * Purpose: Updates the User-Agent string used by the strategy.
            * `set_hook(self, hook_name: str, callback: Callable) -> None`:
                * Purpose: Registers a callback function for a specific hook point in the crawling lifecycle.
            * `async_run_hook(self, hook_name: str, *args, **kwargs) -> Any`:
                * Purpose: Executes a registered hook with the given arguments.
            * `async_get_default_context(self) -> BrowserContext`:
                * Purpose: Retrieves the default browser context (Playwright specific, might raise `NotImplementedError` in non-Playwright strategies).
            * `async_create_new_page(self, context: BrowserContext) -> Page`:
                * Purpose: Creates a new page within a given browser context (Playwright specific).
            * `async_get_page(self, url: str, config: CrawlerRunConfig, session_id: Optional[str]) -> Tuple[Page, BrowserContext]`:
                * Purpose: Gets an existing page/context for a session or creates a new one (Playwright specific, managed by `BrowserManager`).
            * `async_close_page(self, page: Page, session_id: Optional[str]) -> None`:
                * Purpose: Closes a page, potentially keeping the associated context/session alive (Playwright specific).
            * `async_kill_session(self, session_id: str) -> None`:
                * Purpose: Kills (closes) a specific browser session, including its page and context (Playwright specific).

    * 5.2. Class `AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy)`
        * Purpose: The default crawler strategy, using Playwright to control a web browser for fetching and interacting with web pages. It supports complex JavaScript execution and provides hooks for various stages of the crawl.
        * Initialization (`__init__`):
            ```python
            def __init__(
                self,
                browser_config: Optional[BrowserConfig] = None,
                logger: Optional[AsyncLoggerBase] = None,
                browser_manager: Optional[BrowserManager] = None
            ):
            ```
            * Parameters:
                * `browser_config (Optional[BrowserConfig])`: Browser configuration. Defaults to a new `BrowserConfig()` if not provided.
                * `logger (Optional[AsyncLoggerBase])`: Logger instance. Defaults to a new `AsyncLogger()`.
                * `browser_manager (Optional[BrowserManager])`: An instance of `BrowserManager` to manage browser lifecycles and contexts. If `None`, a new `BrowserManager` is created internally.
        * Key Overridden/Implemented Methods:
            * `async crawl(self, url: str, config: CrawlerRunConfig) -> AsyncCrawlResponse`:
                * Purpose: Implements the crawling logic using Playwright. It navigates to the URL, executes JavaScript if specified, waits for conditions, captures screenshots/PDFs if requested, and returns the page content and other metadata.
            * `async aprocess_html(self, url: str, html: str, config: CrawlerRunConfig, **kwargs) -> CrawlResult`:
                * Purpose: (Note: While `AsyncWebCrawler` calls this, the default implementation is in `AsyncPlaywrightCrawlerStrategy` for convenience, acting as a bridge to the scraping strategy.) Processes the fetched HTML to produce a `CrawlResult`. This involves using the `scraping_strategy` from the `config` (defaults to `WebScrapingStrategy`) to clean HTML, extract media/links, and then uses the `markdown_generator` to produce Markdown.
        * Specific Public Methods:
            * `async_create_new_context(self, config: Optional[CrawlerRunConfig] = None) -> BrowserContext`:
                * Purpose: Creates a new Playwright `BrowserContext` based on the global `BrowserConfig` and optional overrides from `CrawlerRunConfig`.
            * `async_setup_context_default(self, context: BrowserContext, config: Optional[CrawlerRunConfig] = None) -> None`:
                * Purpose: Applies default settings to a `BrowserContext`, such as viewport size, user agent, custom headers, locale, timezone, and geolocation, based on `BrowserConfig` and `CrawlerRunConfig`.
            * `async_setup_context_hooks(self, context: BrowserContext, config: CrawlerRunConfig) -> None`:
                * Purpose: Sets up event listeners on the context for capturing network requests and console messages if `config.capture_network_requests` or `config.capture_console_messages` is `True`.
            * `async_handle_storage_state(self, context: BrowserContext, config: CrawlerRunConfig) -> None`:
                * Purpose: Loads cookies and localStorage from a `storage_state` file or dictionary (specified in `BrowserConfig` or `CrawlerRunConfig`) into the given `BrowserContext`.
        * Hooks (Callable via `set_hook(hook_name, callback)` and executed by `async_run_hook`):
            * `on_browser_created`: Called after the Playwright browser instance is launched or connected. Callback receives `(browser, **kwargs)`.
            * `on_page_context_created`: Called after a new Playwright `BrowserContext` and `Page` are created. Callback receives `(page, context, **kwargs)`.
            * `before_goto`: Called just before `page.goto(url)` is executed. Callback receives `(page, context, url, **kwargs)`.
            * `after_goto`: Called after `page.goto(url)` completes successfully. Callback receives `(page, context, url, response, **kwargs)`.
            * `on_user_agent_updated`: Called when the User-Agent string is updated for a context. Callback receives `(page, context, user_agent, **kwargs)`.
            * `on_execution_started`: Called when `js_code` execution begins on a page. Callback receives `(page, context, **kwargs)`.
            * `before_retrieve_html`: Called just before the final HTML content is retrieved from the page. Callback receives `(page, context, **kwargs)`.
            * `before_return_html`: Called just before the `AsyncCrawlResponse` is returned by the `crawl()` method of the strategy. Callback receives `(page, context, html_content, **kwargs)`.

    * 5.3. Class `AsyncHTTPCrawlerStrategy(AsyncCrawlerStrategy)`
        * Purpose: A lightweight crawler strategy that uses direct HTTP requests (via `httpx`) instead of a full browser. Suitable for static sites or when JavaScript execution is not needed.
        * Initialization (`__init__`):
            ```python
            def __init__(self, http_config: Optional[HTTPCrawlerConfig] = None, logger: Optional[AsyncLoggerBase] = None):
            ```
            * Parameters:
                * `http_config (Optional[HTTPCrawlerConfig])`: Configuration for HTTP requests (method, headers, data, etc.). Defaults to a new `HTTPCrawlerConfig()`.
                * `logger (Optional[AsyncLoggerBase])`: Logger instance. Defaults to a new `AsyncLogger()`.
        * Key Overridden/Implemented Methods:
            * `async crawl(self, url: str, http_config: Optional[HTTPCrawlerConfig] = None, **kwargs) -> AsyncCrawlResponse`:
                * Purpose: Fetches content from the URL using an HTTP GET or POST request via `httpx`. Does not execute JavaScript. Returns an `AsyncCrawlResponse` with HTML, status code, and headers. Screenshot, PDF, and MHTML capabilities are not available with this strategy.

## 6. Browser Management (from `crawl4ai.browser_manager`)

    * 6.1. Class `BrowserManager`
        * Purpose: Manages the lifecycle of Playwright browser instances and their contexts. It handles launching/connecting to browsers, creating new contexts with specific configurations, managing sessions for page reuse, and cleaning up resources.
        * Initialization (`__init__`):
            ```python
            def __init__(self, browser_config: BrowserConfig, logger: Optional[AsyncLoggerBase] = None):
            ```
            * Parameters:
                * `browser_config (BrowserConfig)`: The global browser configuration settings.
                * `logger (Optional[AsyncLoggerBase])`: Logger instance for browser management events.
        * Key Methods:
            * `async start() -> None`: Initializes the Playwright instance and launches or connects to the browser based on `browser_config` (e.g., launches a new browser instance or connects to an existing CDP endpoint via `ManagedBrowser`).
            * `async create_browser_context(self, crawlerRunConfig: Optional[CrawlerRunConfig] = None) -> playwright.async_api.BrowserContext`: Creates a new browser context. If `crawlerRunConfig` is provided, its settings (e.g., locale, viewport, proxy) can override the global `BrowserConfig`.
            * `async setup_context(self, context: playwright.async_api.BrowserContext, crawlerRunConfig: Optional[CrawlerRunConfig] = None, is_default: bool = False) -> None`: Applies various settings to a given browser context, including headers, cookies, viewport, geolocation, permissions, and storage state, based on `BrowserConfig` and `CrawlerRunConfig`.
            * `async get_page(self, crawlerRunConfig: CrawlerRunConfig) -> Tuple[playwright.async_api.Page, playwright.async_api.BrowserContext]`: Retrieves an existing page and context for a given `session_id` (if present in `crawlerRunConfig` and the session is active) or creates a new page and context. Manages context reuse based on a signature derived from `CrawlerRunConfig` to ensure contexts with different core settings (like proxy, locale) are isolated.
            * `async kill_session(self, session_id: str) -> None`: Closes the page and browser context associated with the given `session_id`, effectively ending that session.
            * `async close() -> None`: Closes all managed browser contexts and the main browser instance.

    * 6.2. Class `ManagedBrowser`
        * Purpose: Manages the lifecycle of a single, potentially persistent, browser process. It's used when `BrowserConfig.use_managed_browser` is `True` or `BrowserConfig.use_persistent_context` is `True`. It handles launching the browser with a specific user data directory and connecting via CDP.
        * Initialization (`__init__`):
            ```python
            def __init__(
                self,
                browser_type: str = "chromium",
                user_data_dir: Optional[str] = None,
                headless: bool = False,
                logger=None,
                host: str = "localhost",
                debugging_port: int = 9222,
                cdp_url: Optional[str] = None, # Added as per code_analysis
                browser_config: Optional[BrowserConfig] = None # Added as per code_analysis
            ):
            ```
            * Parameters:
                * `browser_type (str)`: "chromium", "firefox", or "webkit". Default: "chromium".
                * `user_data_dir (Optional[str])`: Path to the user data directory for the browser profile. If `None`, a temporary directory might be created.
                * `headless (bool)`: Whether to launch the browser in headless mode. Default: `False` (typically for managed/persistent scenarios).
                * `logger`: Logger instance.
                * `host (str)`: Host for the debugging port. Default: "localhost".
                * `debugging_port (int)`: Port for the Chrome DevTools Protocol. Default: `9222`.
                * `cdp_url (Optional[str])`: If provided, attempts to connect to an existing browser at this CDP URL instead of launching a new one.
                * `browser_config (Optional[BrowserConfig])`: The `BrowserConfig` object providing overall browser settings.
        * Key Methods:
            * `async start() -> str`: Starts the browser process (if not connecting to an existing `cdp_url`). If a new browser is launched, it uses the specified `user_data_dir` and `debugging_port`.
            * Returns: The CDP endpoint URL (e.g., "http://localhost:9222").
            * `async cleanup() -> None`: Terminates the browser process (if launched by this instance) and removes any temporary user data directory created by it.
        * Static Methods:
            * `async create_profile(cls, browser_config: Optional[BrowserConfig] = None, profile_name: Optional[str] = None, logger=None) -> str`:
                * Purpose: Launches a browser instance with a new or existing user profile, allowing interactive setup (e.g., manual login, cookie acceptance). The browser remains open until the user closes it.
                * Parameters:
                    * `browser_config (Optional[BrowserConfig])`: Optional browser configuration to use.
                    * `profile_name (Optional[str])`: Name for the profile. If `None`, a default name is used.
                    * `logger`: Logger instance.
                * Returns: The path to the created/used user data directory, which can then be passed to `BrowserConfig.user_data_dir`.
            * `list_profiles(cls) -> List[str]`:
                * Purpose: Lists the names of all browser profiles stored in the default Crawl4AI profiles directory (`~/.crawl4ai/profiles`).
                * Returns: A list of profile name strings.
            * `delete_profile(cls, profile_name_or_path: str) -> bool`:
                * Purpose: Deletes a browser profile either by its name (if in the default directory) or by its full path.
                * Returns: `True` if deletion was successful, `False` otherwise.

    * 6.3. Function `clone_runtime_state(src: BrowserContext, dst: BrowserContext, crawlerRunConfig: Optional[CrawlerRunConfig] = None, browserConfig: Optional[BrowserConfig] = None) -> None`
        * Purpose: Asynchronously copies runtime state (cookies, localStorage, session storage) from a source `BrowserContext` to a destination `BrowserContext`. Can also apply headers and geolocation from `CrawlerRunConfig` or `BrowserConfig` to the destination context.
        * Parameters:
            * `src (BrowserContext)`: The source browser context.
            * `dst (BrowserContext)`: The destination browser context.
            * `crawlerRunConfig (Optional[CrawlerRunConfig])`: Optional run configuration to apply to `dst`.
            * `browserConfig (Optional[BrowserConfig])`: Optional browser configuration to apply to `dst`.

## 7. Proxy Rotation Strategies (from `crawl4ai.proxy_strategy`)

    * 7.1. Abstract Base Class `ProxyRotationStrategy(ABC)`
        * Purpose: Defines the interface for strategies that provide a sequence of proxy configurations, enabling proxy rotation.
        * Abstract Methods:
            * `async get_next_proxy(self) -> Optional[ProxyConfig]`:
                * Purpose: Asynchronously retrieves the next `ProxyConfig` from the strategy.
                * Returns: A `ProxyConfig` object or `None` if no more proxies are available or an error occurs.
            * `add_proxies(self, proxies: List[ProxyConfig]) -> None`:
                * Purpose: Adds a list of `ProxyConfig` objects to the strategy's pool of proxies.

    * 7.2. Class `RoundRobinProxyStrategy(ProxyRotationStrategy)`
        * Purpose: A simple proxy rotation strategy that cycles through a list of provided proxies in a round-robin fashion.
        * Initialization (`__init__`):
            ```python
            def __init__(self, proxies: Optional[List[ProxyConfig]] = None):
            ```
            * Parameters:
                * `proxies (Optional[List[ProxyConfig]])`: An initial list of `ProxyConfig` objects. If `None`, the list is empty and proxies must be added via `add_proxies`.
        * Methods:
            * `add_proxies(self, proxies: List[ProxyConfig]) -> None`: Adds new `ProxyConfig` objects to the internal list of proxies and reinitializes the cycle.
            * `async get_next_proxy(self) -> Optional[ProxyConfig]`: Returns the next `ProxyConfig` from the list, cycling back to the beginning when the end is reached. Returns `None` if the list is empty.

## 8. Logging (from `crawl4ai.async_logger`)

    * 8.1. Abstract Base Class `AsyncLoggerBase(ABC)`
        * Purpose: Defines the basic interface for an asynchronous logger. Concrete implementations should provide methods for logging messages at different levels.
    * 8.2. Class `AsyncLogger(AsyncLoggerBase)`
        * Purpose: The default asynchronous logger for `crawl4ai`. It provides structured logging to both the console and optionally to a file, with customizable icons, colors, and verbosity levels.
        * Initialization (`__init__`):
            ```python
            def __init__(
                self,
                log_file: Optional[str] = None,
                verbose: bool = True,
                tag_width: int = 15, # outline had 10, code has 15
                icons: Optional[Dict[str, str]] = None,
                colors: Optional[Dict[LogLevel, LogColor]] = None, # Corrected type annotation
                log_level: LogLevel = LogLevel.INFO # Assuming LogLevel.INFO is a typical default
            ):
            ```
            * Parameters:
                * `log_file (Optional[str])`: Path to a file where logs should be written. If `None`, logs only to console.
                * `verbose (bool)`: If `True`, enables more detailed logging (DEBUG level). Default: `True`.
                * `tag_width (int)`: Width for the tag part of the log message. Default: `15`.
                * `icons (Optional[Dict[str, str]])`: Custom icons for different log tags.
                * `colors (Optional[Dict[LogLevel, LogColor]])`: Custom colors for different log levels.
                * `log_level (LogLevel)`: Minimum log level to output.
        * Key Methods (for logging):
            * `info(self, message: str, tag: Optional[str] = None, **params) -> None`: Logs an informational message.
            * `warning(self, message: str, tag: Optional[str] = None, **params) -> None`: Logs a warning message.
            * `error(self, message: str, tag: Optional[str] = None, **params) -> None`: Logs an error message.
            * `debug(self, message: str, tag: Optional[str] = None, **params) -> None`: Logs a debug message (only if `verbose=True` or `log_level` is DEBUG).
            * `url_status(self, url: str, success: bool, timing: float, tag: str = "FETCH", **params) -> None`: Logs the status of a URL fetch operation, including success/failure and timing.
            * `error_status(self, url: str, error: str, tag: str = "ERROR", **params) -> None`: Logs an error encountered for a specific URL.

## 9. Core Utility Functions (from `crawl4ai.async_configs`)
    * 9.1. `to_serializable_dict(obj: Any, ignore_default_value: bool = False) -> Dict`
        * Purpose: Recursively converts a Python object (often a Pydantic model or a dataclass instance used for configuration) into a dictionary that is safe for JSON serialization. It handles nested objects, enums, and basic types.
        * Parameters:
            * `obj (Any)`: The object to be serialized.
            * `ignore_default_value (bool)`: If `True`, fields whose current value is the same as their default value (if applicable, e.g., for Pydantic models) might be omitted from the resulting dictionary. Default: `False`.
        * Returns: `Dict` - A JSON-serializable dictionary representation of the object.
    * 9.2. `from_serializable_dict(data: Any) -> Any`
        * Purpose: Recursively reconstructs Python objects from a dictionary representation (typically one created by `to_serializable_dict`). It attempts to instantiate classes based on a "type" key in the dictionary if present.
        * Parameters:
            * `data (Any)`: The dictionary (or basic type) to be deserialized.
        * Returns: `Any` - The reconstructed Python object or the original data if no special deserialization rule applies.
    * 9.3. `is_empty_value(value: Any) -> bool`
        * Purpose: Checks if a given value is considered "empty" (e.g., `None`, an empty string, an empty list, an empty dictionary).
        * Returns: `bool` - `True` if the value is empty, `False` otherwise.

## 10. Enumerations (Key Enums used in Core)
    * 10.1. `CacheMode` (from `crawl4ai.cache_context`, defined in `crawl4ai.async_configs` as per provided code)
        * Purpose: Defines the caching behavior for crawl operations.
        * Members:
            * `ENABLE`: (Value: "enable") Normal caching behavior; read from cache if available, write to cache after fetching.
            * `DISABLE`: (Value: "disable") No caching at all; always fetch fresh content and do not write to cache.
            * `READ_ONLY`: (Value: "read_only") Only read from the cache; do not write new or updated content to the cache.
            * `WRITE_ONLY`: (Value: "write_only") Only write to the cache after fetching; do not read from the cache.
            * `BYPASS`: (Value: "bypass") Skip the cache entirely for this specific operation; fetch fresh content and do not write to cache. This is often the default for individual `CrawlerRunConfig` instances.
    * 10.2. `DisplayMode` (from `crawl4ai.models`, used by `CrawlerMonitor`)
        * Purpose: Defines the display mode for the `CrawlerMonitor`.
        * Members:
            * `DETAILED`: Shows detailed information for each task.
            * `AGGREGATED`: Shows summary statistics and overall progress.
    * 10.3. `CrawlStatus` (from `crawl4ai.models`, used by `CrawlStats`)
        * Purpose: Represents the status of a crawl task.
        * Members:
            * `QUEUED`: Task is waiting to be processed.
            * `IN_PROGRESS`: Task is currently being processed.
            * `COMPLETED`: Task finished successfully.
            * `FAILED`: Task failed.

## 11. Versioning
    * 11.1. Accessing Library Version:
        * The current version of the `crawl4ai` library can be accessed programmatically via the `__version__` attribute of the top-level `crawl4ai` package.
        * Example:
            ```python
            from crawl4ai import __version__ as crawl4ai_version
            print(f"Crawl4AI Version: {crawl4ai_version}")
            # Expected output based on provided code: Crawl4AI Version: 0.6.3
            ```

## 12. Basic Usage Examples

    * 12.1. Minimal Crawl:
        ```python
        import asyncio
        from crawl4ai import AsyncWebCrawler

        async def main():
            async with AsyncWebCrawler() as crawler:
                result = await crawler.arun(url="http://example.com")
                if result.success:
                    print("Markdown (first 300 chars):")
                    print(result.markdown.raw_markdown[:300]) # Accessing raw_markdown
                else:
                    print(f"Error: {result.error_message}")

        if __name__ == "__main__":
            asyncio.run(main())
        ```

    * 12.2. Crawl with Basic Configuration:
        ```python
        import asyncio
        from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

        async def main():
            browser_cfg = BrowserConfig(headless=True, browser_type="firefox")
            run_cfg = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                word_count_threshold=50
            )
            async with AsyncWebCrawler(config=browser_cfg) as crawler:
                result = await crawler.arun(url="http://example.com", config=run_cfg)
                if result.success:
                    print(f"Status Code: {result.status_code}")
                    print(f"Cleaned HTML length: {len(result.cleaned_html)}")
                else:
                    print(f"Error: {result.error_message}")
        
        if __name__ == "__main__":
            asyncio.run(main())
        ```

    * 12.3. Accessing Links and Images from Result:
        ```python
        import asyncio
        from crawl4ai import AsyncWebCrawler

        async def main():
            async with AsyncWebCrawler() as crawler:
                result = await crawler.arun(url="http://example.com")
                if result.success:
                    print(f"Found {len(result.links.internal)} internal links.")
                    if result.links.internal:
                        print(f"First internal link: {result.links.internal[0].href}")
                    
                    print(f"Found {len(result.media.images)} images.")
                    if result.media.images:
                        print(f"First image src: {result.media.images[0].src}")
                else:
                    print(f"Error: {result.error_message}")

        if __name__ == "__main__":
            asyncio.run(main())
        ```
```

---


## Core Functionality - Reasoning
Source: crawl4ai_core_reasoning_content.llm.md

Okay, I have read and understood the objective and the detailed outline provided for the `core` component. I will now generate the "Reasoning & Problem-Solving Framework" document in Markdown format, strictly adhering to the outline and incorporating the principles for a "reasoning" document.

I will use the code snippets and documentation details you provided in the previous turn as the basis for this document.

```markdown
# Detailed Outline for crawl4ai - core Component

**Target Document Type:** reasoning
**Target Output Filename Suggestion:** `llm_reasoning_core.md`
**Library Version Context:** 0.6.3
**Outline Generation Date:** 2024-05-24
---

# I. Introduction to the Crawl4ai Core

The `core` component of Crawl4ai is the engine that powers all web crawling and scraping activities. Understanding its architecture and how its main pieces fit together is crucial for leveraging the full potential of the library. This section provides a high-level overview to set the stage.

*   A. **Purpose of the Core Component:**
    *   **Explaining why the `core` is central to `crawl4ai`'s functionality:**
        The `core` component serves as the central nervous system of Crawl4ai. It's responsible for orchestrating the entire lifecycle of a web crawl, from launching and managing browser instances to fetching web content, processing it through various strategies (scraping, markdown generation, data extraction), and finally delivering structured results. Without the `core`, other specialized components like PDF processors or advanced extraction strategies would lack the fundamental web interaction capabilities.
    *   **Core problems the `core` component aims to solve for users:**
        *   **Simplified Browser Automation:** Abstracts away the complexities of browser automation libraries like Playwright, providing a cleaner and more focused API for web crawling.
        *   **Flexible Configuration:** Offers granular control over how browsers are launched (`BrowserConfig`) and how individual crawl operations are executed (`CrawlerRunConfig`), allowing users to tailor crawls to specific site behaviors and data needs.
        *   **Unified Result Processing:** Provides a consistent `CrawlResult` object, regardless of whether the content was fetched via a full browser or a simple HTTP request, making it easier to build downstream processing pipelines.
        *   **Efficient Resource Management:** Includes mechanisms for managing browser contexts, sessions, and caching to optimize performance and resource utilization, especially for larger crawls.
        *   **Extensibility:** Designed with a strategy pattern, allowing users to plug in custom behaviors for crawling, scraping, and content processing.

*   B. **Key Abstractions and Design Philosophy:**
    *   **Brief overview of the main classes (`AsyncWebCrawler`, `BrowserConfig`, `CrawlerRunConfig`) and their roles:**
        *   `AsyncWebCrawler`: This is your primary interaction point. You instantiate it, (optionally) configure it with a `BrowserConfig`, and then use its methods (`arun`, `arun_many`) to perform crawls, passing in `CrawlerRunConfig` objects for per-crawl specifics.
        *   `BrowserConfig`: Defines *how the browser itself is set up*. This is typically a one-time configuration for a `AsyncWebCrawler` instance, covering aspects like which browser engine to use, whether to run headless, proxy settings, user agent strings, and persistent user profiles. Think of it as setting up your physical web browser application.
        *   `CrawlerRunConfig`: Defines *how a specific URL or set of URLs should be crawled and processed*. This is passed to `arun` or `arun_many` and can override some browser-level settings. It controls aspects like caching, JavaScript execution for that run, content selectors, media capture (screenshots/PDFs), and extraction strategies. Think of it as the instructions you give your browser for a particular browsing session on a specific site.
    *   **How the separation of configurations (browser vs. run-specific) aids flexibility:**
        This separation is a key design choice. It allows you to:
        1.  **Reuse Browser Setups:** Configure a browser once (e.g., with specific proxies or a logged-in profile via `BrowserConfig`) and then use that same setup for multiple different crawl tasks, each with its own `CrawlerRunConfig` (e.g., one task extracts text, another captures screenshots, another looks for specific data structures).
        2.  **Targeted Overrides:** For a specific `arun` call, you might need to temporarily use a different user-agent or a different proxy without altering the global browser setup. `CrawlerRunConfig` allows these granular overrides.
        3.  **Clarity:** Keeps concerns separate. Browser setup is distinct from the instructions for a particular crawling job.

*   C. **Common Workflows Involving the Core:**
    *   **Single Page Static Crawl:** Initialize `AsyncWebCrawler`, call `arun()` with a URL and a basic `CrawlerRunConfig` (often just defaults or `CacheMode.BYPASS`).
    *   **Single Page Dynamic Crawl:** Similar to static, but `CrawlerRunConfig` might include `js_code` to interact with the page (e.g., click buttons, scroll) and `wait_for` conditions to ensure dynamic content loads.
    *   **Multi-Page Batch Crawl:** Use `arun_many()` with a list of URLs. You might use a shared `CrawlerRunConfig` if processing is similar for all URLs, or provide custom logic to generate different `CrawlerRunConfig` objects per URL if needed.
    *   **Persistent Session Crawl:** Use `BrowserConfig` with `user_data_dir` for profile persistence or `storage_state`. Then, use `CrawlerRunConfig` with a consistent `session_id` across multiple `arun()` calls to maintain login state or navigate multi-step processes.
    *   **API/JSON Fetching:** Use `AsyncHTTPCrawlerStrategy` with `AsyncWebCrawler` and an appropriate `HTTPCrawlerConfig` (within `CrawlerRunConfig`) for lightweight, non-browser data retrieval.

# II. Mastering `AsyncWebCrawler`: The Heart of Crawling

The `AsyncWebCrawler` class is the cornerstone of Crawl4ai. It orchestrates browser interactions, manages configurations, and processes web content. Understanding its nuances will empower you to build sophisticated and efficient web crawlers.

*   A. **Understanding `AsyncWebCrawler`'s Role:**
    *   **Why `AsyncWebCrawler` is the primary entry point for most crawling tasks:**
        `AsyncWebCrawler` provides a high-level, user-friendly API that abstracts away the complexities of direct browser automation (like Playwright or Selenium). It integrates browser launching, context management, page navigation, content retrieval, and initial processing into a cohesive workflow. Whether you're fetching a single page or thousands, `AsyncWebCrawler` is designed to be your go-to tool.
    *   **Its responsibilities in managing browser instances and executing crawl operations:**
        *   **Browser Lifecycle:** Manages the launching and closing of browser instances based on the provided `BrowserConfig`.
        *   **Context and Page Management:** Handles the creation and isolation of browser contexts and pages, which is crucial for maintaining separate states (cookies, local storage) if needed, especially with `session_id` usage.
        *   **Strategy Execution:** Delegates the actual "crawling" (fetching content from a URL) to a configurable `AsyncCrawlerStrategy` (defaulting to `AsyncPlaywrightCrawlerStrategy`).
        *   **Configuration Orchestration:** Applies both `BrowserConfig` (global browser settings) and `CrawlerRunConfig` (per-crawl settings) to each operation.
        *   **Result Aggregation:** Takes the raw response from the crawler strategy and processes it through scraping and markdown generation (via a `ContentScrapingStrategy` and `MarkdownGenerationStrategy`) to produce the final `CrawlResult`.

*   B. **Initialization and Lifecycle Management:**
    *   1.  **Best Practices for Initializing `AsyncWebCrawler`:**
        *   **When to pass a `BrowserConfig` vs. relying on defaults:**
            *   **Rely on defaults:** For quick, simple crawls where standard browser behavior is sufficient (e.g., fetching a public, static webpage).
            *   **Pass a `BrowserConfig`:**
                *   When you need to specify a browser engine other than Chromium (e.g., Firefox, WebKit).
                *   To run in headed (non-headless) mode for debugging.
                *   To configure proxies.
                *   To set custom user agents or other HTTP headers globally.
                *   To use persistent browser profiles (`user_data_dir`) or load existing browser state (`storage_state`).
                *   For advanced launch arguments.
        *   **Considerations for `logger` and `thread_safe` parameters:**
            *   `logger`: Pass a custom `AsyncLogger` instance if you have a centralized logging setup or need specific log formatting/destinations. If `None`, a default logger is created. For reasoning documents, verbose logging is often helpful.
            *   `thread_safe`: The default `False` is generally fine for most `asyncio`-based applications. Set to `True` only if you are interacting with the same `AsyncWebCrawler` instance from multiple OS-level threads, which is a less common pattern in asyncio. It introduces a lock around critical sections.

    *   2.  **Understanding the Crawler Lifecycle (`start`, `close`, and Context Management):**
        *   **When and why to use explicit `crawler.start()` and `crawler.close()`:**
            *   **Scenarios:**
                *   **Long-running applications:** Where the crawler needs to be available for an extended period to process requests on demand.
                *   **Reusing crawler instances:** If you intend to make multiple, separate `arun()` or `arun_many()` calls with the same underlying browser instance and configuration over time.
                *   **Pre-warming/Setup:** If you need to perform setup tasks (like logging into a site) once and then run multiple crawls using that established session.
            *   **Benefits:**
                *   **Resource Control:** You explicitly manage when browser resources are allocated and released.
                *   **Performance:** Avoids the overhead of launching a new browser for every crawl operation if the instance is reused.
            *   **Code Example: Illustrating explicit start/close for a persistent crawler**
                ```python
                import asyncio
                from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

                async def long_running_crawler_task():
                    # Configure browser once
                    browser_cfg = BrowserConfig(headless=True, user_agent="MyPersistentCrawler/1.0")
                    crawler = AsyncWebCrawler(config=browser_cfg)

                    print("Starting crawler...")
                    await crawler.start() # Explicitly start the browser

                    try:
                        # First crawl
                        run_cfg1 = CrawlerRunConfig(url="https://example.com/page1")
                        result1 = await crawler.arun(config=run_cfg1)
                        if result1.success:
                            print(f"Page 1 ({result1.status_code}): {result1.markdown.raw_markdown[:100]}...")

                        # ... some time later, or another task ...

                        # Second crawl with the same browser instance
                        run_cfg2 = CrawlerRunConfig(url="https://example.com/page2")
                        result2 = await crawler.arun(config=run_cfg2)
                        if result2.success:
                            print(f"Page 2 ({result2.status_code}): {result2.markdown.raw_markdown[:100]}...")

                    finally:
                        print("Closing crawler...")
                        await crawler.close() # Explicitly close the browser and release resources

                if __name__ == "__main__":
                    asyncio.run(long_running_crawler_task())
                ```
        *   **Leveraging `async with AsyncWebCrawler(...)` (Context Manager):**
            *   **Benefits:**
                *   **Automatic Resource Cleanup:** Ensures `crawler.start()` is called at the beginning and `crawler.close()` is called at the end, even if errors occur within the block. This is the most common and recommended way for most use cases.
                *   **Cleaner Code:** Reduces boilerplate for resource management.
            *   **When it's most appropriate:**
                For most scripts or functions where the crawler's lifetime is confined to that specific block of code.
            *   **Code Example: Illustrating typical context manager usage**
                ```python
                import asyncio
                from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

                async def simple_crawl_with_context_manager():
                    browser_cfg = BrowserConfig(headless=True)
                    run_cfg = CrawlerRunConfig(url="https://example.com")

                    async with AsyncWebCrawler(config=browser_cfg) as crawler:
                        # crawler.start() is implicitly called
                        result = await crawler.arun(config=run_cfg)
                        if result.success:
                            print(f"Content from {result.url}: {result.markdown.raw_markdown[:100]}...")
                        else:
                            print(f"Failed: {result.error_message}")
                    # crawler.close() is implicitly called here, even if an exception occurred

                if __name__ == "__main__":
                    asyncio.run(simple_crawl_with_context_manager())
                ```
        *   **Potential pitfalls:**
            *   Forgetting to call `await crawler.close()` when using explicit `await crawler.start()`. This can lead to dangling browser processes and resource leaks. The context manager (`async with`) prevents this.

*   C. **Executing Crawl Operations:**
    *   1.  **Single URL Crawling with `arun()`:**
        *   **Common use cases for `arun()`:**
            *   Fetching and processing a single webpage.
            *   Performing a specific interaction on one page (e.g., filling a form, then extracting results).
            *   Testing configurations on a sample URL.
        *   **Essential `CrawlerRunConfig` parameters for `arun()`:**
            *   `url` (str): The URL to crawl. This is the most fundamental parameter.
            *   `js_code` (Optional[str | List[str]]): JavaScript to execute after the page loads.
            *   `wait_for` (Optional[str]): A CSS selector or JS condition to wait for before proceeding.
            *   See Section IV for a full dive into `CrawlerRunConfig`.
        *   **Workflow: Fetching and processing a single page:**
            1.  Initialize `AsyncWebCrawler` (potentially with a `BrowserConfig`).
            2.  Create a `CrawlerRunConfig` instance, setting the `url` and any other desired options.
            3.  Call `await crawler.arun(config=your_run_config)`.
            4.  Process the returned `CrawlResult` object.
        *   **Code Example: Basic `arun()` usage with a simple `CrawlerRunConfig`**
            ```python
            import asyncio
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode

            async def crawl_single_page():
                # Basic CrawlerRunConfig for a specific URL
                run_config = CrawlerRunConfig(
                    url="https://quotes.toscrape.com/",
                    cache_mode=CacheMode.BYPASS # Ensure fresh fetch for this example
                )

                async with AsyncWebCrawler() as crawler:
                    result = await crawler.arun(config=run_config)
                    if result.success:
                        print(f"Successfully crawled: {result.url}")
                        print(f"Markdown (first 200 chars): {result.markdown.raw_markdown[:200]}...")
                    else:
                        print(f"Failed to crawl {result.url}: {result.error_message}")

            if __name__ == "__main__":
                asyncio.run(crawl_single_page())
            ```
        *   **Troubleshooting common `arun()` issues:**
            *   **Timeouts:** The page might be taking too long to load, or a `wait_for` condition isn't met. Adjust `page_timeout` or `wait_for_timeout` in `CrawlerRunConfig`.
            *   **JavaScript Errors:** If `js_code` execution fails, check the browser console (run with `headless=False` in `BrowserConfig`) or enable `log_console=True` in `CrawlerRunConfig` to see console messages in Crawl4ai logs.
            *   **Content Not Found:** The `css_selector` or `target_elements` might be incorrect, or the content might load after the crawler has finished processing. Use `wait_for` or `delay_before_retrieve_html`.

    *   2.  **Batch Crawling with `arun_many()`:**
        *   **When to prefer `arun_many()` over looping `arun()`:**
            `arun_many()` is generally preferred when you have a list of URLs to process because it leverages an internal dispatcher (like `MemoryAdaptiveDispatcher` by default) to manage concurrency, rate limiting, and resource usage more effectively than a simple Python loop of `await crawler.arun()`. This leads to better performance and stability for bulk operations.
        *   **How `arun_many()` handles concurrency and dispatching:**
            It uses a dispatcher strategy. The default `MemoryAdaptiveDispatcher` attempts to run multiple crawl tasks concurrently, adapting the level of concurrency based on available system memory. You can also provide custom dispatchers for more fine-grained control.
        *   **Using a shared `CrawlerRunConfig` vs. per-URL configurations:**
            *   **Shared `CrawlerRunConfig`:** If all URLs require similar processing (e.g., same extraction schema, same JS interactions), you can pass a single `CrawlerRunConfig` to `arun_many()`. The `url` property within this config will be ignored as URLs are taken from the input list.
            *   **Per-URL Configurations:** `arun_many()` is not designed to take a list of `CrawlerRunConfig` objects directly. If you need vastly different configurations per URL, you might iterate and call `arun()` or, for more sophisticated needs, look into customizing the dispatcher or using higher-level orchestration tools. A common pattern is to use a base `CrawlerRunConfig` and then clone/modify it slightly for each URL within a loop if minor variations are needed, though this would be outside the direct `arun_many` call. For `arun_many`, the primary variation is the URL itself.
        *   **Strategies for rate limiting and error handling within `arun_many()` (via Dispatcher configuration):**
            The dispatcher, particularly its `RateLimiter`, handles rate limiting (delays between requests, exponential backoff on 429/503 errors) and retries. If a URL fails after retries, `arun_many` will still proceed with other URLs and the failed result will indicate the error.
        *   **Code Example: Using `arun_many()` with a list of URLs and a shared `CrawlerRunConfig`**
            ```python
            import asyncio
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode

            async def crawl_multiple_pages():
                urls_to_crawl = [
                    "http://quotes.toscrape.com/page/1/",
                    "http://quotes.toscrape.com/page/2/",
                    "http://quotes.toscrape.com/tag/humor/", # This might be a different structure
                ]

                # Shared config for all URLs in this batch
                shared_run_config = CrawlerRunConfig(
                    cache_mode=CacheMode.BYPASS,
                    word_count_threshold=50 # Only process pages with meaningful content
                )

                async with AsyncWebCrawler() as crawler:
                    # results will be a list of CrawlResult objects, in the same order as urls_to_crawl
                    results_container = await crawler.arun_many(urls=urls_to_crawl, config=shared_run_config)
                    
                    for result in results_container: # Iterate through CrawlResultContainer
                        if result.success:
                            print(f"Processed: {result.url}, Markdown Length: {len(result.markdown.raw_markdown)}")
                        else:
                            print(f"Failed: {result.url}, Error: {result.error_message}")

            if __name__ == "__main__":
                asyncio.run(crawl_multiple_pages())
            ```
        *   **Code Example: Using `arun_many()` with a custom dispatcher for advanced control**
            *(Note: Custom dispatchers are more advanced; the default `MemoryAdaptiveDispatcher` is usually sufficient.)*
            ```python
            import asyncio
            from crawl4ai import (
                AsyncWebCrawler, CrawlerRunConfig, CacheMode,
                MemoryAdaptiveDispatcher, RateLimiter, CrawlerMonitor, DisplayMode
            )

            async def crawl_with_custom_dispatcher():
                urls = [f"http://quotes.toscrape.com/page/{i}/" for i in range(1, 4)]
                
                # Custom rate limiter: 0.5-1.0s base delay, max 10s, 2 retries
                rate_limiter = RateLimiter(base_delay=(0.5, 1.0), max_delay=10.0, max_retries=2)
                
                # Custom monitor
                monitor = CrawlerMonitor(display_mode=DisplayMode.DETAILED)

                # Custom dispatcher with more conservative settings
                custom_dispatcher = MemoryAdaptiveDispatcher(
                    memory_threshold_percent=60.0, # Pause if memory > 60%
                    check_interval=2.0,           # Check memory every 2 seconds
                    max_session_permit=5,         # Max 5 concurrent crawls
                    rate_limiter=rate_limiter,
                    monitor=monitor
                )

                run_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)

                async with AsyncWebCrawler() as crawler:
                    results_container = await crawler.arun_many(
                        urls=urls,
                        config=run_config,
                        dispatcher=custom_dispatcher
                    )
                    for result in results_container:
                        if result.success:
                            print(f"URL: {result.url} - Success")
                        else:
                            print(f"URL: {result.url} - Failed: {result.error_message}")
                            
            if __name__ == "__main__":
                asyncio.run(crawl_with_custom_dispatcher())
            ```
*   D. **Understanding the Internal Processing Flow (`aprocess_html`):**
    *   **High-level explanation of what happens after HTML is fetched:**
        Once the `AsyncCrawlerStrategy` (e.g., `AsyncPlaywrightCrawlerStrategy`) successfully fetches the HTML content for a URL, `AsyncWebCrawler`'s `aprocess_html` method (or a similar internal handler) takes over. This method orchestrates several subsequent steps:
        1.  **Scraping Strategy Application:** The raw HTML is passed to a `ContentScrapingStrategy` (defaulting to `WebScrapingStrategy` or `LXMLExtractionStrategy` based on configuration). This strategy is responsible for:
            *   Cleaning the HTML (removing scripts, styles, comments).
            *   Applying `css_selector` or `target_elements` to scope the content if specified.
            *   Extracting links (`<a>` tags).
            *   Extracting media information (`<img>`, `<video>`, `<audio>` tags, and data tables).
            *   Extracting page metadata (title, meta description, OpenGraph tags, etc.).
            The output of this step is a `ScrapingResult` object containing `cleaned_html`, `media`, `links`, and `metadata`.
        2.  **Content Filtering (Optional):** If a `content_filter` (like `PruningContentFilter` or `LLMContentFilter`) is configured in the `MarkdownGenerationStrategy` (which is part of `CrawlerRunConfig`), the `cleaned_html` from the `ScrapingResult` is passed through this filter. This step aims to further refine the HTML to only include the most relevant content blocks. The output is "fit HTML".
        3.  **Markdown Generation:** The (potentially filtered) HTML is then passed to a `MarkdownGenerationStrategy` (defaulting to `DefaultMarkdownGenerator`). This strategy converts the HTML into Markdown. If content filtering occurred, it might generate both a `raw_markdown` (from the original cleaned HTML) and a `fit_markdown` (from the filtered HTML). It also handles citation generation for links. The output is a `MarkdownGenerationResult`.
        4.  **Structured Data Extraction (Optional):** If an `extraction_strategy` is provided in `CrawlerRunConfig`, it's applied *after* Markdown generation. The input to the extraction strategy depends on its `input_format` (e.g., "markdown", "html", "fit_markdown"). This strategy extracts structured data (like JSON) based on its specific logic (e.g., LLM prompting, CSS selectors via `JsonCssExtractionStrategy`).
        5.  **Result Aggregation:** Finally, all these pieces (`CrawlResult.html` (original), `cleaned_html`, `markdown` (the `MarkdownGenerationResult` object), `extracted_content`, `media`, `links`, `metadata`, etc.) are assembled into the final `CrawlResult` object.
    *   **How this internal flow influences the choice and impact of `CrawlerRunConfig` parameters:**
        *   `css_selector`: Applied early by the `ScrapingStrategy`. If set, *only* the HTML within this selector is processed for everything downstream (Markdown, extraction, etc.). This can significantly speed up processing and improve relevance.
        *   `target_elements`: Also applied by `ScrapingStrategy`, but its effect is primarily on what content is considered for *Markdown generation* and *structured data extraction* if the extraction strategy uses HTML as input. Links and media are still typically extracted from the whole (scoped by `css_selector` if present) page.
        *   `extraction_strategy`: This is one of the last steps. Its effectiveness depends on the quality and format of its input (e.g., clean Markdown or specific HTML structure).
        *   `markdown_generator`: Affects how `cleaned_html` (or `fit_html`) is converted to Markdown. The `content_filter` within it can drastically change the `fit_markdown`.
    *   **Decision Guide: When to rely on default processing vs. providing custom strategies:**
        *   **Default Processing:** Sufficient for many common use cases where you need the main content of a page converted to clean Markdown, along with standard metadata, links, and images.
        *   **Custom `ContentScrapingStrategy`:** Consider if you need highly specialized HTML cleaning, link/media extraction logic that differs significantly from the defaults, or if you're dealing with non-standard HTML structures.
        *   **Custom `MarkdownGenerationStrategy`:** If you need a different HTML-to-Markdown conversion engine, different citation styles, or very specific pre/post-processing of the Markdown.
        *   **Custom `ContentFilter` (within `MarkdownGenerationStrategy`):** If the default pruning/fitting logic isn't capturing the desired content accurately and you need a more sophisticated way (e.g., LLM-based, custom heuristics) to identify relevant sections.
        *   **Custom `ExtractionStrategy`:** Essential if you need to extract structured data beyond what `JsoupCssExtractionStrategy` can offer, or if you want to use LLMs for extraction based on natural language prompts or a defined schema.

# III. Configuring the Browser: `BrowserConfig` Deep Dive

`BrowserConfig` is your toolkit for defining the environment in which your web crawls will run. It dictates everything from the browser engine and its appearance (headless/headed) to network settings like proxies and crucial identity aspects like user agents and persistent storage. A well-configured `BrowserConfig` is often the key to successful and reliable crawling, especially on modern, dynamic websites.

*   A. **Purpose and Importance of `BrowserConfig`:**
    *   **Explaining how `BrowserConfig` defines the foundational browser environment:**
        Think of `BrowserConfig` as setting up your web browser application *before* you even type a URL. It's the global configuration for the `AsyncWebCrawler` instance. Changes here affect all crawl operations performed by that instance, unless specifically overridden by a `CrawlerRunConfig`.
    *   **Why getting this right is crucial for successful and stealthy crawling:**
        *   **Site Compatibility:** Some sites render or behave differently based on the browser engine or viewport size.
        *   **Stealth:** Websites employ various techniques to detect and block automated crawlers. A realistic `BrowserConfig` (e.g., common user agent, appropriate headers, possibly proxies) can significantly reduce the chances of detection.
        *   **Resource Management:** Settings like `headless` mode or `browser_mode` can impact how many resources your crawler consumes.
        *   **Session Persistence:** For sites requiring logins or multi-step interactions, `use_persistent_context` and `user_data_dir` are essential for maintaining state.

*   B. **Key `BrowserConfig` Decision Points and Workflows:**
    *   1.  **Choosing a `browser_type` (`chromium`, `firefox`, `webkit`):**
        *   **Trade-offs:**
            *   `chromium`: Most widely used, excellent DevTools support, generally good performance and compatibility. Often the default and a good starting point.
            *   `firefox`: Strong alternative, good privacy features, sometimes handles certain sites differently.
            *   `webkit`: Engine behind Safari. Useful for testing Safari-specific rendering or behavior.
        *   **When one might be preferred:**
            *   Start with `chromium`.
            *   If you encounter issues specific to Chromium-based browsers or need to emulate Firefox/Safari users, switch accordingly.
            *   Some sites might have better compatibility or less aggressive bot detection for one engine over others.
    *   2.  **Headless vs. Headed Mode (`headless`):**
        *   **Rationale for headless (`headless=True`, default):**
            *   No visible GUI, runs in the background.
            *   Essential for server environments or automated scripts.
            *   Generally consumes fewer resources than headed mode.
        *   **When to use headed mode (`headless=False`):**
            *   **Debugging:** Visually inspect what the crawler is doing, observe page rendering, and identify issues with selectors or interactions.
            *   **Complex Interactions:** For sites with very complex JavaScript, CAPTCHAs (though Crawl4ai doesn't solve these directly), or interactions that are hard to automate blindly.
            *   **Initial Setup:** When first developing a script for a new site, running in headed mode can be invaluable.
        *   **Impact on resource consumption and stealth:**
            *   Headed mode consumes more CPU and memory.
            *   Some rudimentary bot detection systems might flag headless browsers, though modern headless modes are much harder to detect than older versions.
    *   3.  **Browser Launch Modes (`browser_mode`):**
        *   **Understanding `"dedicated"` (default):**
            *   **Pros:** Each `AsyncWebCrawler` instance (or more accurately, its `BrowserManager`) launches and manages its own independent browser process. This provides strong isolation.
            *   **Cons:** Can be resource-intensive if you have many `AsyncWebCrawler` instances, as each spawns a new browser.
        *   **Understanding `"builtin"` (CDP based):**
            *   The `builtin` mode is designed to use a browser instance that Crawl4ai itself manages in the background, potentially shared across different `AsyncWebCrawler` instances if configured carefully (though this is an advanced use case). It connects via the Chrome DevTools Protocol (CDP).
            *   **When to use:** When you want Crawl4ai to manage the browser lifecycle but need to connect to it via CDP for more direct control or to share a browser instance.
            *   **Setup:** `use_managed_browser` is implicitly `True`. `cdp_url` is typically set by the internal `ManagedBrowser`.
        *   **Understanding `"cdp"` (external CDP):**
            *   **Scenarios:** You have an existing browser instance already running (e.g., a Chrome browser launched with `--remote-debugging-port=9222`) and you want Crawl4ai to connect to and control that instance.
            *   **Setup:** You must provide the `cdp_url` (e.g., `"ws://localhost:9222/devtools/browser/..."`).
        *   **Understanding `"docker"`:**
            *   **Benefits:** Runs the browser within a Docker container, providing excellent isolation, reproducibility, and easier dependency management, especially in CI/CD environments or when deploying to different systems. The core library handles launching a pre-configured Docker container (like `browserless/chrome`) and connecting to it.
            *   Refer to [Docker Deployment Guide](../../../basic/docker-deployment.md) for details.
        *   **Decision Guide: Selecting the appropriate `browser_mode`:**
            *   `"dedicated"`: Good default for simplicity and isolation when running a moderate number of crawlers.
            *   `"cdp"`: Use if you need to attach to an externally managed browser.
            *   `"docker"`: Excellent for reproducible environments, CI/CD, and avoiding local browser/driver issues.
            *   `"builtin"`: More of an internal mechanism, usually not directly set by users unless for advanced shared browser scenarios.
    *   4.  **User Agent Management (`user_agent`, `user_agent_mode`):**
        *   **Importance of a realistic User-Agent:** Many sites use the User-Agent string for basic bot detection or to serve different content. A missing or suspicious UA can lead to blocks or incorrect page versions.
        *   **Strategies for User-Agent rotation using `user_agent_mode="random"`:**
            Crawl4ai uses the `fake_useragent` library (via `ValidUAGenerator`) to pick from a list of common, valid user agents. This helps in making requests appear as if they are coming from different browsers/devices.
        *   **When to provide a specific `user_agent`:**
            If you need to emulate a very specific browser or device, or if a site requires a particular User-Agent to function correctly.
        *   **How `user_agent_generator_config` can be used for fine-tuning:**
            You can pass a dictionary to `user_agent_generator_config` to control aspects of the `ValidUAGenerator`, such as specifying browser types (`browsers=['chrome', 'edge']`), OS types, or min/max popularity.
        *   **Code Example: Setting a custom User-Agent**
            ```python
            from crawl4ai import BrowserConfig

            # To emulate a specific mobile browser
            custom_ua_config = BrowserConfig(
                user_agent="Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1"
            )
            # Now use this custom_ua_config when initializing AsyncWebCrawler
            ```
        *   **Code Example: Using random User-Agent generation for Chrome and Edge on Windows**
            ```python
            from crawl4ai import BrowserConfig

            random_ua_config = BrowserConfig(
                user_agent_mode="random",
                user_agent_generator_config={
                    "os_names": ["windows"],
                    "browser_names": ["chrome", "edge"]
                }
            )
            # This will generate UAs like Chrome on Windows or Edge on Windows
            ```
    *   5.  **Proxy Configuration (`proxy_config` via `ProxyConfig` object):**
        *   **When and why to use proxies:**
            *   **IP Rotation:** To avoid IP-based blocking when making many requests to the same site.
            *   **Geo-targeting:** To access content as if you are browsing from a specific geographical location.
            *   **Anonymity/Privacy:** To mask your actual IP address.
        *   **Workflow: Setting up a single proxy:**
            1.  Create a `ProxyConfig` instance with `server`, and optionally `username` and `password`.
            2.  Assign this `ProxyConfig` instance to `BrowserConfig.proxy_config`.
        *   **Workflow: Integrating with proxy rotation strategies:**
            This is typically handled at the `CrawlerRunConfig` level for more dynamic rotation, but `BrowserConfig` can set a default proxy. For rotation, you'd use `CrawlerRunConfig(proxy_rotation_strategy=YourStrategy())`. See Section VII.A for details on rotation.
        *   **Code Example: Configuring a single HTTP/SOCKS proxy with authentication**
            ```python
            from crawl4ai import BrowserConfig, ProxyConfig

            # HTTP Proxy with authentication
            http_proxy_cfg = ProxyConfig(
                server="http://proxy.example.com:8080",
                username="proxy_user",
                password="proxy_password"
            )
            browser_with_http_proxy = BrowserConfig(proxy_config=http_proxy_cfg)

            # SOCKS5 Proxy (authentication handled in server string if supported by proxy)
            socks_proxy_cfg = ProxyConfig(
                server="socks5://another-proxy.example.com:1080"
            )
            browser_with_socks_proxy = BrowserConfig(proxy_config=socks_proxy_cfg)
            ```
        *   **Troubleshooting common proxy connection issues:**
            *   Incorrect server address or port.
            *   Wrong username/password.
            *   Proxy server is down or not reachable.
            *   Firewall blocking connection to the proxy.
            *   The website itself might be blocking the proxy's IP.
    *   6.  **Persistent Context and User Data (`use_persistent_context`, `user_data_dir`):**
        *   **Understanding the benefits of persistent contexts:**
            When `use_persistent_context=True` and a `user_data_dir` is specified, Playwright creates a persistent browser profile in that directory. This means cookies, localStorage, sessionStorage, and other browser data are saved between sessions. This is invaluable for:
            *   **Login Persistence:** Log in to a site once, and subsequent crawls using the same `user_data_dir` will likely remain logged in.
            *   **Maintaining Site Preferences:** If a site stores user preferences (e.g., dark mode, language) in local storage or cookies.
            *   **Reducing Repeated Setup:** Avoid re-doing consent banners or initial setup steps on every crawl.
        *   **Workflow: Creating and reusing a browser profile for login persistence:**
            1.  **First Run (Profile Creation & Login):**
                *   Set `headless=False` initially to manually perform the login.
                *   Specify a `user_data_dir` (e.g., `./my_browser_profile`).
                *   Set `use_persistent_context=True`.
                *   Run the crawler, navigate to the login page, and log in manually.
                *   Close the browser. The session data is now saved in `user_data_dir`.
            2.  **Subsequent Runs (Reusing Profile):**
                *   Use the *same* `user_data_dir`.
                *   Ensure `use_persistent_context=True`.
                *   You can now set `headless=True`.
                *   The crawler should start with the saved session, already logged in.
        *   **Best practices for managing `user_data_dir`:**
            *   Choose a unique, descriptive path for each distinct profile/session you want to maintain.
            *   Be aware of the disk space used, as profiles can grow.
            *   Ensure your script has write permissions to the specified directory.
        *   **Code Example: Setting up and using a persistent context for a login workflow**
            ```python
            import asyncio
            import os
            from pathlib import Path
            from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

            PROFILE_DIR = Path("./my_persistent_profile")

            async def initial_login_setup():
                # Create profile dir if it doesn't exist
                PROFILE_DIR.mkdir(parents=True, exist_ok=True)
                
                login_browser_cfg = BrowserConfig(
                    headless=False, # Run headed to manually log in
                    user_data_dir=str(PROFILE_DIR),
                    use_persistent_context=True
                )
                # For this setup run, we don't need a complex CrawlerRunConfig
                login_run_cfg = CrawlerRunConfig(url="https://example.com/login") # Replace with actual login URL

                print(f"Please log in manually in the browser window using profile: {PROFILE_DIR}")
                print("Close the browser window once logged in to save the session.")
                
                # We use a longer timeout to allow for manual login
                # And we don't need to process the result here, just establish the session
                async with AsyncWebCrawler(config=login_browser_cfg) as crawler:
                    # The page_timeout in BrowserConfig will apply here if not overridden in CrawlerRunConfig
                    # A simple goto is enough; the user handles the login in the browser
                    await crawler.arun(config=CrawlerRunConfig(url=login_run_cfg.url, page_timeout=300000)) # 5 min timeout

                print("Login session should be saved.")

            async def crawl_protected_page_with_session():
                if not PROFILE_DIR.exists() or not any(PROFILE_DIR.iterdir()):
                     print(f"Profile directory {PROFILE_DIR} is empty or does not exist. Run initial_login_setup() first.")
                     return

                persistent_browser_cfg = BrowserConfig(
                    headless=True, # Can now run headless
                    user_data_dir=str(PROFILE_DIR),
                    use_persistent_context=True
                )
                # Target a page that requires login
                protected_run_cfg = CrawlerRunConfig(url="https://example.com/dashboard") 

                async with AsyncWebCrawler(config=persistent_browser_cfg) as crawler:
                    result = await crawler.arun(config=protected_run_cfg)
                    if result.success and "Welcome User" in result.html: # Check for logged-in content
                        print(f"Successfully accessed protected page: {result.url}")
                        print(f"Content snippet: {result.markdown.raw_markdown[:200]}...")
                    elif result.status_code == 403 or "login" in result.url.lower():
                         print(f"Failed to access protected page. Still on login page or got 403. URL: {result.url}")
                    else:
                        print(f"Failed to crawl protected page: {result.error_message}, URL: {result.url}")
            
            async def main():
                # Run this once to log in and create the profile
                # await initial_login_setup() 
                
                # Then run this to use the saved session
                await crawl_protected_page_with_session()

            if __name__ == "__main__":
                # IMPORTANT: You'd typically run initial_login_setup() once manually,
                # then comment it out and run crawl_protected_page_with_session() for subsequent crawls.
                asyncio.run(main())
            ```
    *   7.  **Viewport Configuration (`viewport_width`, `viewport_height`, `viewport`):**
        *   **How viewport size can affect page rendering and element visibility:**
            Websites often use responsive design, meaning their layout and the visibility of certain elements change based on the viewport (browser window) size. If your target elements are only visible on larger screens (or mobile views), setting an appropriate viewport is crucial.
        *   **Strategies for choosing appropriate viewport dimensions:**
            *   Inspect the target website in a regular browser, resize the window, and see how content changes.
            *   Common desktop viewports: `1920x1080`, `1366x768`, `1280x720`.
            *   Common mobile viewports (emulated): `375x667` (iPhone 6/7/8), `414x896` (iPhone XR/11).
            *   If `viewport` dict is provided, it overrides `viewport_width` and `viewport_height`. E.g., `viewport={"width": 1920, "height": 1080}`.
    *   8.  **Advanced Browser Arguments (`extra_args`):**
        *   **When to use `extra_args`:**
            For passing command-line arguments directly to the browser executable. This is useful for enabling/disabling experimental features, or for very specific browser configurations not exposed by Playwright's high-level API.
        *   **Commonly used arguments and their effects (Chromium examples):**
            *   `--disable-gpu`: Can sometimes help in headless environments or reduce resource usage.
            *   `--no-sandbox`: Often required in Docker/CI environments, but use with caution as it reduces security. (Crawl4ai's `ManagedBrowser` often adds this automatically in headless Linux).
            *   `--lang=fr-FR`: Set browser UI language.
            *   `--disable-blink-features=AutomationControlled`: Attempt to make the browser appear less like an automated tool.
        *   **Caution:** Incorrect or incompatible arguments can prevent the browser from launching or cause instability. Refer to the browser's documentation (e.g., [Chromium Command Line Switches](https://peter.sh/experiments/chromium-command-line-switches/)) for a full list.
    *   9.  **Storage State Management (`storage_state`):**
        *   **What is storage state and why is it useful?**
            Storage state is a JSON object (or path to a JSON file) that captures cookies, `localStorage`, and `sessionStorage` from a browser context. It's a powerful way to persist and reuse login sessions or application states without relying on full user profiles (`user_data_dir`). It's generally more lightweight than managing entire profile directories.
        *   **Workflow: Capturing storage state:**
            1.  Launch a browser (often headed for manual interaction).
            2.  Perform the necessary actions (e.g., log in, accept cookies, set preferences).
            3.  Use Playwright's `context.storage_state(path="my_storage_state.json")` to save the state. Crawl4ai doesn't have a direct command to save state *during* a crawl, so this is typically done in a separate setup script.
                ```python
                # Example: Separate script to save storage state
                import asyncio
                from playwright.async_api import async_playwright

                async def save_state():
                    async with async_playwright() as p:
                        browser = await p.chromium.launch(headless=False)
                        context = await browser.new_context()
                        page = await context.new_page()
                        await page.goto("https://example.com/login") # Replace
                        # ... user manually logs in ...
                        input("Press Enter after logging in to save state...")
                        await context.storage_state(path="auth_state.json")
                        await browser.close()
                        print("Storage state saved to auth_state.json")

                # asyncio.run(save_state()) # Run this once
                ```
        *   **Workflow: Reusing storage state in Crawl4ai:**
            Pass the path to the saved JSON file (or the loaded JSON object itself) to `BrowserConfig(storage_state="auth_state.json")`.
        *   **Code Example: Illustrating how to load storage state in Crawl4ai**
            ```python
            import asyncio
            from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

            async def crawl_with_saved_state():
                # Assumes "auth_state.json" was created by a previous login session
                browser_cfg = BrowserConfig(
                    headless=True,
                    storage_state="auth_state.json" # Load cookies, localStorage, etc.
                )
                # Target a page that requires the saved session
                run_cfg = CrawlerRunConfig(url="https://example.com/dashboard")

                async with AsyncWebCrawler(config=browser_cfg) as crawler:
                    result = await crawler.arun(config=run_cfg)
                    if result.success and "Welcome User" in result.html: # Or other logged-in indicator
                        print("Successfully accessed dashboard using saved state.")
                    else:
                        print(f"Failed. Status: {result.status_code}, URL: {result.url}")
            
            if __name__ == "__main__":
                asyncio.run(crawl_with_saved_state())
            ```
*   C. **Best Practices for `BrowserConfig`:**
    *   **Start Simple:** Begin with minimal configuration (e.g., just `headless`) and add options as needed to address specific site requirements or improve stealth.
    *   **Prioritize Realistic Emulation:** For stealth, aim to make your crawler's browser fingerprint (User-Agent, viewport, headers, language settings) appear as close to a real user's browser as possible.
    *   **Manage User Data:** If using `user_data_dir` or `storage_state`, have a clear strategy for creating, updating, and cleaning up these profile/state files. Version control or separate directories for different tasks/sites can be helpful.
    *   **Test Proxy Configurations:** Thoroughly test proxies to ensure they are working and not blocked by target sites.
*   D. **Troubleshooting `BrowserConfig` Issues:**
    *   **Browser Not Launching:**
        *   Ensure Playwright and its browser drivers are correctly installed (`playwright install` or `crawl4ai-setup`).
        *   Check for conflicting `extra_args`.
        *   Permissions issues if `user_data_dir` is in a restricted location.
    *   **Proxy Connection Errors:**
        *   Verify proxy server address, port, username, and password.
        *   Test proxy connectivity outside of Crawl4ai.
        *   The proxy itself might be down or the target site might be blocking it.
    *   **User-Agent Related Detection:**
        *   If blocked, try different, common User-Agents or use `user_agent_mode="random"`.
        *   Ensure client hints (`sec-ch-ua`) sent by the browser (often managed by Playwright automatically based on UA) are consistent.
    *   **Persistent Context Not Working:**
        *   Double-check `user_data_dir` path and permissions.
        *   Ensure `use_persistent_context=True`.
        *   Some sites have aggressive session invalidation mechanisms.
        *   `storage_state` might be more reliable than `user_data_dir` for some sites as it's a more focused snapshot.

# IV. Customizing Each Crawl: `CrawlerRunConfig` In-Depth

While `BrowserConfig` sets the stage for the browser environment, `CrawlerRunConfig` provides the script for each act. It allows you to fine-tune how Crawl4ai interacts with and processes individual URLs or batches of URLs. This granular control is essential for adapting to diverse website structures and extracting the specific information you need.

*   A. **Purpose and Flexibility of `CrawlerRunConfig`:**
    *   **Explaining how `CrawlerRunConfig` allows fine-tuning behavior for each `arun()` or for individual URLs in `arun_many()`:**
        `CrawlerRunConfig` is passed directly to `crawler.arun()` or as a base configuration to `crawler.arun_many()`. This means you can have one `AsyncWebCrawler` instance (with its `BrowserConfig`) and then execute various types of crawls with different objectives by simply changing the `CrawlerRunConfig` for each call.
    *   **How it can override or augment settings from `BrowserConfig`:**
        Certain parameters in `CrawlerRunConfig` (like `user_agent` or `proxy_config`) can override those set in the `BrowserConfig` for that specific run. This is useful for targeted adjustments without reconfiguring the entire browser. For example, you might use a general pool of proxies in `BrowserConfig` but need a specific geo-targeted proxy for one particular `arun()` call.

*   B. **Key `CrawlerRunConfig` Decision Points and Workflows:**
    *   1.  **Caching Strategies (`cache_mode`):**
        *   **Understanding `CacheMode` options:**
            *   `CacheMode.BYPASS` (Default for `arun_many`, was `False` for `bypass_cache` in `arun`): Fetches fresh content every time. Use when up-to-date data is critical.
            *   `CacheMode.ENABLED`: Reads from cache if available and valid; writes to cache if not. This is the default for `arun()`. Good for development to speed up iterations.
            *   `CacheMode.READ_ONLY`: Only reads from the cache; never writes. Useful if you have a pre-populated cache and don't want to modify it.
            *   `CacheMode.WRITE_ONLY`: Always fetches fresh content and writes it to the cache, but never reads from the cache for serving the request. Useful for populating/updating a cache without using stale data.
            *   `CacheMode.DISABLED`: No caching involved at all. Similar to `BYPASS` but might have subtle differences in how the cache check is skipped entirely. `BYPASS` is generally clearer for "always fetch fresh".
        *   **Decision Guide: Choosing the right cache mode:**
            *   **Development/Testing:** `CacheMode.ENABLED` can save significant time by reusing previously fetched content. Use `CacheMode.BYPASS` when you explicitly need to test fresh fetching.
            *   **Production (Data Freshness Critical):** `CacheMode.BYPASS` or `CacheMode.WRITE_ONLY` (if you still want to update your cache).
            *   **Production (Speed/Cost Important, Some Staleness OK):** `CacheMode.ENABLED` with appropriate cache TTL settings (not directly in `CrawlerRunConfig`, but a general system consideration).
            *   **Using a Static Cache:** `CacheMode.READ_ONLY` if you have a pre-built dataset you want the crawler to use.
        *   **Impact on crawl speed and data freshness:** Caching improves speed for repeated requests to the same URL but can serve stale data if not managed. `BYPASS` ensures freshness but is slower for repeated crawls.
    *   2.  **Content Selection and Filtering (`css_selector`, `target_elements`, `excluded_tags`, `excluded_selector`):**
        These parameters help you narrow down the HTML content that Crawl4ai processes, leading to cleaner Markdown, more accurate extractions, and faster processing.
        *   **`css_selector` (string):**
            *   **Workflow:** If you only care about a specific section of a webpage (e.g., the main article content, a product description block), provide a CSS selector that uniquely identifies this container.
            *   **Impact:** Crawl4ai will take the HTML *within* this selected element and process only that for Markdown generation, link/media extraction *within that scope*, and structured data extraction.
            *   **When this is more efficient:** When the desired content is well-contained and you want to discard irrelevant surrounding HTML (headers, footers, sidebars, ads) *before* any further processing, saving computational resources.
        *   **`target_elements` (List[str]):**
            *   **Workflow:** If you want to generate Markdown or extract structured data from *multiple specific sections* of a page, provide a list of CSS selectors.
            *   **Impact:** Unlike `css_selector` which creates a single "scoped" document, `target_elements` tells the `MarkdownGenerationStrategy` and some `ExtractionStrategy` instances to focus their efforts only on the content within these elements. However, link and media extraction might still consider the whole page (or the scope defined by `css_selector` if also present).
            *   **Comparison with `css_selector`:** `css_selector` creates a new, smaller effective document for all downstream processing. `target_elements` guides specific downstream processes (like Markdown or schema-based extraction) to look only within those targets, but the "document" for other purposes (like general link extraction) might still be broader.
        *   **`excluded_tags` (List[str]) and `excluded_selector` (string):**
            *   **Strategies for removing unwanted content:** These are applied *after* `css_selector` (if used) but generally *before* `target_elements` are specifically focused on for Markdown/extraction.
            *   `excluded_tags`: Provide a list of HTML tag names (e.g., `["nav", "footer", "script", "style"]`) to remove entirely.
            *   `excluded_selector`: Provide a CSS selector for more complex elements to remove (e.g., `".ads-sidebar", "#cookie-banner"`).
            *   **Impact:** Cleaner input for Markdown generation and extraction, leading to more relevant and concise results.
        *   **Code Example: Using `css_selector` to focus on an article body**
            ```python
            # Assuming an article page where main content is in <article class="main-story">
            run_config_article = CrawlerRunConfig(
                url="https://example.com/news/my-article",
                css_selector="article.main-story" 
            )
            # result.cleaned_html and result.markdown will only contain content from within that article tag.
            ```
        *   **Code Example: Using `target_elements` to extract multiple sections for structured data**
            ```python
            # Assuming a product page with separate sections for description and reviews
            run_config_product = CrawlerRunConfig(
                url="https://example.com/product/123",
                target_elements=["#product-description", ".customer-reviews-section"],
                # Extraction strategy would then know to look within these.
            )
            ```
        *   **Code Example: Using `excluded_tags` to remove headers and footers**
            ```python
            run_config_clean = CrawlerRunConfig(
                url="https://example.com/some-page",
                excluded_tags=["header", "footer", "nav", "aside"]
            )
            ```
    *   3.  **JavaScript Execution and Interaction (`js_code`, `js_only`, `wait_for`, `wait_for_timeout`, `scan_full_page`, `scroll_delay`):**
        These parameters are vital for handling dynamic websites where content is loaded or modified by JavaScript.
        *   **Workflow: Executing simple JS snippets with `js_code` (str or List[str]):**
            *   Use `js_code` to run JavaScript after the initial page load. This can be for clicking buttons, expanding sections, scrolling, or any other client-side interaction needed to reveal content.
            *   If providing a list, scripts are executed sequentially.
        *   **Understanding `js_only` (bool):**
            *   Set to `True` for subsequent interactions on a page *within the same session* where you don't need a full page reload/navigation, but only want to execute new `js_code` and re-evaluate the page state.
            *   Requires `session_id` to be set to maintain the page context.
            *   Example: Clicking pagination buttons on a Single Page Application (SPA).
        *   **`wait_for` (Optional[str]):**
            *   **Strategies for effective waiting:**
                *   **CSS Selector:** `wait_for="css:.my-dynamic-content"` - Waits until an element matching the selector appears.
                *   **JavaScript Condition:** `wait_for="js:() => window.myAppDataLoaded === true"` - Waits until the JS expression evaluates to true.
                *   **Network Idle:** `wait_for="networkidle"` (Playwright specific event) - Waits until network activity subsides.
                *   **Timeout (number):** `wait_for=5000` - Simply waits for a fixed number of milliseconds.
            *   **Impact of `wait_for_timeout` (int, milliseconds):**
                If the `wait_for` condition isn't met within this timeout, the crawl proceeds or may fail depending on other settings. Defaults to `page_timeout`.
        *   **`scan_full_page` (bool) and `scroll_delay` (float, seconds):**
            *   **Handling lazy-loaded content and infinite scroll:**
                Set `scan_full_page=True` to make Crawl4ai attempt to scroll through the entire page, triggering lazy-loaded images or infinite scroll content.
            *   **Best practices for `scroll_delay`:**
                The `scroll_delay` is the pause between each scroll step. Adjust this based on how quickly the target site loads new content upon scrolling. Too short might miss content; too long will slow down the crawl. Start with `0.2` to `0.5` and adjust.
        *   **Code Example: Clicking a "Load More" button and waiting for new content**
            ```python
            # Assumes a button with id="load-more-btn" and new items get class ".new-item"
            run_config_load_more = CrawlerRunConfig(
                url="https://example.com/feed",
                js_code="document.getElementById('load-more-btn').click();",
                wait_for="css:.new-item" # Wait for at least one new item to appear
            )
            ```
        *   **Code Example: Scrolling to the bottom of a page to trigger lazy loading**
            ```python
            run_config_scroll = CrawlerRunConfig(
                url="https://example.com/gallery",
                scan_full_page=True,
                scroll_delay=0.75 # Give a bit more time for images to load
            )
            ```
    *   4.  **Media Capture (`screenshot`, `pdf`, `capture_mhtml`):**
        *   **When to capture screenshots (`screenshot=True`):**
            Useful for debugging (to see what the crawler "saw"), for visual verification of page state, or for archiving a visual snapshot. The result is a base64-encoded PNG string in `result.screenshot`.
        *   **When to generate PDFs (`pdf=True`):**
            Ideal for archiving web content in a stable, printable format. The result is raw PDF bytes in `result.pdf_data`.
        *   **Understanding MHTML (`capture_mhtml=True`):**
            MHTML (MIME HTML) saves a complete webpage (HTML, CSS, images, etc.) into a single `.mhtml` file. This is excellent for perfect archival as it can be opened in browsers like Chrome/Edge and displays the page as it was. Result is in `result.mhtml_data`.
        *   **Configuring related parameters like `screenshot_wait_for`:**
            If you need to wait for a specific element or condition *before* taking the screenshot (e.g., an animation to complete), you can use `screenshot_wait_for` with similar syntax to `wait_for`. (Note: check if `screenshot_wait_for` is a direct param or if you should use a general `wait_for` before setting `screenshot=True`). Typically, `wait_for` applies before the final HTML/media capture.
    *   5.  **Link and Domain Handling (`exclude_external_links`, `exclude_social_media_links`, `exclude_domains`):**
        *   **Strategies for controlling the scope of link extraction:**
            These flags help you refine the `result.links` collection.
            *   `exclude_external_links=True`: Only keeps links pointing to the same base domain.
            *   `exclude_social_media_links=True`: Removes links to common social media platforms (Facebook, Twitter, LinkedIn, etc.). It uses a predefined list which can be augmented via `exclude_social_media_domains`.
            *   `exclude_domains=["ads.example.com", "tracker.net"]`: Provide a list of specific domains whose links should be entirely ignored.
        *   **When and why to exclude certain types of links:**
            *   To focus on internal site structure.
            *   To avoid crawling irrelevant third-party sites.
            *   To clean up link data for analysis.
    *   6.  **Identity and Proxy Overrides (`user_agent`, `proxy_config` in `CrawlerRunConfig`):**
        *   **Scenarios where overriding `BrowserConfig` settings per-run is useful:**
            *   **A/B Testing:** Test how a site responds to different User-Agents.
            *   **Geo-Targeted Proxies:** Use a general proxy pool in `BrowserConfig` but switch to a specific country's proxy for a particular URL via `CrawlerRunConfig(proxy_config=...)`.
            *   **Site-Specific UAs:** If one site requires a very specific User-Agent not suitable for others.
    *   7.  **Session Management (`session_id`):**
        *   **Workflow: Maintaining a consistent browser state across multiple `arun()` calls:**
            1.  Choose a unique string for `session_id`.
            2.  On the first `arun()` call for this session, Crawl4ai creates a new browser page/tab associated with this ID.
            3.  On subsequent `arun()` calls with the *same* `session_id`, Crawl4ai reuses that existing page/tab, preserving its cookies, `localStorage`, and current URL (unless a new URL is provided in the `CrawlerRunConfig` for navigation).
            4.  Use `js_only=True` for these subsequent calls if you're just running JS on the *current* page of that session rather than navigating to a new URL.
            5.  When done with the session, explicitly kill it using `await crawler.browser_manager.kill_session(session_id)`.
        *   **How `session_id` interacts with `BrowserConfig.use_persistent_context`:**
            They are complementary. `use_persistent_context` (with `user_data_dir`) saves state *between crawler restarts*. `session_id` maintains state *within a single `AsyncWebCrawler` instance's lifetime* across multiple `arun` calls. You can use both: `use_persistent_context` to load an initial logged-in state, and then `session_id` to continue interacting with that state across several `arun` operations.
        *   **Best practices for managing session lifecycles:**
            *   Always kill sessions when they are no longer needed to free up browser resources.
            *   Use descriptive `session_id`s if managing multiple concurrent logical sessions.
        *   **Code Example: A multi-step form submission using the same `session_id`**
            ```python
            import asyncio
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

            async def multi_step_form():
                session_id = "form_submission_1"
                base_url = "https://example.com/multi-step-form" # Replace

                async with AsyncWebCrawler() as crawler:
                    # Step 1: Load first page of the form
                    config_step1 = CrawlerRunConfig(url=f"{base_url}?step=1", session_id=session_id)
                    result_step1 = await crawler.arun(config=config_step1)
                    print(f"Step 1 loaded: {result_step1.url}")

                    # Step 2: Fill first part, submit (JS might handle this, or a POST via strategy)
                    # For JS-driven submit on the same page:
                    js_fill_step1 = """
                        document.getElementById('field1').value = 'Value 1';
                        document.getElementById('nextButtonStep1').click();
                    """
                    config_step2_action = CrawlerRunConfig(
                        session_id=session_id, 
                        js_code=js_fill_step1,
                        js_only=True, # We are on the same page context
                        wait_for="css:#formStep2Indicator" # Wait for step 2 to appear
                    )
                    result_step2_load = await crawler.arun(config=config_step2_action) 
                    print(f"Step 2 loaded after JS action. Current URL: {result_step2_load.url}")
                    
                    # ... continue for other steps ...

                    # Finally, kill the session
                    await crawler.browser_manager.kill_session(session_id)
                    print(f"Session {session_id} killed.")

            if __name__ == "__main__":
                asyncio.run(multi_step_form())
            ```
    *   8.  **Robots.txt Handling (`check_robots_txt`):**
        *   **Understanding the importance of respecting `robots.txt`:**
            `robots.txt` is a standard used by websites to communicate with web crawlers about which parts of their site should not be accessed. Respecting these rules is crucial for ethical and legal crawling.
        *   **How Crawl4ai handles it:**
            If `check_robots_txt=True` (default is `False` in `CrawlerRunConfig` for flexibility, but often good practice to enable), Crawl4ai will:
            1.  Attempt to fetch `/robots.txt` for the domain.
            2.  Parse the rules relevant to its User-Agent (or a generic one if specific rules are absent).
            3.  If the target URL is disallowed, the crawl will be skipped, and the `CrawlResult` will indicate this (e.g., status code 403, error message).
            *   **When you might override it (with caution):**
                Disabling `check_robots_txt` (`False`) should only be done if you have explicit permission to crawl restricted areas or if you are certain `robots.txt` is misconfigured and blocking legitimate access. Always prioritize ethical crawling.

*   C. **Combining `CrawlerRunConfig` with `BrowserConfig`:**
    *   **Understanding precedence:** Settings in `CrawlerRunConfig` generally take precedence over those in `BrowserConfig` for the duration of that specific `arun()` call. For example, if `BrowserConfig` defines a default User-Agent, but `CrawlerRunConfig` also specifies a `user_agent`, the latter will be used for that run.
    *   **Examples of synergistic configurations:**
        *   `BrowserConfig` with a `user_data_dir` to load a logged-in profile.
        *   `CrawlerRunConfig` with `js_code` to navigate within the logged-in application and `screenshot=True` to capture the state of a specific dashboard.
        *   `BrowserConfig` with a default set of `extra_args` for browser hardening.
        *   `CrawlerRunConfig` with a specific `css_selector` to target only the main content of articles from various sites, using the same hardened browser setup.

*   D. **Best Practices for `CrawlerRunConfig`:**
    *   **Be Specific:** Tailor your `CrawlerRunConfig` to the specific requirements of the URL(s) you are crawling and the data you intend to extract.
    *   **Iterate and Test:** Start with a minimal `CrawlerRunConfig`. Test it. Add complexity (JS interactions, wait conditions, extraction strategies) step-by-step, testing at each stage.
    *   **Use `wait_for` Judiciously:** Effective `wait_for` conditions are key for dynamic content. Avoid overly long fixed delays; prefer waiting for specific elements or JS flags.
    *   **Manage `session_id` State:** If using `session_id`, ensure you have a clear understanding of the session's lifecycle and kill sessions when they are no longer needed.
    *   **Scope Content Wisely:** Use `css_selector` or `target_elements` to reduce processing overhead and improve the relevance of extracted data.

*   E. **Troubleshooting `CrawlerRunConfig` Issues:**
    *   **Content Not Being Extracted as Expected:**
        *   Verify your `css_selector` or `target_elements` are correct and unique using browser developer tools.
        *   Content might be loaded dynamically *after* your `js_code` or `wait_for` completes. Add more specific waits or delays.
        *   The structure of the page might be different from what your selectors expect.
    *   **Timeouts or Hangs During JS Execution or Waiting:**
        *   `js_code` might have an error or an infinite loop. Test it in the browser console.
        *   `wait_for` condition might never be met. Check the selector/JS expression and increase `wait_for_timeout` if necessary, or make the condition more robust.
        *   The page itself might be unresponsive or very slow.
    *   **Incorrect Media Capture:**
        *   Screenshots/PDFs might be taken too early. Use `wait_for` effectively before enabling capture.
        *   Content might be outside the configured viewport.
    *   **`CacheMode` Not Behaving as Expected:**
        *   Ensure you understand the differences between `ENABLED`, `BYPASS`, `READ_ONLY`, and `WRITE_ONLY`.
        *   Cache keys are based on the URL. If URL parameters change, it's a new cache entry.

# V. Interpreting Crawl Outputs: `CrawlResult` and Data Models

After a crawl operation completes, `AsyncWebCrawler.arun()` (or `arun_many()`) returns a `CrawlResult` object (or a `CrawlResultContainer` which wraps a list of `CrawlResult` objects). This object is a rich container holding all the data and metadata collected during the crawl.

*   A. **Understanding the `CrawlResult` Object:**
    *   **Navigating the key attributes:**
        *   `url` (str): The final URL crawled, after any redirects.
        *   `html` (str): The raw HTML content of the page as fetched.
        *   `cleaned_html` (Optional[str]): HTML after basic cleaning (scripts, styles removed) and potential scoping by `css_selector`. This is often the input for Markdown generation.
        *   `markdown` (Optional[MarkdownGenerationResult]): An object containing various Markdown outputs. See B.
        *   `extracted_content` (Optional[str]): JSON string (or other text) if an `ExtractionStrategy` was used and successfully extracted data.
        *   `media` (Dict[str, List[Dict]]): A dictionary containing lists of media items, keyed by type (e.g., "images", "videos", "audios", "tables").
        *   `links` (Dict[str, List[Dict]]): A dictionary containing lists of links, keyed by "internal" and "external".
        *   `metadata` (Optional[dict]): Page metadata (title, meta description, OpenGraph tags, etc.).
        *   `status_code` (Optional[int]): The HTTP status code of the final response.
        *   `success` (bool): `True` if the crawl and initial processing were successful.
        *   `error_message` (Optional[str]): Description of an error if `success` is `False`.
        *   `session_id` (Optional[str]): The session ID used for this crawl, if any.
        *   `response_headers` (Optional[dict]): HTTP response headers.
        *   `ssl_certificate` (Optional[SSLCertificate]): SSL certificate information if requested.
        *   `mhtml_data` (Optional[str]): MHTML snapshot if `capture_mhtml=True`.
        *   `screenshot` (Optional[str]): Base64 encoded screenshot if `screenshot=True`.
        *   `pdf_data` (Optional[bytes]): Raw PDF bytes if `pdf=True`.
        *   `downloaded_files` (Optional[List[str]]): List of paths to downloaded files if downloads occurred.
        *   `js_execution_result` (Optional[Dict[str, Any]]): Result of the last executed JS snippet if it returned a value.
    *   **How different `CrawlerRunConfig` settings populate these fields:**
        *   `css_selector`, `target_elements`, `excluded_tags`: Affect `cleaned_html` and subsequently `markdown`.
        *   `extraction_strategy`: Populates `extracted_content`.
        *   `screenshot`, `pdf`, `capture_mhtml`: Populate their respective fields.
        *   `fetch_ssl_certificate`: Populates `ssl_certificate`.

*   B. **Working with `MarkdownGenerationResult`:**
    The `result.markdown` attribute (if Markdown generation occurred) is an instance of `MarkdownGenerationResult`.
    *   `raw_markdown` (str): Markdown generated from `cleaned_html` *before* any `ContentFilter` (like `PruningContentFilter`) is applied.
    *   `markdown_with_citations` (str): `raw_markdown` but with inline links converted to citation style (e.g., `[text](1)`) if `citations=True` in the generator.
    *   `references_markdown` (str): The list of numbered references corresponding to the citations.
    *   `fit_markdown` (Optional[str]): Markdown generated from HTML that has been processed by a `ContentFilter`. This usually represents the most "relevant" content. If no filter was used, this might be empty or same as `raw_markdown`.
    *   `fit_html` (Optional[str]): The HTML content *after* a `ContentFilter` has been applied, which was then used to generate `fit_markdown`.
    *   **When to use `fit_markdown`:** This is often your desired output if you've configured a content filter to isolate the core article or relevant sections, as it's the most distilled version.
    *   **Code Example:**
        ```python
        if result.success and result.markdown:
            print("--- Raw Markdown (Snippet) ---")
            print(result.markdown.raw_markdown[:300])
            
            if result.markdown.fit_markdown:
                print("\n--- Fit Markdown (More Relevant Content - Snippet) ---")
                print(result.markdown.fit_markdown[:300])
            
            if result.markdown.references_markdown:
                print("\n--- References ---")
                print(result.markdown.references_markdown)
        ```

*   C. **Leveraging `ScrapingResult` (from `ContentScrapingStrategy`):**
    The `ScrapingResult` is an intermediate object produced by the `ContentScrapingStrategy` (e.g., `WebScrapingStrategy`). Its fields are then used to populate the main `CrawlResult`.
    *   `cleaned_html`: This becomes `CrawlResult.cleaned_html`.
    *   `media`: This dictionary populates `CrawlResult.media`.
    *   `links`: This dictionary populates `CrawlResult.links`.
    *   `metadata`: This populates `CrawlResult.metadata`.
    Understanding this helps if you're creating a custom scraping strategy, as you'll need to return a `ScrapingResult` object.

*   D. **Understanding `MediaItem` and `Link` Structures:**
    *   **Common fields in `MediaItem` (for images, videos, audio):**
        *   `src` (str): The source URL of the media.
        *   `alt` (Optional[str]): Alt text (primarily for images).
        *   `type` (str): "image", "video", or "audio".
        *   `score` (Optional[int]): Heuristic score indicating relevance (if scoring is applied).
        *   `desc` (Optional[str]): Text from nearby elements, potentially describing the image.
        *   `group_id` (Optional[int]): If part of a group of related images (e.g., variants from `srcset`).
        *   `format` (Optional[str]): Detected format (e.g., "jpeg", "png").
        *   `width` (Optional[int]): Detected width.
        *   `data` (Optional[str]): Base64 encoded data if it's a data URI.
    *   **Common fields in `Link`:**
        *   `href` (str): The target URL of the link.
        *   `text` (Optional[str]): The anchor text of the link.
        *   `title` (Optional[str]): The `title` attribute of the link.
        *   `base_domain` (Optional[str]): The base domain of the `href`.
    *   **Practical examples of iterating and processing these collections:**
        ```python
        if result.success:
            # Example: Filter high-score images or JPEGs
            high_quality_images = [
                img for img in result.media.get("images", [])
                if (img.get("score", 0) > 3 and img.get("format") == "jpeg") or \
                   (img.get("score", 0) > 3 and img.get("src", "").lower().endswith(".jpg"))
            ]
            print(f"\nFound {len(high_quality_images)} high-quality JPEGs:")
            for img in high_quality_images[:2]: # Show first 2
                print(f"  - Src: {img.get('src')}, Alt: {img.get('alt', 'N/A')}")

            # Example: Extract all PDF links
            pdf_links = [
                link.get("href") for link in result.links.get("internal", []) + result.links.get("external", [])
                if link.get("href", "").lower().endswith(".pdf")
            ]
            if pdf_links:
                print(f"\nFound {len(pdf_links)} PDF links:")
                for pdf_url in pdf_links[:2]:
                    print(f"  - {pdf_url}")
        ```

*   E. **Making Sense of `AsyncCrawlResponse` (from `AsyncCrawlerStrategy`):**
    This object is returned by the `AsyncCrawlerStrategy.crawl()` method and is more low-level than `CrawlResult`. It typically contains:
    *   `html` (str): The raw HTML.
    *   `response_headers` (Dict): HTTP headers from the server's response.
    *   `status_code` (int): HTTP status code.
    *   `screenshot` (Optional[str]): Base64 screenshot data.
    *   `pdf_data` (Optional[bytes]): PDF data.
    *   `mhtml_data` (Optional[str]): MHTML data.
    *   And potentially other strategy-specific fields.
    The `AsyncWebCrawler` uses the information from `AsyncCrawlResponse` to construct the more comprehensive `CrawlResult`. You'd typically interact with `AsyncCrawlResponse` only if you are developing a custom `AsyncCrawlerStrategy`.

*   F. **Tracking Costs with `TokenUsage` (for LLM-based strategies):**
    *   When using strategies that involve Large Language Models (e.g., `LLMExtractionStrategy`, `LLMContentFilter`), the `TokenUsage` object provides a breakdown of token consumption.
    *   **Where to find it:**
        *   For `LLMExtractionStrategy`: The `run()` method of the strategy itself often returns a list of results, and each result or an aggregated summary might contain `TokenUsage`. The exact location can depend on the specific LLM provider integration within the strategy. It's often part of the metadata returned by the LLM API call.
        *   For `LLMContentFilter`: Similarly, the filter might expose token usage details.
        *   It's not directly a field in `CrawlResult` unless a specific extraction strategy adds it to `extracted_content` or `metadata`. You'd typically access it from the strategy's direct output or by inspecting logs if the strategy logs token usage.
    *   **Importance:** Crucial for monitoring and managing costs associated with LLM API calls. `TokenUsage` usually includes `prompt_tokens`, `completion_tokens`, and `total_tokens`.

*   G. **Inspecting `SSLCertificate` Information:**
    *   **How to access:** If `CrawlerRunConfig(fetch_ssl_certificate=True)` is set, the `CrawlResult.ssl_certificate` attribute will be an instance of `SSLCertificate`.
    *   **Use cases:**
        *   **Security Audits:** Programmatically check certificate issuers, validity periods, or signature algorithms.
        *   **Verifying Certificate Validity:** Ensure the certificate is not expired and is issued by a trusted authority.
        *   **Data Collection:** Gather data on SSL/TLS deployment across a set of websites.
    *   The `SSLCertificate` object (as seen in `crawl4ai/ssl_certificate.py`) provides properties like `.issuer`, `.subject`, `.valid_from`, `.valid_until`, `.fingerprint`, and methods like `.to_pem()`, `.to_der()`, `.to_json()`.
        ```python
        if result.success and result.ssl_certificate:
            cert = result.ssl_certificate
            print(f"\nSSL Certificate for {result.url}:")
            print(f"  Issuer CN: {cert.issuer.get('CN', 'N/A')}")
            print(f"  Subject CN: {cert.subject.get('CN', 'N/A')}")
            print(f"  Valid until: {cert.valid_until}") # Already a decoded string
            # cert.to_pem(path="certificate.pem") # Example of saving
        ```

# VI. Core Crawler Strategies: `AsyncPlaywrightCrawlerStrategy` and `AsyncHTTPCrawlerStrategy`

Crawl4ai's power comes from its flexible strategy pattern. At the core of fetching web content are the "Crawler Strategies." You primarily interact with these through `AsyncWebCrawler`, but understanding their differences and capabilities helps in choosing the right tool for the job and customizing behavior.

*   A. **Choosing the Right Strategy:**
    The `AsyncWebCrawler` is initialized with a crawler strategy. If none is provided, `AsyncPlaywrightCrawlerStrategy` is the default.
    *   **`AsyncPlaywrightCrawlerStrategy` (Default):**
        *   **Pros:**
            *   **Full JavaScript Rendering:** Executes JavaScript on the page, just like a real browser. Essential for modern SPAs (Single Page Applications) and sites that load content dynamically.
            *   **Complex Interactions:** Can handle clicks, form submissions, scrolling, and other user-like interactions via `js_code` or hooks.
            *   **Robust:** Benefits from Playwright's mature browser automation capabilities.
            *   **Media Capture:** Natively supports screenshots, PDFs, and MHTML.
        *   **Cons:**
            *   **Higher Resource Usage:** Launching and managing a full browser instance is more memory and CPU intensive.
            *   **Slower than HTTP:** Browser rendering and JS execution take time.
        *   **When it's the best choice:**
            *   Most modern websites that rely heavily on JavaScript.
            *   When you need to interact with the page (e.g., click buttons, fill forms) to access content.
            *   When you need to capture screenshots or PDFs accurately reflecting the rendered page.
    *   **`AsyncHTTPCrawlerStrategy`:**
        *   **Pros:**
            *   **Lightweight & Very Fast:** Makes direct HTTP requests using libraries like `requests` or `httpx`. Skips browser rendering entirely.
            *   **Low Resource Usage:** Minimal memory and CPU footprint.
            *   Good for high-volume, simple fetches.
        *   **Cons:**
            *   **No JavaScript Execution:** Only retrieves the initial HTML response from the server. Dynamic content loaded by client-side JS will be missing.
            *   **Limited Interaction:** Cannot perform clicks, scrolls, or form submissions in the browser context.
            *   Screenshots/PDFs are not applicable as there's no rendering.
        *   **When it's a suitable alternative:**
            *   Fetching data from APIs that return JSON or XML.
            *   Crawling purely static HTML websites where all content is present in the initial server response.
            *   Parsing sitemaps or `robots.txt` files.
            *   When speed and resource efficiency are paramount, and JS rendering is not required.
    *   **Decision Guide: Playwright vs. HTTP:**
        1.  **Does the target content require JavaScript to load/render?**
            *   **Yes:** Use `AsyncPlaywrightCrawlerStrategy`.
            *   **No:** `AsyncHTTPCrawlerStrategy` is a good candidate.
        2.  **Do you need to interact with the page (clicks, forms, scrolls)?**
            *   **Yes:** Use `AsyncPlaywrightCrawlerStrategy`.
            *   **No:** `AsyncHTTPCrawlerStrategy` might be sufficient.
        3.  **Are you fetching from an API endpoint that returns structured data (JSON/XML)?**
            *   **Yes:** `AsyncHTTPCrawlerStrategy` is ideal.
        4.  **Is performance (requests/second) and low resource use a top priority for simple HTML pages?**
            *   **Yes:** `AsyncHTTPCrawlerStrategy`.
        5.  **Do you need screenshots or PDFs of the rendered page?**
            *   **Yes:** `AsyncPlaywrightCrawlerStrategy`.
        
        *If unsure, start with `AsyncPlaywrightCrawlerStrategy` as it's more versatile, then consider `AsyncHTTPCrawlerStrategy` if you find JS rendering is unnecessary for your target.*

*   B. **Deep Dive into `AsyncPlaywrightCrawlerStrategy` Hooks:**
    Hooks are a powerful feature of `AsyncPlaywrightCrawlerStrategy` that allow you to inject custom asynchronous code at various stages of the crawling lifecycle for a specific page context.
    *   1.  **Purpose and Power of Hooks:**
        *   **Customization:** Tailor browser behavior beyond what standard configuration options offer.
        *   **Interaction:** Perform complex page interactions that are difficult to express with simple `js_code`.
        *   **State Management:** Set up or clean up page/context state.
        *   **Conditional Logic:** Implement dynamic logic based on page content or state during the crawl.
    *   2.  **Available Hooks and Their Triggers:**
        *(Refer to `async_crawler_strategy.py` for exact signatures; parameters often include `page`, `context`, `url`, `response`, `config`, `shared_data` etc.)*
        *   `on_browser_created(browser, **kwargs)`: Called once after the Playwright `Browser` object is created but before any `BrowserContext` is made. Useful for global browser setup, but limited as there's no page yet.
        *   `on_page_context_created(page: Page, context: BrowserContext, **kwargs)`: Called right after a new `BrowserContext` and its initial `Page` are created. This is the **most common and recommended hook for setup tasks** like authentication, setting cookies, or configuring routes, as you have both `page` and `context` available.
        *   `before_goto(page: Page, context: BrowserContext, url: str, **kwargs)`: Called just before `page.goto(url)` is executed. Useful for setting request interception or last-minute header modifications specific to this navigation.
        *   `after_goto(page: Page, context: BrowserContext, url: str, response, **kwargs)`: Called after `page.goto(url)` completes successfully (i.e., navigation didn't fail). `response` is the Playwright `Response` object. Good for initial checks post-navigation.
        *   `on_user_agent_updated(page: Page, context: BrowserContext, user_agent: str, **kwargs)`: Triggered if the user agent is changed dynamically during the crawl lifecycle (less common for standard crawls).
        *   `on_execution_started(page: Page, context: BrowserContext, **kwargs)`: Called just before any `js_code` specified in `CrawlerRunConfig` is executed on the page.
        *   `before_retrieve_html(page: Page, context: BrowserContext, **kwargs)`: Called right before the final HTML content of the page is snapshot. This is your last chance to perform interactions that might alter the DOM before it's captured.
        *   `before_return_html(page: Page, context: BrowserContext, html: str, **kwargs)`: Called just before the `AsyncCrawlResponse` is constructed. You have access to the `html` string that's about to be returned. Useful for final HTML modifications if absolutely necessary, though generally, it's better to do this in a `ContentScrapingStrategy`.
    *   3.  **Common Workflows Using Hooks:**
        *   **Workflow: Implementing a login sequence using `on_page_context_created`:**
            This is the ideal hook for logins as it runs once per new context (or session).
            *   **Code Example:**
                ```python
                import asyncio
                from playwright.async_api import Page, BrowserContext
                from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, AsyncPlaywrightCrawlerStrategy

                async def login_hook(page: Page, context: BrowserContext, shared_data: dict, **kwargs):
                    print("[HOOK login_hook] Navigating to login page...")
                    await page.goto(shared_data["login_url"])
                    await page.fill("input[name='username']", shared_data["username"])
                    await page.fill("input[name='password']", shared_data["password"])
                    await page.click("button[type='submit']")
                    # Wait for a post-login element to ensure success
                    await page.wait_for_selector("#dashboard-welcome", timeout=10000) 
                    print("[HOOK login_hook] Login successful!")
                    # Cookies are now set in the context and will be used for subsequent requests

                async def main():
                    # Create a BrowserConfig that will use our hook
                    strategy = AsyncPlaywrightCrawlerStrategy()
                    strategy.set_hook("on_page_context_created", login_hook)
                    
                    # Use a persistent context to potentially save the login cookies across crawler restarts
                    # PROFILE_DIR_HOOK = Path("./hook_login_profile")
                    # PROFILE_DIR_HOOK.mkdir(parents=True, exist_ok=True)

                    browser_cfg = BrowserConfig(
                        headless=False, # Easier to debug login
                        # user_data_dir=str(PROFILE_DIR_HOOK), # Optional: for persistence
                        # use_persistent_context=True      # Optional: for persistence
                    )
                    
                    # Pass login credentials and URL via shared_data
                    run_cfg = CrawlerRunConfig(
                        url="https://example.com/protected-page", # Target page after login
                        shared_data={
                            "login_url": "https://example.com/login", # Replace
                            "username": "testuser",
                            "password": "testpassword"
                        }
                    )

                    async with AsyncWebCrawler(config=browser_cfg, crawler_strategy=strategy) as crawler:
                        result = await crawler.arun(config=run_cfg)
                        if result.success:
                            print(f"Crawled protected page content: {result.markdown.raw_markdown[:200]}...")
                        else:
                            print(f"Failed: {result.error_message}")
                
                if __name__ == "__main__":
                    asyncio.run(main())
                ```
        *   **Workflow: Setting custom cookies or headers for all requests in a context via `on_page_context_created`:**
            ```python
            async def set_custom_headers_hook(page: Page, context: BrowserContext, **kwargs):
                await context.set_extra_http_headers({"X-Custom-Auth": "mysecrettoken"})
                print("[HOOK set_custom_headers_hook] Custom headers set.")
            
            # strategy.set_hook("on_page_context_created", set_custom_headers_hook)
            ```
        *   **Workflow: Blocking specific resource types (e.g., fonts, images) using `page.route()` in `on_page_context_created`:**
            This can speed up page loads if you don't need these resources.
            ```python
            async def block_resources_hook(page: Page, context: BrowserContext, **kwargs):
                await context.route("**/*.{png,jpg,jpeg,woff,woff2}", lambda route: route.abort())
                print("[HOOK block_resources_hook] Aborting image and font requests.")

            # strategy.set_hook("on_page_context_created", block_resources_hook)
            ```
        *   **Workflow: Performing complex page interactions before HTML retrieval using `before_retrieve_html`:**
            Useful if content is revealed only after a sequence of actions not easily done with simple `js_code`.
            ```python
            async def reveal_content_hook(page: Page, context: BrowserContext, **kwargs):
                print("[HOOK reveal_content_hook] Clicking multiple expanders...")
                expanders = await page.query_selector_all(".accordion-header")
                for expander in expanders:
                    await expander.click()
                    await page.wait_for_timeout(100) # Small delay for content to render
            
            # strategy.set_hook("before_retrieve_html", reveal_content_hook)
            ```
    *   4.  **Best Practices for Writing Hook Functions:**
        *   **Keep them Focused:** Each hook should perform a specific, well-defined task.
        *   **Handle Errors Gracefully:** Use `try-except` blocks within hooks to catch potential errors (e.g., element not found, navigation failure) and prevent them from crashing the entire crawl. Log errors appropriately.
        *   **Async/Await:** Hooks are `async` functions. Ensure you `await` any Playwright calls or other asynchronous operations within them.
        *   **Idempotency (where applicable):** If a hook might be called multiple times for the same context (less common but possible in complex scenarios), design it to be idempotent if its actions are stateful (e.g., don't try to set the same route handler multiple times without removing the old one).
    *   5.  **Passing Shared Data to Hooks (`CrawlerRunConfig.shared_data`):**
        *   The `shared_data` dictionary in `CrawlerRunConfig` is passed as keyword arguments to your hook functions.
        *   This is the recommended way to pass dynamic information (like credentials, API keys, or state from previous steps) into your hooks without hardcoding them or relying on global variables.
        *   **Code Example:** (Shown in the login hook example above where `shared_data` passes `login_url`, `username`, `password`).

*   C. **Using `AsyncHTTPCrawlerStrategy`:**
    When you don't need JavaScript rendering, `AsyncHTTPCrawlerStrategy` offers a much faster and lighter alternative.
    *   **Simpler Configuration:** Many `BrowserConfig` options (like `headless`, `viewport`) are irrelevant. The core HTTP request details are controlled via `HTTPCrawlerConfig` passed within `CrawlerRunConfig`.
    *   **When to use `HTTPCrawlerConfig` for `method`, `headers`, `data`, `json`:**
        *   `method`: "GET" (default), "POST", "PUT", etc.
        *   `headers`: Custom HTTP headers for the request.
        *   `data`: For form-encoded POST data (`application/x-www-form-urlencoded`).
        *   `json`: For JSON POST data (`application/json`).
    *   **Code Example: Fetching JSON from an API using `AsyncHTTPCrawlerStrategy` and `HTTPCrawlerConfig`**
        ```python
        import asyncio
        import json
        from crawl4ai import (
            AsyncWebCrawler, CrawlerRunConfig, AsyncHTTPCrawlerStrategy, HTTPCrawlerConfig
        )

        async def fetch_api_data():
            # HTTPCrawlerConfig for a POST request with JSON payload
            http_cfg = HTTPCrawlerConfig(
                method="POST",
                headers={"Authorization": "Bearer myapitoken"},
                json_data={"query": "fetch_user_data", "user_id": 123}
            )

            # CrawlerRunConfig specific to this API call
            run_cfg = CrawlerRunConfig(
                url="https://api.example.com/data",
                http_crawler_config=http_cfg # Pass the HTTP-specific config
            )
            
            # Initialize crawler with AsyncHTTPCrawlerStrategy
            http_strategy = AsyncHTTPCrawlerStrategy()
            async with AsyncWebCrawler(crawler_strategy=http_strategy) as crawler:
                result = await crawler.arun(config=run_cfg)
                if result.success and result.status_code == 200:
                    try:
                        api_data = json.loads(result.html) # The 'html' field contains the raw response body
                        print("API Data:", api_data)
                    except json.JSONDecodeError:
                        print("Failed to parse JSON response:", result.html)
                else:
                    print(f"API call failed. Status: {result.status_code}, Error: {result.error_message}")
                    print(f"Response body: {result.html}")


        if __name__ == "__main__":
            asyncio.run(fetch_api_data())
        ```

# VII. Advanced Core Concepts and Workflows

Beyond basic crawling, Crawl4ai's core offers mechanisms for more sophisticated scenarios, including robust proxy management, fine-grained browser control, and configuration persistence.

*   A. **Proxy Management and Rotation:**
    While single proxies can be set in `BrowserConfig` or `CrawlerRunConfig`, robust applications often require rotating through a list of proxies to avoid IP bans and access geo-restricted content.
    *   1.  **Using `ProxyConfig` for Single Proxies:**
        (Recap from `BrowserConfig` and `CrawlerRunConfig` sections for completeness)
        -   **Purpose:** To route a specific `AsyncWebCrawler` instance or a particular `arun()` call through a designated proxy server.
        -   **How:**
            ```python
            from crawl4ai import BrowserConfig, CrawlerRunConfig, ProxyConfig
            
            # For all crawls by a crawler instance
            browser_cfg_proxy = BrowserConfig(
                proxy_config=ProxyConfig(server="http://user:pass@proxy1.example.com:8000")
            )
            # crawler = AsyncWebCrawler(config=browser_cfg_proxy)

            # For a single arun() call
            run_cfg_proxy = CrawlerRunConfig(
                url="https://example.com",
                proxy_config=ProxyConfig(server="http://user:pass@proxy2.example.com:8001")
            )
            # result = await crawler.arun(config=run_cfg_proxy)
            ```
        -   **Why:** Useful for testing with a specific proxy or when you have a stable, dedicated proxy for certain tasks.
    *   2.  **Implementing Proxy Rotation with `ProxyRotationStrategy`:**
        *   **Understanding the need for proxy rotation:**
            Websites often track and block IPs that make too many requests. Rotating proxies distributes your requests across multiple IPs, making your crawler appear less like a bot and reducing the likelihood of blocks.
        *   **How `RoundRobinProxyStrategy` works:**
            This is the primary built-in strategy. You provide it with a list of `ProxyConfig` objects. For each new crawl operation (or more granularly, depending on how the strategy is integrated), it cycles through this list, selecting the next proxy in a round-robin fashion.
        *   **Workflow: Setting up `RoundRobinProxyStrategy`:**
            1.  Define a list of `ProxyConfig` objects, each representing one proxy server.
            2.  Instantiate `RoundRobinProxyStrategy` with this list.
            3.  Assign this strategy instance to `CrawlerRunConfig.proxy_rotation_strategy`.
        *   **Code Example: Using `RoundRobinProxyStrategy` in `CrawlerRunConfig`**
            (Note: `ProxyRotationStrategy` is typically used with `arun_many` or in custom loops with `arun` where the `CrawlerRunConfig` is updated before each call).
            ```python
            import asyncio
            from crawl4ai import (
                AsyncWebCrawler, CrawlerRunConfig, ProxyConfig, 
                RoundRobinProxyStrategy, BrowserConfig
            )

            async def crawl_with_rotating_proxies():
                proxies = [
                    ProxyConfig(server="http://user1:pass1@proxy1.example.com:8000", ip="1.1.1.1"),
                    ProxyConfig(server="http://user2:pass2@proxy2.example.com:8001", ip="2.2.2.2"),
                    ProxyConfig(server="http://user3:pass3@proxy3.example.com:8002", ip="3.3.3.3"),
                ]
                
                proxy_strategy = RoundRobinProxyStrategy(proxies=proxies)

                # This run_config will now use the rotation strategy.
                # The proxy_config within it will be set dynamically by the strategy.
                run_cfg = CrawlerRunConfig(
                    url="https://api.ipify.org?format=json", # A site to check current IP
                    proxy_rotation_strategy=proxy_strategy
                )
                
                # Browser config remains simple or can have its own default proxy (which would be overridden)
                browser_cfg = BrowserConfig(headless=True)

                async with AsyncWebCrawler(config=browser_cfg) as crawler:
                    for i in range(5): # Make 5 requests to see rotation
                        print(f"\nRequest {i+1}:")
                        # The strategy will pick the next proxy for each arun() call
                        # when proxy_rotation_strategy is set on the CrawlerRunConfig.
                        # The proxy_config field in run_cfg is effectively managed by the strategy.
                        
                        # It's more typical to see proxy_rotation_strategy used with arun_many,
                        # where the dispatcher would call get_next_proxy for each task.
                        # For a loop with arun(), you'd typically call get_next_proxy manually:
                        
                        next_proxy = await proxy_strategy.get_next_proxy()
                        current_run_cfg = run_cfg.clone(proxy_config=next_proxy) # Clone and set specific proxy

                        result = await crawler.arun(config=current_run_cfg)
                        if result.success:
                            print(f"  Crawled {result.url} via proxy {next_proxy.server if next_proxy else 'None'}")
                            print(f"  Response: {result.html[:100]}")
                        else:
                            print(f"  Failed to crawl {result.url}: {result.error_message}")
                        await asyncio.sleep(0.5) # Small delay between requests

            if __name__ == "__main__":
                asyncio.run(crawl_with_rotating_proxies())
            ```
            *Important Note:* The `ProxyRotationStrategy` itself usually doesn't automatically apply the proxy to each `arun` call if you're looping. It's designed to be queried (e.g., `await strategy.get_next_proxy()`) and then that specific `ProxyConfig` is passed to the `CrawlerRunConfig` for *that particular* `arun` call. `AsyncDispatcher` in `arun_many` handles this querying internally.

        *   **Considerations for managing proxy lists and health:**
            *   Real-world proxy lists can be large and proxies can become unavailable.
            *   You might need external logic to check proxy health and update the list passed to `RoundRobinProxyStrategy` periodically or implement a more advanced strategy that handles proxy failures (e.g., by temporarily removing a failing proxy from rotation).

*   B. **Browser and Session Management (`BrowserManager`, `ManagedBrowser`):**
    These classes are mostly for internal use by `AsyncPlaywrightCrawlerStrategy` but understanding their concepts can be beneficial.
    *   1.  **Understanding `BrowserManager`'s Role:**
        *   The `BrowserManager` (an internal component of `AsyncPlaywrightCrawlerStrategy`) is responsible for the lifecycle of Playwright `BrowserContext` objects.
        *   It handles creating new contexts based on the `BrowserConfig` and the specifics of a `CrawlerRunConfig` (like `session_id` or per-run proxy/UA overrides).
        *   **Context Signatures:** It generates a "signature" for each requested context configuration. If a subsequent request has the same signature (e.g., same `session_id`, same identity overrides), it can reuse an existing, compatible `BrowserContext` instead of creating a new one, which is efficient.
        *   It also manages the closing of contexts when a session is killed or the crawler is shut down.
    *   2.  **Leveraging `ManagedBrowser` (typically internal but concepts are useful):**
        *   `ManagedBrowser` is used when `BrowserConfig.use_managed_browser` is `True` (e.g., for `"builtin"` or `"docker"` modes, or when `use_persistent_context=True`).
        *   It's responsible for launching and managing the actual browser *process* (e.g., Chrome executable).
        *   **Persistent Profiles:** When `user_data_dir` is set, `ManagedBrowser` launches the browser with that profile directory, enabling persistence.
        *   **CDP Connection:** If `cdp_url` is provided in `BrowserConfig` (for `browser_mode="cdp"`), `ManagedBrowser` is bypassed, and Playwright connects directly to the existing browser endpoint. If `ManagedBrowser` launches the browser, it makes the CDP endpoint available (e.g., `ws://localhost:9222/...`).
    *   3.  **Explicit Session Termination with `crawler.browser_manager.kill_session(session_id)`:**
        *   **When and why:** If you've used a `session_id` in `CrawlerRunConfig` to maintain a specific page/tab across multiple `arun` calls, you should explicitly kill that session when it's no longer needed.
        *   **Impact:** This closes the associated browser page and potentially the `BrowserContext` (if no other pages are using it and it's not the default shared context), freeing up browser resources. If you don't kill sessions, they might linger until the entire `AsyncWebCrawler` is closed.
        *   Code Example:
            ```python
            # ... (inside an async with AsyncWebCrawler as crawler:)
            # await crawler.arun(config=CrawlerRunConfig(url="...", session_id="my_task_session"))
            # ...
            # await crawler.arun(config=CrawlerRunConfig(url="...", session_id="my_task_session", js_only=True, ...))
            # ...
            # Done with this logical task
            # await crawler.browser_manager.kill_session("my_task_session")
            ```

*   C. **Configuration Serialization and Deserialization (`to_serializable_dict`, `from_serializable_dict`):**
    `BrowserConfig`, `CrawlerRunConfig`, and their nested config objects (like `ProxyConfig`, `GeolocationConfig`) provide methods for easy serialization and deserialization. This is useful for:
    *   Storing complex configurations in files (e.g., JSON, YAML).
    *   Sharing configurations between different parts of an application or different scripts.
    *   Dynamically loading configurations at runtime.
    *   **Workflow: Saving and Loading Configurations:**
        1.  Create your config object (e.g., `my_run_config = CrawlerRunConfig(...)`).
        2.  Get a serializable dictionary: `serializable_dict = my_run_config.to_dict()` (or use `my_run_config.dump()` which leverages `to_serializable_dict` from `async_configs.py`).
        3.  Save this dictionary to a file (e.g., using `json.dump()`).
        4.  To load, read the dictionary from the file.
        5.  Recreate the config object: `loaded_run_config = CrawlerRunConfig.from_kwargs(loaded_dict)` (or use `CrawlerRunConfig.load(loaded_dict)`).
    *   **Code Example: Saving and loading a `CrawlerRunConfig` object**
        ```python
        import json
        import asyncio
        from crawl4ai import CrawlerRunConfig, CacheMode, GeolocationConfig, ProxyConfig
        from crawl4ai.async_configs import to_serializable_dict # For direct use if needed
        
        # Create a complex CrawlerRunConfig
        original_config = CrawlerRunConfig(
            url="https://example.com",
            cache_mode=CacheMode.ENABLED,
            js_code="console.log('hello');",
            screenshot=True,
            geolocation=GeolocationConfig(latitude=34.05, longitude=-118.24),
            proxy_config=ProxyConfig(server="http://myproxy.com:3128")
        )

        # 1. Serialize to a dictionary (using the .dump() method which calls to_serializable_dict internally)
        # config_dict = original_config.dump() 
        # Or, if using the utility function directly on an object not having .dump()
        config_dict = to_serializable_dict(original_config, ignore_default_value=True)


        # 2. Save to JSON file
        with open("my_crawler_run_config.json", "w") as f:
            json.dump(config_dict, f, indent=2)
        print("Config saved to my_crawler_run_config.json")

        # 3. Load from JSON file
        with open("my_crawler_run_config.json", "r") as f:
            loaded_dict = json.load(f)
        
        # 4. Recreate CrawlerRunConfig object (using .load() static method)
        # loaded_config = CrawlerRunConfig.load(loaded_dict)
        # Or using from_kwargs if .load() is not available or for more general objects
        from crawl4ai.async_configs import from_serializable_dict
        loaded_config_generic = from_serializable_dict(loaded_dict) # This will give back the CrawlerRunConfig instance

        # Verify (loaded_config_generic will be an instance of CrawlerRunConfig)
        assert isinstance(loaded_config_generic, CrawlerRunConfig)
        assert loaded_config_generic.url == original_config.url
        assert loaded_config_generic.cache_mode == original_config.cache_mode
        print(f"Loaded config successfully. Screenshot setting: {loaded_config_generic.screenshot}")
        print(f"Loaded proxy server: {loaded_config_generic.proxy_config.server if loaded_config_generic.proxy_config else 'None'}")

        # Example of using it
        # async with AsyncWebCrawler() as crawler:
        #     result = await crawler.arun(config=loaded_config_generic)
        #     # ...
        ```
    *   **Benefits:** This structured approach is more robust than pickling and ensures that only intended configuration parameters are serialized, making it easier to manage versions and share configurations.

*   D. **Logging and Debugging Core Operations:**
    *   **Leveraging `AsyncLogger`:**
        Crawl4ai uses an `AsyncLogger` instance (by default, or you can pass your own). This logger provides tagged and colored output, making it easier to follow the flow of operations.
        *   Configure verbosity via `BrowserConfig(verbose=True)` or `CrawlerRunConfig(verbose=True)`. The `CrawlerRunConfig` setting usually takes precedence for a specific run.
    *   **Interpreting verbose logs for troubleshooting:**
        Verbose logs will show:
        *   [INIT] tags for crawler and strategy initialization.
        *   [BROWSER] tags for browser launching, context creation, page events.
        *   [FETCH] tags for URL fetching attempts, cache hits/misses, status codes.
        *   [SCRAPE] tags for scraping strategy execution.
        *   [MARKDOWN] tags for markdown generation steps.
        *   [EXTRACTION] tags if an extraction strategy is active.
        *   [JS] tags for JavaScript execution.
        *   [HOOK] tags when custom hooks are executed.
        *   [ERROR] tags for any exceptions or failures.
        *   [CACHE] tags for cache operations.
    *   **Key log messages to watch for:**
        *   Browser launch arguments.
        *   CDP connection messages (if using managed/CDP modes).
        *   Cache hits/misses: `Cache HIT for <URL>` or `Cache MISS for <URL>`.
        *   Navigation events: `Navigating to <URL>...`, `Navigation to <URL> successful`.
        *   JavaScript execution: `Executing JS...`, `JS execution result...`.
        *   Hook execution: `Executing hook <hook_name>...`.
        *   Any messages tagged with [ERROR] or [WARNING].

# VIII. Integrating Core with Specialized Strategies

The `core` components of Crawl4ai (like `AsyncWebCrawler`, `BrowserConfig`, `CrawlerRunConfig`) provide the foundation upon which more specialized strategies for PDF processing, Markdown generation, data extraction, chunking, and content filtering are built. Understanding how core configurations influence these specialized strategies is key to using them effectively.

*   A. **How Core Configurations Influence Specialized Crawlers/Processors:**
    *   **`PDFCrawlerStrategy` & `PDFContentScrapingStrategy`:**
        *   The `PDFCrawlerStrategy` itself often uses an underlying `AsyncHTTPCrawlerStrategy` to download the PDF file because PDFs are typically static assets not requiring browser rendering for the download itself.
        *   Thus, `BrowserConfig` settings like `headless` or `js_code` are usually *not directly relevant* to the PDF fetching part.
        *   However, `HTTPCrawlerConfig` (passed via `CrawlerRunConfig.http_crawler_config`) for setting custom headers (e.g., authentication) for the PDF download request can be important.
        *   The `PDFContentScrapingStrategy` (used by `PDFCrawlerStrategy` and can be used standalone with `AsyncWebCrawler` if you point it to a PDF URL with the right Content-Type) then processes the downloaded PDF bytes. Core configs like `word_count_threshold` or `css_selector` are *not applicable* here as it's not HTML. PDF-specific parameters for this strategy (like `save_images_locally`, `extract_images`) are passed during its initialization.
    *   **`MarkdownGenerationStrategy` (e.g., `DefaultMarkdownGenerator`):**
        *   This strategy consumes HTML, typically `CrawlResult.cleaned_html` or `CrawlResult.fit_html` (if a `content_filter` was used).
        *   Therefore, `CrawlerRunConfig` parameters that affect `cleaned_html` (like `css_selector`, `excluded_tags`, `target_elements`) directly influence the input to the Markdown generator. A well-scoped `cleaned_html` leads to more relevant Markdown.
        *   The `content_filter` *within* the `MarkdownGenerationStrategy` (e.g., `PruningContentFilter`) further refines the HTML before final Markdown conversion, producing `fit_markdown`.
    *   **`ExtractionStrategy` (e.g., `LLMExtractionStrategy`, `JsonCSSExtractionStrategy`, `RegexExtractionStrategy`):**
        *   This is typically set via `CrawlerRunConfig.extraction_strategy`.
        *   The `input_format` property of the chosen `ExtractionStrategy` determines what content it receives from `CrawlResult`. Common options are:
            *   `"markdown"`: Uses `CrawlResult.markdown.raw_markdown` (or `fit_markdown` if available and preferred by the strategy).
            *   `"html"`: Uses `CrawlResult.cleaned_html`.
            *   `"fit_markdown"`: Specifically uses `CrawlResult.markdown.fit_markdown`.
            *   `"fit_html"`: Specifically uses `CrawlResult.markdown.fit_html`.
        *   Core configurations that shape these inputs (e.g., `css_selector`, `target_elements`, `markdown_generator`'s `content_filter`) are thus critical for the success of the extraction.
        *   For `LLMExtractionStrategy`, the `LLMConfig` (set when initializing the strategy or passed via `CrawlerRunConfig.llm_config`) is paramount, controlling the LLM provider, model, API key, and prompting parameters.
        *   `CrawlerRunConfig.target_elements` can be particularly useful with extraction. If your extraction schema is designed to pull data from specific sections, `target_elements` ensures only those sections are considered by strategies that respect it (like an LLM strategy prompted to process focused HTML snippets).
    *   **`ChunkingStrategy` (e.g., `RegexChunking`):**
        *   Chunking strategies typically operate on plain text, often derived from `CrawlResult.markdown.raw_markdown` or `CrawlResult.markdown.fit_markdown`.
        *   Core configurations affecting the quality and scope of the source Markdown will, therefore, impact the chunks produced.
    *   **`ContentFilteringStrategy` (e.g., `PruningContentFilter`, `LLMContentFilter`):**
        *   These are usually part of a `MarkdownGenerationStrategy` configuration.
        *   They take `cleaned_html` (output of `ContentScrapingStrategy`) as input and produce a "fit" (more relevant) HTML, which is then converted to `fit_markdown`.
        *   Core settings like `css_selector` apply *before* these filters, pre-scoping the HTML they receive.

*   B. **Pointers to Dedicated Documentation for Specialized Strategies:**
    *   **PDF Processing:** For details on `PDFCrawlerStrategy` and `PDFContentScrapingStrategy`, including image extraction and text processing from PDFs, refer to:
        *   [PDF Processing Guide](../../processors/pdf-processing.md) *(Assuming this link structure)*
    *   **Markdown Generation:** To understand `DefaultMarkdownGenerator`, custom HTML-to-Markdown options, and content filtering within Markdown generation, see:
        *   [Markdown Generation Guide](../../strategies/markdown-generation.md)
    *   **Data Extraction:**
        *   For LLM-based extraction (`LLMExtractionStrategy`), schema definition, and prompting: [LLM Extraction Guide](../../strategies/llm-extraction.md)
        *   For CSS/XPath-based structured data extraction (`JsonCssExtractionStrategy`, `JsonXpathExtractionStrategy`): [Selector-Based Extraction Guide](../../strategies/selector-extraction.md)
        *   For Regex-based extraction (`RegexExtractionStrategy`): [Regex Extraction Guide](../../strategies/regex-extraction.md)
    *   **Content Chunking:** For details on `RegexChunking` and other chunking approaches for LLM context preparation:
        *   [Content Chunking Guide](../../strategies/chunking.md)
    *   **Content Filtering:** For `PruningContentFilter`, `LLMContentFilter`, and creating custom filters:
        *   [Content Filtering Guide](../../strategies/content-filtering.md)

# IX. Troubleshooting Common Core Issues

Even with robust configurations, crawling can sometimes hit snags. Here are common issues related to the core components and how to approach them:

*   A. **Browser Launch Failures:**
    *   **Symptom:** `AsyncWebCrawler.start()` or the `async with` block fails immediately, often with errors related to Playwright or browser executables.
    *   **Causes & Solutions:**
        *   **Missing Playwright Drivers:** Playwright needs browser-specific drivers.
            *   **Fix:** Run `playwright install` or `crawl4ai-setup` to install them. If in a restricted environment, you might need to specify driver download paths manually using Playwright environment variables.
        *   **Permissions Issues for `user_data_dir`:** If you've specified a `user_data_dir` in `BrowserConfig`, the crawler process needs write permissions to that directory.
            *   **Fix:** Ensure permissions are correct or choose a writable directory.
        *   **Conflicting Browser Processes:** An old or hung browser process might be using the same debugging port or profile directory.
            *   **Fix:** Manually kill lingering browser processes. `ManagedBrowser` attempts some cleanup, but it's not always foolproof.
        *   **Incompatible `extra_args`:** Custom browser arguments might be incorrect or conflict.
            *   **Fix:** Remove or verify `extra_args` one by one.
        *   **Unsupported OS/Architecture:** Ensure your Playwright version supports your OS and CPU architecture.

*   B. **Navigation Timeouts or Errors (`page.goto()` failures):**
    *   **Symptom:** The crawl hangs or fails during the page navigation step, often with `TimeoutError` or network-related errors in logs.
    *   **Causes & Solutions:**
        *   **Incorrect `wait_for` Conditions:** If `CrawlerRunConfig.wait_for` specifies a condition that's never met, the page will time out.
            *   **Fix:** Verify your CSS selector or JS condition. Use browser dev tools to test selectors. Increase `wait_for_timeout` if the condition legitimately takes longer to meet.
        *   **Network Issues or Site Blocking:** The target site might be down, slow, or actively blocking your IP/User-Agent.
            *   **Fix:** Check site accessibility manually. Try different User-Agents or proxies (`BrowserConfig` or `CrawlerRunConfig`). Implement robust rate limiting via a custom dispatcher if hitting site limits.
        *   **`page_timeout` Too Short:** The global `CrawlerRunConfig.page_timeout` (default 60s) might be too short for very slow-loading pages.
            *   **Fix:** Increase `page_timeout` for problematic sites.
        *   **SSL/TLS Errors:** The site might have an invalid SSL certificate.
            *   **Fix:** Set `BrowserConfig(ignore_https_errors=True)` if you trust the site and want to proceed (use with caution). Check `result.ssl_certificate` if `fetch_ssl_certificate=True` for details.

*   C. **Content Not Appearing as Expected:**
    *   **Symptom:** `result.markdown` is empty or missing key content; `result.extracted_content` is empty or incorrect.
    *   **Causes & Solutions:**
        *   **Lazy Loading:** Content is loaded via JavaScript as the user scrolls.
            *   **Fix:** Set `CrawlerRunConfig(scan_full_page=True, scroll_delay=0.5)` (adjust delay as needed). For more complex infinite scroll, you might need custom `js_code` to trigger loads and appropriate `wait_for` conditions.
        *   **JavaScript Errors on the Page:** Errors in the site's own JS can prevent content from rendering.
            *   **Fix:** Run with `BrowserConfig(headless=False)` to inspect the browser console. Enable `CrawlerRunConfig(log_console=True)` to capture console messages in Crawl4ai's logs.
        *   **Incorrect CSS Selectors or `target_elements`:** Your selectors might be wrong or too broad/narrow.
            *   **Fix:** Use browser developer tools to test and refine your CSS selectors.
        *   **Content Loaded After Crawler Finishes:** The crawler might be capturing HTML before all dynamic content has finished loading.
            *   **Fix:** Use more robust `wait_for` conditions (e.g., wait for a specific "data loaded" flag set by page JS, or wait for a specific network request to complete using Playwright's advanced routing/waiting features via hooks). Increase `delay_before_retrieve_html` as a simpler, less precise fix.
        *   **AJAX Content:** Content is loaded via AJAX calls. `wait_for="networkidle"` or waiting for specific XHR responses (via hooks) can help.

*   D. **Proxy-Related Problems:**
    *   **Symptom:** Connection errors, timeouts, or getting content from your own IP instead of the proxy.
    *   **Causes & Solutions:**
        *   **Invalid Proxy Credentials/Format:** Ensure `ProxyConfig.server` (e.g., `http://user:pass@host:port`) and auth details are correct.
        *   **Proxy Server Unresponsive:** The proxy server itself might be down or overloaded. Test it independently.
        *   **Website Blocking Proxy IP:** The target site may have blacklisted the proxy's IP. Rotate proxies.
        *   **Proxy Type Mismatch:** Ensure the proxy protocol (HTTP, HTTPS, SOCKS5) matches what the server expects.
        *   **Firewall Issues:** Local or network firewalls might be blocking outbound connections to the proxy port.

*   E. **Session State Not Persisting (`session_id`, `user_data_dir`, `storage_state`):**
    *   **Symptom:** Logins are lost between `arun()` calls despite using `session_id`, or site preferences aren't remembered when using `user_data_dir` or `storage_state`.
    *   **Causes & Solutions:**
        *   **Incorrect `session_id` Usage:** Ensure you're passing the *exact same* `session_id` string to subsequent `arun()` calls that need to share the state.
        *   **`js_only=False` on Subsequent Calls:** If you navigate to a new URL (even the same one) in a subsequent `arun` call for an existing session, it's a full navigation. If you only want to run JS on the *current page* of that session, set `js_only=True`.
        *   **`use_persistent_context=False` with `user_data_dir`:** Both must be set correctly in `BrowserConfig` for profile persistence to work.
        *   **Incorrect `user_data_dir` Path or Permissions:** Verify the path and ensure write access.
        *   **Invalid `storage_state` File/Format:** Ensure the JSON file is valid and was correctly saved by Playwright.
        *   **Site-Specific Session Handling:** Some sites have very aggressive session timeout mechanisms, use multiple cookies, or employ JavaScript-based session checks that might invalidate sessions even if cookies are present. Debugging these often requires running headed and observing network requests and storage.
        *   **Context Replaced by `BrowserManager`:** If you drastically change other context-affecting parameters in `CrawlerRunConfig` (like per-run proxy or UA) even with the same `session_id`, the `BrowserManager` might decide it needs a new `BrowserContext` due to a different "signature," potentially losing the state of the old one associated with that `session_id`. Keep context-affecting overrides consistent for a given session.

# X. Conclusion and Best Practices Summary

You've now explored the foundational `core` components of Crawl4ai, from launching browsers with `BrowserConfig` to executing fine-tuned crawl operations with `CrawlerRunConfig`, and interpreting the rich `CrawlResult`. The core is designed for flexibility and power, enabling you to tackle a wide array of web data extraction challenges.

**Recap of Key Core Concepts:**

*   **`AsyncWebCrawler`:** Your main interaction point, orchestrating the crawl.
*   **`BrowserConfig`:** Defines the global browser environment (engine, headless, proxy, UA). Set it up once for a crawler instance.
*   **`CrawlerRunConfig`:** Defines per-crawl behavior (URL, caching, JS, content selection, extraction). Pass it to `arun()` or `arun_many()`.
*   **Strategies:** Crawl4ai uses a strategy pattern for crawling (`AsyncPlaywrightCrawlerStrategy`, `AsyncHTTPCrawlerStrategy`), scraping (`WebScrapingStrategy`), Markdown generation (`DefaultMarkdownGenerator`), and data extraction.
*   **Hooks (in `AsyncPlaywrightCrawlerStrategy`):** Allow custom code injection at key lifecycle points for advanced control and interactions.
*   **`CrawlResult`:** The comprehensive output object containing HTML variants, Markdown, media, links, metadata, and more.

**Best Practices Summary:**

1.  **Plan Your Configuration:**
    *   Distinguish between global browser setup (`BrowserConfig`) and per-crawl instructions (`CrawlerRunConfig`).
    *   Start with simpler configurations and add complexity (like JS execution or specific selectors) iteratively.
2.  **Leverage Caching Wisely:**
    *   Use `CacheMode.ENABLED` during development to speed up iterations.
    *   Use `CacheMode.BYPASS` or `CacheMode.WRITE_ONLY` for production runs where fresh data is paramount.
3.  **Be Specific with Content Selection:**
    *   Use `css_selector` to scope processing to relevant page areas early.
    *   Employ `target_elements`, `excluded_tags`, and `excluded_selector` to refine the content fed into Markdown and extraction processes.
4.  **Handle Dynamic Content Effectively:**
    *   Use `js_code` for interactions.
    *   Rely on robust `wait_for` conditions (CSS selectors or JS expressions) rather than fixed delays.
    *   Utilize `scan_full_page` for lazy-loading and infinite scroll, adjusting `scroll_delay` as needed.
5.  **Manage Sessions and State:**
    *   For multi-step processes or login persistence within a crawler's lifetime, use `session_id` and `js_only=True` for subsequent on-page JS executions.
    *   For persistence across crawler restarts, use `BrowserConfig(use_persistent_context=True, user_data_dir="...")` or `BrowserConfig(storage_state="...")`.
    *   Always `kill_session()` when a `session_id`-based task is complete.
6.  **Prioritize Realistic Browser Emulation for Stealth:**
    *   Use common, valid User-Agents (consider `user_agent_mode="random"`).
    *   Set appropriate viewports.
    *   Use proxies, especially for larger crawls or geo-specific content.
7.  **Use Hooks for Complex Interactions:**
    *   For tasks like login sequences or intricate page manipulations, hooks (especially `on_page_context_created` and `before_retrieve_html`) offer more control than simple `js_code`.
    *   Pass dynamic data to hooks via `CrawlerRunConfig.shared_data`.
8.  **Choose the Right Crawler Strategy:**
    *   Default to `AsyncPlaywrightCrawlerStrategy` for most modern web pages.
    *   Opt for `AsyncHTTPCrawlerStrategy` for APIs, static HTML, or when JS rendering is not needed, to gain significant speed and reduce resource usage.
9.  **Log and Debug Systematically:**
    *   Enable `verbose=True` in `BrowserConfig` or `CrawlerRunConfig`.
    *   Run `headless=False` during development to visually inspect browser behavior.
    *   Check `result.error_message` and `result.status_code` for failures.
10. **Respect `robots.txt`:**
    *   Enable `check_robots_txt=True` in `CrawlerRunConfig` for ethical crawling, unless you have specific reasons and permissions to do otherwise.

By mastering these core components and applying these best practices, you can build powerful, efficient, and resilient web crawlers with Crawl4ai, capable of extracting valuable data from even the most challenging websites. Happy crawling!
```

---


## Configuration Objects - Memory
Source: crawl4ai_config_objects_memory_content.llm.md

# Detailed Outline for crawl4ai - config_objects Component

**Target Document Type:** memory
**Target Output Filename Suggestion:** `llm_memory_config_objects.md`
**Library Version Context:** 0.6.3
**Outline Generation Date:** 2024-05-24
---

## 1. Introduction to Configuration Objects in Crawl4ai

*   **1.1. Purpose of Configuration Objects**
    *   Explanation: Configuration objects in `crawl4ai` serve to centralize and manage settings for various components and behaviors of the library. This includes browser setup, individual crawl run parameters, LLM provider interactions, proxy settings, and more.
    *   Benefit: This approach enhances code readability by grouping related settings, improves maintainability by providing a clear structure for configurations, and offers ease of customization for users to tailor the library's behavior to their specific needs.
*   **1.2. General Principles and Usage**
    *   **1.2.1. Immutability/Cloning:**
        *   Concept: Most configuration objects are designed with a `clone()` method, allowing users to create modified copies without altering the original configuration instance. This promotes safer state management, especially when reusing base configurations for multiple tasks.
        *   Method: `clone(**kwargs)` on most configuration objects.
    *   **1.2.2. Serialization and Deserialization:**
        *   Concept: `crawl4ai` configuration objects can be serialized to dictionary format (e.g., for saving to JSON) and deserialized back into their respective class instances.
        *   Methods:
            *   `dump() -> dict`: Serializes the object to a dictionary suitable for JSON, often using the internal `to_serializable_dict` helper.
            *   `load(data: dict) -> ConfigClass` (Static Method): Deserializes an object from a dictionary, often using the internal `from_serializable_dict` helper.
            *   `to_dict() -> dict`: Converts the object to a standard Python dictionary.
            *   `from_dict(data: dict) -> ConfigClass` (Static Method): Creates an instance from a standard Python dictionary.
        *   Helper Functions:
            *   `crawl4ai.async_configs.to_serializable_dict(obj: Any, ignore_default_value: bool = False) -> Dict`: Recursively converts objects into a serializable dictionary format, handling complex types like enums and nested objects.
            *   `crawl4ai.async_configs.from_serializable_dict(data: Any) -> Any`: Reconstructs Python objects from the serializable dictionary format.
*   **1.3. Scope of this Document**
    *   Statement: This document provides a factual API reference for the primary configuration objects within the `crawl4ai` library, detailing their purpose, initialization parameters, attributes, and key methods.

## 2. Core Configuration Objects

### 2.1. `BrowserConfig`
Located in `crawl4ai.async_configs`.

*   **2.1.1. Purpose:**
    *   Description: The `BrowserConfig` class is used to configure the settings for a browser instance and its associated contexts when using browser-based crawler strategies like `AsyncPlaywrightCrawlerStrategy`. It centralizes all parameters that affect the creation and behavior of the browser.
*   **2.1.2. Initialization (`__init__`)**
    *   Signature:
        ```python
        class BrowserConfig:
            def __init__(
                self,
                browser_type: str = "chromium",
                headless: bool = True,
                browser_mode: str = "dedicated",
                use_managed_browser: bool = False,
                cdp_url: Optional[str] = None,
                use_persistent_context: bool = False,
                user_data_dir: Optional[str] = None,
                chrome_channel: Optional[str] = "chromium", # Note: 'channel' is preferred
                channel: Optional[str] = "chromium",
                proxy: Optional[str] = None,
                proxy_config: Optional[Union[ProxyConfig, dict]] = None,
                viewport_width: int = 1080,
                viewport_height: int = 600,
                viewport: Optional[dict] = None,
                accept_downloads: bool = False,
                downloads_path: Optional[str] = None,
                storage_state: Optional[Union[str, dict]] = None,
                ignore_https_errors: bool = True,
                java_script_enabled: bool = True,
                sleep_on_close: bool = False,
                verbose: bool = True,
                cookies: Optional[List[dict]] = None,
                headers: Optional[dict] = None,
                user_agent: Optional[str] = "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/116.0.0.0 Safari/537.36",
                user_agent_mode: Optional[str] = "",
                user_agent_generator_config: Optional[dict] = None, # Default is {} in __init__
                text_mode: bool = False,
                light_mode: bool = False,
                extra_args: Optional[List[str]] = None,
                debugging_port: int = 9222,
                host: str = "localhost"
            ): ...
        ```
    *   Parameters:
        *   `browser_type (str, default: "chromium")`: Specifies the browser engine to use. Supported values: `"chromium"`, `"firefox"`, `"webkit"`.
        *   `headless (bool, default: True)`: If `True`, runs the browser without a visible GUI. Set to `False` for debugging or visual interaction.
        *   `browser_mode (str, default: "dedicated")`: Defines how the browser is initialized. Options: `"builtin"` (uses built-in CDP), `"dedicated"` (new instance each time), `"cdp"` (connects to an existing CDP endpoint specified by `cdp_url`), `"docker"` (runs browser in a Docker container).
        *   `use_managed_browser (bool, default: False)`: If `True`, launches the browser using a managed approach (e.g., via CDP or Docker), allowing for more advanced control. Automatically set to `True` if `browser_mode` is `"builtin"`, `"docker"`, or if `cdp_url` is provided, or if `use_persistent_context` is `True`.
        *   `cdp_url (Optional[str], default: None)`: The URL for the Chrome DevTools Protocol (CDP) endpoint. If not provided and `use_managed_browser` is active, it might be set by an internal browser manager.
        *   `use_persistent_context (bool, default: False)`: If `True`, uses a persistent browser context (profile), saving cookies, localStorage, etc., across sessions. Requires `user_data_dir`. Sets `use_managed_browser=True`.
        *   `user_data_dir (Optional[str], default: None)`: Path to a directory for storing user data for persistent sessions. If `None` and `use_persistent_context` is `True`, a temporary directory might be used.
        *   `chrome_channel (Optional[str], default: "chromium")`: Specifies the Chrome channel (e.g., "chrome", "msedge", "chromium-beta"). Only applicable if `browser_type` is "chromium".
        *   `channel (Optional[str], default: "chromium")`: Preferred alias for `chrome_channel`. Set to `""` for Firefox or WebKit.
        *   `proxy (Optional[str], default: None)`: A string representing the proxy server URL (e.g., "http://username:password@proxy.example.com:8080").
        *   `proxy_config (Optional[Union[ProxyConfig, dict]], default: None)`: A `ProxyConfig` object or a dictionary specifying detailed proxy settings. Overrides the `proxy` string if both are provided.
        *   `viewport_width (int, default: 1080)`: Default width of the browser viewport in pixels.
        *   `viewport_height (int, default: 600)`: Default height of the browser viewport in pixels.
        *   `viewport (Optional[dict], default: None)`: A dictionary specifying viewport dimensions, e.g., `{"width": 1920, "height": 1080}`. If set, overrides `viewport_width` and `viewport_height`.
        *   `accept_downloads (bool, default: False)`: If `True`, allows files to be downloaded by the browser.
        *   `downloads_path (Optional[str], default: None)`: Directory path where downloaded files will be stored. Required if `accept_downloads` is `True`.
        *   `storage_state (Optional[Union[str, dict]], default: None)`: Path to a JSON file or a dictionary containing the browser's storage state (cookies, localStorage, etc.) to load.
        *   `ignore_https_errors (bool, default: True)`: If `True`, HTTPS certificate errors will be ignored.
        *   `java_script_enabled (bool, default: True)`: If `True`, JavaScript execution is enabled on web pages.
        *   `sleep_on_close (bool, default: False)`: If `True`, introduces a small delay before the browser is closed.
        *   `verbose (bool, default: True)`: If `True`, enables verbose logging for browser operations.
        *   `cookies (Optional[List[dict]], default: None)`: A list of cookie dictionaries to be set in the browser context. Each dictionary should conform to Playwright's cookie format.
        *   `headers (Optional[dict], default: None)`: A dictionary of additional HTTP headers to be sent with every request made by the browser.
        *   `user_agent (Optional[str], default: "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/116.0.0.0 Safari/537.36")`: The User-Agent string the browser will use.
        *   `user_agent_mode (Optional[str], default: "")`: Mode for generating the User-Agent string. If set (e.g., to "random"), `user_agent_generator_config` can be used.
        *   `user_agent_generator_config (Optional[dict], default: {})`: Configuration dictionary for the User-Agent generator if `user_agent_mode` is active.
        *   `text_mode (bool, default: False)`: If `True`, attempts to disable images and other rich content to potentially speed up loading for text-focused crawls.
        *   `light_mode (bool, default: False)`: If `True`, disables certain background browser features for potential performance gains.
        *   `extra_args (Optional[List[str]], default: None)`: A list of additional command-line arguments to pass to the browser executable upon launch.
        *   `debugging_port (int, default: 9222)`: The port to use for the browser's remote debugging protocol (CDP).
        *   `host (str, default: "localhost")`: The host on which the browser's remote debugging protocol will listen.
*   **2.1.3. Key Public Attributes/Properties:**
    *   All parameters listed in `__init__` are available as public attributes with the same names and types.
    *   `browser_hint (str)`: [Read-only] - A string representing client hints (Sec-CH-UA) generated based on the `user_agent` string. This is automatically set during initialization.
*   **2.1.4. Key Public Methods:**
    *   `from_kwargs(cls, kwargs: dict) -> BrowserConfig` (Static Method):
        *   Purpose: Creates a `BrowserConfig` instance from a dictionary of keyword arguments.
    *   `to_dict(self) -> dict`:
        *   Purpose: Converts the `BrowserConfig` instance into a dictionary representation.
    *   `clone(self, **kwargs) -> BrowserConfig`:
        *   Purpose: Creates a deep copy of the current `BrowserConfig` instance. Keyword arguments can be provided to override specific attributes in the new instance.
    *   `dump(self) -> dict`:
        *   Purpose: Serializes the `BrowserConfig` object into a dictionary format that is suitable for JSON storage or transmission, utilizing the `to_serializable_dict` helper.
    *   `load(cls, data: dict) -> BrowserConfig` (Static Method):
        *   Purpose: Deserializes a `BrowserConfig` object from a dictionary (typically one created by `dump()`), utilizing the `from_serializable_dict` helper.

### 2.2. `CrawlerRunConfig`
Located in `crawl4ai.async_configs`.

*   **2.2.1. Purpose:**
    *   Description: The `CrawlerRunConfig` class encapsulates all settings that control the behavior of a single crawl operation performed by `AsyncWebCrawler.arun()` or multiple operations within `AsyncWebCrawler.arun_many()`. This includes parameters for content processing, page interaction, caching, and media handling.
*   **2.2.2. Initialization (`__init__`)**
    *   Signature:
        ```python
        class CrawlerRunConfig:
            def __init__(
                self,
                url: Optional[str] = None,
                word_count_threshold: int = MIN_WORD_THRESHOLD,
                extraction_strategy: Optional[ExtractionStrategy] = None,
                chunking_strategy: Optional[ChunkingStrategy] = RegexChunking(),
                markdown_generator: Optional[MarkdownGenerationStrategy] = DefaultMarkdownGenerator(),
                only_text: bool = False,
                css_selector: Optional[str] = None,
                target_elements: Optional[List[str]] = None, # Default is [] in __init__
                excluded_tags: Optional[List[str]] = None, # Default is [] in __init__
                excluded_selector: Optional[str] = "", # Default is "" in __init__
                keep_data_attributes: bool = False,
                keep_attrs: Optional[List[str]] = None, # Default is [] in __init__
                remove_forms: bool = False,
                prettify: bool = False,
                parser_type: str = "lxml",
                scraping_strategy: Optional[ContentScrapingStrategy] = None, # Instantiated with WebScrapingStrategy() if None
                proxy_config: Optional[Union[ProxyConfig, dict]] = None,
                proxy_rotation_strategy: Optional[ProxyRotationStrategy] = None,
                locale: Optional[str] = None,
                timezone_id: Optional[str] = None,
                geolocation: Optional[GeolocationConfig] = None,
                fetch_ssl_certificate: bool = False,
                cache_mode: CacheMode = CacheMode.BYPASS,
                session_id: Optional[str] = None,
                shared_data: Optional[dict] = None,
                wait_until: str = "domcontentloaded",
                page_timeout: int = PAGE_TIMEOUT,
                wait_for: Optional[str] = None,
                wait_for_timeout: Optional[int] = None,
                wait_for_images: bool = False,
                delay_before_return_html: float = 0.1,
                mean_delay: float = 0.1,
                max_range: float = 0.3,
                semaphore_count: int = 5,
                js_code: Optional[Union[str, List[str]]] = None,
                js_only: bool = False,
                ignore_body_visibility: bool = True,
                scan_full_page: bool = False,
                scroll_delay: float = 0.2,
                process_iframes: bool = False,
                remove_overlay_elements: bool = False,
                simulate_user: bool = False,
                override_navigator: bool = False,
                magic: bool = False,
                adjust_viewport_to_content: bool = False,
                screenshot: bool = False,
                screenshot_wait_for: Optional[float] = None,
                screenshot_height_threshold: int = SCREENSHOT_HEIGHT_THRESHOLD,
                pdf: bool = False,
                capture_mhtml: bool = False,
                image_description_min_word_threshold: int = IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD,
                image_score_threshold: int = IMAGE_SCORE_THRESHOLD,
                table_score_threshold: int = 7,
                exclude_external_images: bool = False,
                exclude_all_images: bool = False,
                exclude_social_media_domains: Optional[List[str]] = None, # Uses SOCIAL_MEDIA_DOMAINS if None
                exclude_external_links: bool = False,
                exclude_social_media_links: bool = False,
                exclude_domains: Optional[List[str]] = None, # Default is [] in __init__
                exclude_internal_links: bool = False,
                verbose: bool = True,
                log_console: bool = False,
                capture_network_requests: bool = False,
                capture_console_messages: bool = False,
                method: str = "GET",
                stream: bool = False,
                check_robots_txt: bool = False,
                user_agent: Optional[str] = None,
                user_agent_mode: Optional[str] = None,
                user_agent_generator_config: Optional[dict] = None, # Default is {} in __init__
                deep_crawl_strategy: Optional[DeepCrawlStrategy] = None,
                experimental: Optional[Dict[str, Any]] = None # Default is {} in __init__
            ): ...
        ```
    *   Parameters:
        *   `url (Optional[str], default: None)`: The target URL for this specific crawl run.
        *   `word_count_threshold (int, default: MIN_WORD_THRESHOLD)`: Minimum word count for a text block to be considered significant during content processing.
        *   `extraction_strategy (Optional[ExtractionStrategy], default: None)`: Strategy for extracting structured data from the page. If `None`, `NoExtractionStrategy` is used.
        *   `chunking_strategy (Optional[ChunkingStrategy], default: RegexChunking())`: Strategy to split content into chunks before extraction.
        *   `markdown_generator (Optional[MarkdownGenerationStrategy], default: DefaultMarkdownGenerator())`: Strategy for converting HTML to Markdown.
        *   `only_text (bool, default: False)`: If `True`, attempts to extract only textual content, potentially ignoring structural elements beneficial for rich Markdown.
        *   `css_selector (Optional[str], default: None)`: A CSS selector defining the primary region of the page to focus on for content extraction. The raw HTML is reduced to this region.
        *   `target_elements (Optional[List[str]], default: [])`: A list of CSS selectors. If provided, only the content within these elements will be considered for Markdown generation and structured data extraction. Unlike `css_selector`, this does not reduce the raw HTML but scopes the processing.
        *   `excluded_tags (Optional[List[str]], default: [])`: A list of HTML tag names (e.g., "nav", "footer") to be removed from the HTML before processing.
        *   `excluded_selector (Optional[str], default: "")`: A CSS selector specifying elements to be removed from the HTML before processing.
        *   `keep_data_attributes (bool, default: False)`: If `True`, `data-*` attributes on HTML elements are preserved during cleaning.
        *   `keep_attrs (Optional[List[str]], default: [])`: A list of specific HTML attribute names to preserve during HTML cleaning.
        *   `remove_forms (bool, default: False)`: If `True`, all `<form>` elements are removed from the HTML.
        *   `prettify (bool, default: False)`: If `True`, the cleaned HTML output is "prettified" for better readability.
        *   `parser_type (str, default: "lxml")`: The HTML parser to be used by the scraping strategy (e.g., "lxml", "html.parser").
        *   `scraping_strategy (Optional[ContentScrapingStrategy], default: WebScrapingStrategy())`: The strategy for scraping content from the HTML.
        *   `proxy_config (Optional[Union[ProxyConfig, dict]], default: None)`: Proxy configuration for this specific run. Overrides any proxy settings in `BrowserConfig`.
        *   `proxy_rotation_strategy (Optional[ProxyRotationStrategy], default: None)`: Strategy to use for rotating proxies if multiple are available.
        *   `locale (Optional[str], default: None)`: Locale to set for the browser context (e.g., "en-US", "fr-FR"). Affects `Accept-Language` header and JavaScript `navigator.language`.
        *   `timezone_id (Optional[str], default: None)`: Timezone ID to set for the browser context (e.g., "America/New_York", "Europe/Paris"). Affects JavaScript `Date` objects.
        *   `geolocation (Optional[GeolocationConfig], default: None)`: A `GeolocationConfig` object or dictionary to set the browser's mock geolocation.
        *   `fetch_ssl_certificate (bool, default: False)`: If `True`, the SSL certificate information for the main URL will be fetched and included in the `CrawlResult`.
        *   `cache_mode (CacheMode, default: CacheMode.BYPASS)`: Defines caching behavior for this run. See `CacheMode` enum for options.
        *   `session_id (Optional[str], default: None)`: An identifier for a browser session. If provided, `crawl4ai` will attempt to reuse an existing page/context associated with this ID, or create a new one and associate it.
        *   `shared_data (Optional[dict], default: None)`: A dictionary for passing custom data between hooks during the crawl lifecycle.
        *   `wait_until (str, default: "domcontentloaded")`: Playwright's page navigation wait condition (e.g., "load", "domcontentloaded", "networkidle", "commit").
        *   `page_timeout (int, default: PAGE_TIMEOUT)`: Maximum time in milliseconds for page navigation and other page operations.
        *   `wait_for (Optional[str], default: None)`: A CSS selector or a JavaScript expression (prefixed with "js:"). The crawler will wait until this condition is met before proceeding.
        *   `wait_for_timeout (Optional[int], default: None)`: Specific timeout in milliseconds for the `wait_for` condition. If `None`, `page_timeout` is used.
        *   `wait_for_images (bool, default: False)`: If `True`, attempts to wait for all images on the page to finish loading.
        *   `delay_before_return_html (float, default: 0.1)`: Delay in seconds to wait just before the final HTML content is retrieved from the page.
        *   `mean_delay (float, default: 0.1)`: Used with `arun_many`. The mean base delay in seconds between processing URLs.
        *   `max_range (float, default: 0.3)`: Used with `arun_many`. The maximum additional random delay (added to `mean_delay`) between processing URLs.
        *   `semaphore_count (int, default: 5)`: Used with `arun_many` and semaphore-based dispatchers. The maximum number of concurrent crawl operations.
        *   `js_code (Optional[Union[str, List[str]]], default: None)`: A string or list of strings containing JavaScript code to be executed on the page after it loads.
        *   `js_only (bool, default: False)`: If `True`, indicates that this `arun` call is primarily for JavaScript execution on an already loaded page (within a session) and a full page navigation might not be needed.
        *   `ignore_body_visibility (bool, default: True)`: If `True`, proceeds with content extraction even if the `<body>` element is not deemed visible by Playwright.
        *   `scan_full_page (bool, default: False)`: If `True`, the crawler will attempt to scroll through the entire page to trigger lazy-loaded content.
        *   `scroll_delay (float, default: 0.2)`: Delay in seconds between each scroll step when `scan_full_page` is `True`.
        *   `process_iframes (bool, default: False)`: If `True`, attempts to extract and inline content from `<iframe>` elements.
        *   `remove_overlay_elements (bool, default: False)`: If `True`, attempts to identify and remove common overlay elements (popups, cookie banners) before content extraction.
        *   `simulate_user (bool, default: False)`: If `True`, enables heuristics to simulate user interactions (like mouse movements) to potentially bypass some anti-bot measures.
        *   `override_navigator (bool, default: False)`: If `True`, overrides certain JavaScript `navigator` properties to appear more like a standard browser.
        *   `magic (bool, default: False)`: If `True`, enables a combination of techniques (like `remove_overlay_elements`, `simulate_user`) to try and handle dynamic/obfuscated sites.
        *   `adjust_viewport_to_content (bool, default: False)`: If `True`, attempts to adjust the browser viewport size to match the dimensions of the page content.
        *   `screenshot (bool, default: False)`: If `True`, a screenshot of the page will be taken and included in `CrawlResult.screenshot`.
        *   `screenshot_wait_for (Optional[float], default: None)`: Additional delay in seconds to wait before taking the screenshot.
        *   `screenshot_height_threshold (int, default: SCREENSHOT_HEIGHT_THRESHOLD)`: If page height exceeds this, a full-page screenshot strategy might be different.
        *   `pdf (bool, default: False)`: If `True`, a PDF version of the page will be generated and included in `CrawlResult.pdf`.
        *   `capture_mhtml (bool, default: False)`: If `True`, an MHTML archive of the page will be captured and included in `CrawlResult.mhtml`.
        *   `image_description_min_word_threshold (int, default: IMAGE_DESCRIPTION_MIN_WORD_THRESHOLD)`: Minimum word count for surrounding text to be considered as an image description.
        *   `image_score_threshold (int, default: IMAGE_SCORE_THRESHOLD)`: Heuristic score threshold for an image to be included in `CrawlResult.media`.
        *   `table_score_threshold (int, default: 7)`: Heuristic score threshold for an HTML table to be considered a data table and included in `CrawlResult.media`.
        *   `exclude_external_images (bool, default: False)`: If `True`, images hosted on different domains than the main page URL are excluded.
        *   `exclude_all_images (bool, default: False)`: If `True`, all images are excluded from `CrawlResult.media`.
        *   `exclude_social_media_domains (Optional[List[str]], default: SOCIAL_MEDIA_DOMAINS from config)`: List of social media domains whose links should be excluded.
        *   `exclude_external_links (bool, default: False)`: If `True`, all links pointing to external domains are excluded from `CrawlResult.links`.
        *   `exclude_social_media_links (bool, default: False)`: If `True`, links to domains in `exclude_social_media_domains` are excluded.
        *   `exclude_domains (Optional[List[str]], default: [])`: A list of specific domains whose links should be excluded.
        *   `exclude_internal_links (bool, default: False)`: If `True`, all links pointing to the same domain are excluded.
        *   `verbose (bool, default: True)`: Enables verbose logging for this specific crawl run. Overrides `BrowserConfig.verbose`.
        *   `log_console (bool, default: False)`: If `True`, browser console messages are captured (requires `capture_console_messages=True` to be effective).
        *   `capture_network_requests (bool, default: False)`: If `True`, captures details of network requests and responses made by the page.
        *   `capture_console_messages (bool, default: False)`: If `True`, captures messages logged to the browser's console.
        *   `method (str, default: "GET")`: HTTP method to use, primarily for `AsyncHTTPCrawlerStrategy`.
        *   `stream (bool, default: False)`: If `True` when using `arun_many`, results are yielded as an async generator instead of returned as a list at the end.
        *   `check_robots_txt (bool, default: False)`: If `True`, `robots.txt` rules for the domain will be checked and respected.
        *   `user_agent (Optional[str], default: None)`: User-Agent string for this specific run. Overrides `BrowserConfig.user_agent`.
        *   `user_agent_mode (Optional[str], default: None)`: User-Agent generation mode for this specific run.
        *   `user_agent_generator_config (Optional[dict], default: {})`: Configuration for User-Agent generator for this run.
        *   `deep_crawl_strategy (Optional[DeepCrawlStrategy], default: None)`: Strategy to use for deep crawling beyond the initial URL.
        *   `experimental (Optional[Dict[str, Any]], default: {})`: A dictionary for passing experimental or beta parameters.
*   **2.2.3. Key Public Attributes/Properties:**
    *   All parameters listed in `__init__` are available as public attributes with the same names and types.
*   **2.2.4. Deprecated Property Handling (`__getattr__`, `_UNWANTED_PROPS`)**
    *   Behavior: Attempting to access a deprecated property (e.g., `bypass_cache`, `disable_cache`, `no_cache_read`, `no_cache_write`) raises an `AttributeError`. The error message directs the user to use the `cache_mode` parameter with the appropriate `CacheMode` enum member instead.
    *   List of Deprecated Properties and their `CacheMode` Equivalents:
        *   `bypass_cache`: Use `cache_mode=CacheMode.BYPASS`.
        *   `disable_cache`: Use `cache_mode=CacheMode.DISABLE`.
        *   `no_cache_read`: Use `cache_mode=CacheMode.WRITE_ONLY`.
        *   `no_cache_write`: Use `cache_mode=CacheMode.READ_ONLY`.
*   **2.2.5. Key Public Methods:**
    *   `from_kwargs(cls, kwargs: dict) -> CrawlerRunConfig` (Static Method):
        *   Purpose: Creates a `CrawlerRunConfig` instance from a dictionary of keyword arguments.
    *   `dump(self) -> dict`:
        *   Purpose: Serializes the `CrawlerRunConfig` object to a dictionary suitable for JSON storage, handling complex nested objects using `to_serializable_dict`.
    *   `load(cls, data: dict) -> CrawlerRunConfig` (Static Method):
        *   Purpose: Deserializes a `CrawlerRunConfig` object from a dictionary (typically one created by `dump()`), using `from_serializable_dict`.
    *   `to_dict(self) -> dict`:
        *   Purpose: Converts the `CrawlerRunConfig` instance into a dictionary representation. Complex objects like strategies are typically represented by their class name or a simplified form.
    *   `clone(self, **kwargs) -> CrawlerRunConfig`:
        *   Purpose: Creates a deep copy of the current `CrawlerRunConfig` instance. Keyword arguments can be provided to override specific attributes in the new instance.

### 2.3. `LLMConfig`
Located in `crawl4ai.async_configs`.

*   **2.3.1. Purpose:**
    *   Description: The `LLMConfig` class provides configuration for interacting with Large Language Model (LLM) providers. It includes settings for the provider name, API token, base URL, and various model-specific parameters like temperature and max tokens.
*   **2.3.2. Initialization (`__init__`)**
    *   Signature:
        ```python
        class LLMConfig:
            def __init__(
                self,
                provider: str = DEFAULT_PROVIDER, # e.g., "openai/gpt-4o-mini"
                api_token: Optional[str] = None,
                base_url: Optional[str] = None,
                temperature: Optional[float] = None,
                max_tokens: Optional[int] = None,
                top_p: Optional[float] = None,
                frequency_penalty: Optional[float] = None,
                presence_penalty: Optional[float] = None,
                stop: Optional[List[str]] = None,
                n: Optional[int] = None,
            ): ...
        ```
    *   Parameters:
        *   `provider (str, default: DEFAULT_PROVIDER)`: The identifier for the LLM provider and model (e.g., "openai/gpt-4o-mini", "ollama/llama3.3", "gemini/gemini-1.5-pro").
        *   `api_token (Optional[str], default: None)`: The API token for authenticating with the LLM provider. If `None`, it attempts to load from environment variables based on the provider (e.g., `OPENAI_API_KEY` for OpenAI, `GEMINI_API_KEY` for Gemini). Can also be set as "env:YOUR_ENV_VAR_NAME".
        *   `base_url (Optional[str], default: None)`: A custom base URL for the LLM API endpoint, useful for self-hosted models or proxies.
        *   `temperature (Optional[float], default: None)`: Controls the randomness of the LLM's output. Higher values (e.g., 0.8) make output more random, lower values (e.g., 0.2) make it more deterministic.
        *   `max_tokens (Optional[int], default: None)`: The maximum number of tokens the LLM should generate in its response.
        *   `top_p (Optional[float], default: None)`: Nucleus sampling parameter. The model considers only tokens with cumulative probability mass up to `top_p`.
        *   `frequency_penalty (Optional[float], default: None)`: Penalizes new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
        *   `presence_penalty (Optional[float], default: None)`: Penalizes new tokens based on whether they have appeared in the text so far, increasing the model's likelihood to talk about new topics.
        *   `stop (Optional[List[str]], default: None)`: A list of sequences where the API will stop generating further tokens.
        *   `n (Optional[int], default: None)`: The number of completions to generate for each prompt.
*   **2.3.3. Key Public Attributes/Properties:**
    *   All parameters listed in `__init__` are available as public attributes with the same names and types.
*   **2.3.4. Key Public Methods:**
    *   `from_kwargs(cls, kwargs: dict) -> LLMConfig` (Static Method):
        *   Purpose: Creates an `LLMConfig` instance from a dictionary of keyword arguments.
    *   `to_dict(self) -> dict`:
        *   Purpose: Converts the `LLMConfig` instance into a dictionary representation.
    *   `clone(self, **kwargs) -> LLMConfig`:
        *   Purpose: Creates a deep copy of the current `LLMConfig` instance. Keyword arguments can be provided to override specific attributes in the new instance.

### 2.4. `GeolocationConfig`
Located in `crawl4ai.async_configs`.

*   **2.4.1. Purpose:**
    *   Description: The `GeolocationConfig` class stores settings for mocking the browser's geolocation, including latitude, longitude, and accuracy.
*   **2.4.2. Initialization (`__init__`)**
    *   Signature:
        ```python
        class GeolocationConfig:
            def __init__(
                self,
                latitude: float,
                longitude: float,
                accuracy: Optional[float] = 0.0
            ): ...
        ```
    *   Parameters:
        *   `latitude (float)`: The latitude coordinate (e.g., 37.7749 for San Francisco).
        *   `longitude (float)`: The longitude coordinate (e.g., -122.4194 for San Francisco).
        *   `accuracy (Optional[float], default: 0.0)`: The accuracy of the geolocation in meters.
*   **2.4.3. Key Public Attributes/Properties:**
    *   `latitude (float)`: Stores the latitude.
    *   `longitude (float)`: Stores the longitude.
    *   `accuracy (Optional[float])`: Stores the accuracy.
*   **2.4.4. Key Public Methods:**
    *   `from_dict(cls, geo_dict: dict) -> GeolocationConfig` (Static Method):
        *   Purpose: Creates a `GeolocationConfig` instance from a dictionary.
    *   `to_dict(self) -> dict`:
        *   Purpose: Converts the `GeolocationConfig` instance to a dictionary: `{"latitude": ..., "longitude": ..., "accuracy": ...}`.
    *   `clone(self, **kwargs) -> GeolocationConfig`:
        *   Purpose: Creates a copy of the `GeolocationConfig` instance, allowing for overriding specific attributes with `kwargs`.

### 2.5. `ProxyConfig`
Located in `crawl4ai.async_configs` (and `crawl4ai.proxy_strategy`).

*   **2.5.1. Purpose:**
    *   Description: The `ProxyConfig` class encapsulates the configuration for a single proxy server, including its address, authentication credentials (if any), and optionally its public IP address.
*   **2.5.2. Initialization (`__init__`)**
    *   Signature:
        ```python
        class ProxyConfig:
            def __init__(
                self,
                server: str,
                username: Optional[str] = None,
                password: Optional[str] = None,
                ip: Optional[str] = None,
            ): ...
        ```
    *   Parameters:
        *   `server (str)`: The proxy server URL, including protocol and port (e.g., "http://127.0.0.1:8080", "socks5://proxy.example.com:1080").
        *   `username (Optional[str], default: None)`: The username for proxy authentication, if required.
        *   `password (Optional[str], default: None)`: The password for proxy authentication, if required.
        *   `ip (Optional[str], default: None)`: The public IP address of the proxy server. If not provided, it will be automatically extracted from the `server` string if possible.
*   **2.5.3. Key Public Attributes/Properties:**
    *   `server (str)`: The proxy server URL.
    *   `username (Optional[str])`: The username for proxy authentication.
    *   `password (Optional[str])`: The password for proxy authentication.
    *   `ip (Optional[str])`: The public IP address of the proxy. This is either user-provided or automatically extracted from the `server` string during initialization via the internal `_extract_ip_from_server` method.
*   **2.5.4. Key Public Methods:**
    *   `_extract_ip_from_server(self) -> Optional[str]` (Internal method):
        *   Purpose: Extracts the IP address component from the `self.server` URL string.
    *   `from_string(cls, proxy_str: str) -> ProxyConfig` (Static Method):
        *   Purpose: Creates a `ProxyConfig` instance from a string.
        *   Formats:
            *   `'ip:port:username:password'`
            *   `'ip:port'` (no authentication)
    *   `from_dict(cls, proxy_dict: dict) -> ProxyConfig` (Static Method):
        *   Purpose: Creates a `ProxyConfig` instance from a dictionary with keys "server", "username", "password", and "ip".
    *   `from_env(cls, env_var: str = "PROXIES") -> List[ProxyConfig]` (Static Method):
        *   Purpose: Loads a list of `ProxyConfig` objects from a comma-separated environment variable. Each proxy string in the variable should conform to the format accepted by `from_string`.
    *   `to_dict(self) -> dict`:
        *   Purpose: Converts the `ProxyConfig` instance to a dictionary: `{"server": ..., "username": ..., "password": ..., "ip": ...}`.
    *   `clone(self, **kwargs) -> ProxyConfig`:
        *   Purpose: Creates a copy of the `ProxyConfig` instance, allowing for overriding specific attributes with `kwargs`.

### 2.6. `HTTPCrawlerConfig`
Located in `crawl4ai.async_configs`.

*   **2.6.1. Purpose:**
    *   Description: The `HTTPCrawlerConfig` class holds configuration settings specific to direct HTTP-based crawling strategies (e.g., `AsyncHTTPCrawlerStrategy`), which do not use a full browser environment.
*   **2.6.2. Initialization (`__init__`)**
    *   Signature:
        ```python
        class HTTPCrawlerConfig:
            def __init__(
                self,
                method: str = "GET",
                headers: Optional[Dict[str, str]] = None,
                data: Optional[Dict[str, Any]] = None,
                json: Optional[Dict[str, Any]] = None,
                follow_redirects: bool = True,
                verify_ssl: bool = True,
            ): ...
        ```
    *   Parameters:
        *   `method (str, default: "GET")`: The HTTP method to use for the request (e.g., "GET", "POST", "PUT").
        *   `headers (Optional[Dict[str, str]], default: None)`: A dictionary of custom HTTP headers to send with the request.
        *   `data (Optional[Dict[str, Any]], default: None)`: Data to be sent in the body of the request, typically for "POST" or "PUT" requests (e.g., form data).
        *   `json (Optional[Dict[str, Any]], default: None)`: JSON data to be sent in the body of the request. If provided, the `Content-Type` header is typically set to `application/json`.
        *   `follow_redirects (bool, default: True)`: If `True`, the crawler will automatically follow HTTP redirects.
        *   `verify_ssl (bool, default: True)`: If `True`, SSL certificates will be verified. Set to `False` to ignore SSL errors (use with caution).
*   **2.6.3. Key Public Attributes/Properties:**
    *   All parameters listed in `__init__` are available as public attributes with the same names and types.
*   **2.6.4. Key Public Methods:**
    *   `from_kwargs(cls, kwargs: dict) -> HTTPCrawlerConfig` (Static Method):
        *   Purpose: Creates an `HTTPCrawlerConfig` instance from a dictionary of keyword arguments.
    *   `to_dict(self) -> dict`:
        *   Purpose: Converts the `HTTPCrawlerConfig` instance into a dictionary representation.
    *   `clone(self, **kwargs) -> HTTPCrawlerConfig`:
        *   Purpose: Creates a deep copy of the current `HTTPCrawlerConfig` instance. Keyword arguments can be provided to override specific attributes in the new instance.
    *   `dump(self) -> dict`:
        *   Purpose: Serializes the `HTTPCrawlerConfig` object to a dictionary.
    *   `load(cls, data: dict) -> HTTPCrawlerConfig` (Static Method):
        *   Purpose: Deserializes an `HTTPCrawlerConfig` object from a dictionary.

## 3. Enumerations and Helper Constants

### 3.1. `CacheMode` (Enum)
Located in `crawl4ai.cache_context`.

*   **3.1.1. Purpose:**
    *   Description: The `CacheMode` enumeration defines the different caching behaviors that can be applied to a crawl operation. It is used in `CrawlerRunConfig` to control how results are read from and written to the cache.
*   **3.1.2. Enum Members:**
    *   `ENABLE (str)`: Value: "ENABLE". Description: Enables normal caching behavior. The crawler will attempt to read from the cache first, and if a result is not found or is stale, it will perform the crawl and write the new result to the cache.
    *   `DISABLE (str)`: Value: "DISABLE". Description: Disables all caching. The crawler will not read from or write to the cache. Every request will be a fresh crawl.
    *   `READ_ONLY (str)`: Value: "READ_ONLY". Description: The crawler will only attempt to read from the cache. If a result is found, it will be used. If not, the crawl will not proceed further for that URL, and no new data will be written to the cache.
    *   `WRITE_ONLY (str)`: Value: "WRITE_ONLY". Description: The crawler will not attempt to read from the cache. It will always perform a fresh crawl and then write the result to the cache.
    *   `BYPASS (str)`: Value: "BYPASS". Description: The crawler will skip reading from the cache for this specific operation and will perform a fresh crawl. The result of this crawl *will* be written to the cache. This is the default `cache_mode` for `CrawlerRunConfig`.
*   **3.1.3. Usage:**
    *   Example:
        ```python
        from crawl4ai import CrawlerRunConfig, CacheMode
        config = CrawlerRunConfig(cache_mode=CacheMode.ENABLE) # Use cache fully
        config_bypass = CrawlerRunConfig(cache_mode=CacheMode.BYPASS) # Force fresh crawl, then cache
        ```

## 4. Serialization Helper Functions
Located in `crawl4ai.async_configs`.

### 4.1. `to_serializable_dict(obj: Any, ignore_default_value: bool = False) -> Dict`

*   **4.1.1. Purpose:**
    *   Description: This utility function recursively converts various Python objects, including `crawl4ai` configuration objects, into a dictionary format that is suitable for JSON serialization. It uses a `{ "type": "ClassName", "params": { ... } }` structure for custom class instances to enable proper deserialization later.
*   **4.1.2. Parameters:**
    *   `obj (Any)`: The Python object to be serialized.
    *   `ignore_default_value (bool, default: False)`: If `True`, when serializing class instances, parameters whose current values match their `__init__` default values might be excluded from the "params" dictionary. (Note: The exact behavior depends on the availability of default values in the class signature and handling of empty/None values).
*   **4.1.3. Returns:**
    *   `Dict`: A dictionary representation of the input object, structured for easy serialization (e.g., to JSON) and later deserialization by `from_serializable_dict`.
*   **4.1.4. Key Behaviors:**
    *   **Basic Types:** `str`, `int`, `float`, `bool`, `None` are returned as is.
    *   **Enums:** Serialized as `{"type": "EnumClassName", "params": enum_member.value}`.
    *   **Datetime Objects:** Serialized to their ISO 8601 string representation.
    *   **Lists, Tuples, Sets, Frozensets:** Serialized by recursively calling `to_serializable_dict` on each of their elements, returning a list.
    *   **Plain Dictionaries:** Serialized as `{"type": "dict", "value": {key: serialized_value, ...}}`.
    *   **Class Instances (e.g., Config Objects):**
        *   The object's class name is stored in the "type" field.
        *   Parameters from the `__init__` signature and attributes from `__slots__` (if defined) are collected.
        *   Their current values are recursively serialized and stored in the "params" dictionary.
        *   The structure is `{"type": "ClassName", "params": {"param_name": serialized_param_value, ...}}`.

### 4.2. `from_serializable_dict(data: Any) -> Any`

*   **4.2.1. Purpose:**
    *   Description: This utility function reconstructs Python objects, including `crawl4ai` configuration objects, from the serializable dictionary format previously created by `to_serializable_dict`.
*   **4.2.2. Parameters:**
    *   `data (Any)`: The dictionary (or basic data type) to be deserialized. This is typically the output of `to_serializable_dict` after being, for example, loaded from a JSON string.
*   **4.2.3. Returns:**
    *   `Any`: The reconstructed Python object (e.g., an instance of `BrowserConfig`, `LLMConfig`, a list, a plain dictionary, etc.).
*   **4.2.4. Key Behaviors:**
    *   **Basic Types:** `str`, `int`, `float`, `bool`, `None` are returned as is.
    *   **Typed Dictionaries (from `to_serializable_dict`):**
        *   If `data` is a dictionary and contains a "type" key:
            *   If `data["type"] == "dict"`, it reconstructs a plain Python dictionary from `data["value"]` by recursively deserializing its items.
            *   Otherwise, it attempts to locate the class specified by `data["type"]` within the `crawl4ai` module.
                *   If the class is an `Enum`, it instantiates the enum member using `data["params"]` (the enum value).
                *   If it's a regular class, it recursively deserializes the items in `data["params"]` and uses them as keyword arguments (`**kwargs`) to instantiate the class.
    *   **Lists:** If `data` is a list, it reconstructs a list by recursively calling `from_serializable_dict` on each of its elements.
    *   **Legacy Dictionaries:** If `data` is a dictionary but does not conform to the "type" key structure (for backward compatibility), it attempts to deserialize its values.

## 5. Cross-References and Relationships

*   **5.1. `BrowserConfig` Usage:**
    *   Typically instantiated once and passed to the `AsyncWebCrawler` constructor via its `config` parameter.
    *   `browser_config = BrowserConfig(headless=False)`
    *   `crawler = AsyncWebCrawler(config=browser_config)`
    *   It defines the global browser settings that will be used for all subsequent crawl operations unless overridden by `CrawlerRunConfig` on a per-run basis.
*   **5.2. `CrawlerRunConfig` Usage:**
    *   Passed to the `arun()` or `arun_many()` methods of `AsyncWebCrawler`.
    *   `run_config = CrawlerRunConfig(screenshot=True, cache_mode=CacheMode.BYPASS)`
    *   `result = await crawler.arun(url="https://example.com", config=run_config)`
    *   Allows for fine-grained control over individual crawl requests, overriding global settings from `BrowserConfig` or `AsyncWebCrawler`'s defaults where applicable (e.g., `user_agent`, `proxy_config`, `cache_mode`).
*   **5.3. `LLMConfig` Usage:**
    *   Instantiated and passed to LLM-based extraction strategies (e.g., `LLMExtractionStrategy`) or content filters (`LLMContentFilter`) during their initialization.
    *   `llm_conf = LLMConfig(provider="openai/gpt-4o-mini", api_token="sk-...")`
    *   `extraction_strategy = LLMExtractionStrategy(llm_config=llm_conf, schema=my_schema)`
*   **5.4. `GeolocationConfig` and `ProxyConfig` Usage:**
    *   `GeolocationConfig` is typically instantiated and assigned to the `geolocation` parameter of `CrawlerRunConfig`.
        *   `geo_conf = GeolocationConfig(latitude=34.0522, longitude=-118.2437)`
        *   `run_config = CrawlerRunConfig(geolocation=geo_conf)`
    *   `ProxyConfig` can be assigned to the `proxy_config` parameter of `BrowserConfig` (for a global proxy applied to all contexts) or `CrawlerRunConfig` (for a proxy specific to a single crawl run).
        *   `proxy_conf = ProxyConfig(server="http://myproxy:8080")`
        *   `browser_config = BrowserConfig(proxy_config=proxy_conf)` (global)
        *   `run_config = CrawlerRunConfig(proxy_config=proxy_conf)` (per-run)
*   **5.5. `HTTPCrawlerConfig` Usage:**
    *   Used when the `crawler_strategy` for `AsyncWebCrawler` is set to `AsyncHTTPCrawlerStrategy` (for non-browser-based HTTP requests).
    *   `http_conf = HTTPCrawlerConfig(method="POST", json={"key": "value"})`
    *   `http_strategy = AsyncHTTPCrawlerStrategy(http_crawler_config=http_conf)`
    *   `crawler = AsyncWebCrawler(crawler_strategy=http_strategy)`
    *   Alternatively, parameters like `method`, `data`, `json` can be passed directly to `arun()` when using `AsyncHTTPCrawlerStrategy` if they are part of the `CrawlerRunConfig`.

---


## Configuration Objects - Reasoning
Source: crawl4ai_config_objects_reasoning_content.llm.md

Okay, I'm ready to generate the detailed "Reasoning & Problem-Solving Framework" document for the `config_objects` component based on the provided outline and information.

```markdown
# Detailed Outline for crawl4ai - config_objects Component

**Target Document Type:** reasoning
**Target Output Filename Suggestion:** `llm_reasoning_config_objects.md`
**Library Version Context:** 0.6.3
**Outline Generation Date:** 2024-05-24
---

## 1. Introduction to Configuration in Crawl4ai

*   1.1. **The "Why": The Importance of Configuration**
    *   1.1.1. **Explaining how configuration objects provide granular control over crawling.**
        Crawl4ai is designed to tackle a wide array of web crawling and scraping tasks, from simple page fetches to complex interactions with dynamic websites and data extraction using LLMs. To manage this complexity effectively, Crawl4ai employs a system of dedicated configuration objects. These objects allow you to precisely define how the crawler behaves at different stages: how the browser is set up, how individual web pages are processed, and how interactions with Large Language Models (LLMs) are handled.
        Without a robust configuration system, you'd be forced to pass numerous, often-conflicting parameters to a single function, making your code hard to read, maintain, and debug. Configuration objects provide a structured, organized, and explicit way to tell Crawl4ai exactly what you want it to do.

    *   1.1.2. **Discussing the benefits of separating browser setup (`BrowserConfig`) from individual crawl behavior (`CrawlerRunConfig`) and LLM settings (`LLMConfig`).**
        The separation of concerns is a key design principle in Crawl4ai's configuration system:
        *   **`BrowserConfig`:** This object dictates the *environment* in which your crawls will run. It handles aspects like which browser to use (Chrome, Firefox), whether to run in headless mode, proxy settings, and browser identity (user-agent). This setup is typically done once per `AsyncWebCrawler` instance or per logical group of crawling tasks that require the same browser environment.
        *   **`CrawlerRunConfig`:** This object controls the specifics of *each individual crawl operation* (e.g., a single call to `arun()`). It defines how a particular URL is fetched, what content to extract, which JavaScript to execute on the page, caching behavior for that specific URL, and any media capture settings (screenshots, PDFs). This allows you to use the same browser setup to crawl different URLs with vastly different processing requirements.
        *   **`LLMConfig`:** When leveraging LLMs for tasks like content summarization or structured data extraction, `LLMConfig` centralizes all settings related to the LLM provider, model choice, API keys, and generation parameters (like temperature or max tokens). This keeps LLM-specific details separate from the core crawling and browser logic.

        This separation offers significant advantages:
        *   **Modularity:** You can define a browser setup once and reuse it for many different crawl tasks, each with its own `CrawlerRunConfig`.
        *   **Clarity:** It's easier to understand which settings affect which part of the crawling process.
        *   **Maintainability:** Changes to browser setup don't require modifying every crawl task's configuration, and vice-versa.
        *   **Flexibility:** You can easily swap out different LLM providers or models without altering your core crawling logic.

    *   1.1.3. **Overview of how these objects work together to achieve complex crawling scenarios.**
        Imagine you need to crawl a series of product pages.
        1.  You'd first instantiate an `AsyncWebCrawler` with a `BrowserConfig` that sets up a browser with, perhaps, a common desktop user-agent and no proxy.
        2.  Then, for each product page URL, you'd call `crawler.arun()` with a `CrawlerRunConfig`. This `CrawlerRunConfig` might specify:
            *   A `css_selector` to target only the main product information block.
            *   An `extraction_strategy` (like `JsonCssExtractionStrategy` or `LLMExtractionStrategy` with an `LLMConfig`) to pull out the product name, price, and description.
            *   `screenshot=True` to capture an image of the product page.
        3.  If another part of your task involves crawling blog posts from the same site, you could reuse the same `AsyncWebCrawler` (and thus the same `BrowserConfig`) but pass a *different* `CrawlerRunConfig` to `arun()` tailored for blog posts (e.g., different selectors, a different extraction strategy focused on article text).

        This layered approach allows you to build sophisticated crawlers by combining these configuration objects in a logical and manageable way.

*   1.2. **Core Philosophy: Flexibility and Reusability**
    *   1.2.1. **How the design promotes creating base configurations and specializing them.**
        A common and highly recommended pattern is to define "base" configuration objects that capture common settings for your project or for a specific type of task. Then, for individual crawls or variations, you can use the `clone()` method to create a new instance of the configuration object and override only the specific parameters you need to change. This significantly reduces code duplication and makes your configurations easier to manage.

        For example, you might have a `base_browser_config` for all your crawls and a `base_ecommerce_run_config` for scraping e-commerce sites. When scraping a specific e-commerce site, you'd clone `base_ecommerce_run_config` and only adjust, say, the `css_selector` or `extraction_strategy`.

    *   1.2.2. **The role of `clone()`, `dump()`, and `load()` in managing configuration lifecycle.**
        Crawl4ai's configuration objects come with built-in methods to streamline their management:
        *   **`clone(**kwargs)`:** Creates a deep copy of the configuration object, allowing you to override specific parameters for the new instance without affecting the original. This is perfect for creating specialized versions from a base configuration.
        *   **`dump()`:** Serializes the configuration object into a Python dictionary. This dictionary can then be easily saved to a JSON or YAML file, stored in a database, or transmitted over a network.
        *   **`load(data: dict)`:** A static method on each configuration class that reconstructs a configuration object from a dictionary (typically one produced by `dump()`). This allows you to load configurations from external sources, making your crawling setup more dynamic and shareable.

        These methods facilitate:
        *   **Versioning:** Store different configuration versions in files.
        *   **Sharing:** Easily share configurations between different parts of your application or with team members.
        *   **Dynamic Setup:** Load configurations based on runtime parameters or external inputs.

*   1.3. **Scope of This Guide**
    *   1.3.1. **What this guide will cover (deep dive into reasoning for `BrowserConfig`, `CrawlerRunConfig`, `LLMConfig`, `GeolocationConfig`, `ProxyConfig`, `HTTPCrawlerConfig`).**
        This guide focuses on the *reasoning* behind using various configuration objects and their parameters. We'll explore *how* to make effective choices, *why* certain features are designed the way they are, and *when* to use specific settings to solve common crawling challenges. We will perform a deep dive into:
        *   `BrowserConfig`: For setting up the browser's environment and identity.
        *   `CrawlerRunConfig`: For tailoring individual crawl operations.
        *   `LLMConfig`: For configuring interactions with Large Language Models.
        *   And touch upon specialized configs like `GeolocationConfig`, `ProxyConfig`, and `HTTPCrawlerConfig` for specific use cases.
    *   1.3.2. **Briefly mentioning where to find exhaustive API parameter lists (referencing a "memory" document or API docs).**
        While this guide provides practical examples and discusses many key parameters, it is not an exhaustive API reference. For a complete list of all available parameters, their types, default values, and concise descriptions, please refer to the official API documentation or the "Foundational Memory" document for `config_objects` if available. This guide aims to complement that factual information by providing the "how-to" and "why."

## 2. Mastering `BrowserConfig`: Setting Up Your Crawler's Identity and Environment

*   2.1. **Understanding `BrowserConfig`: Beyond Default Behavior**
    *   2.1.1. **When is the default `BrowserConfig` sufficient?**
        If you're performing simple crawls of public, static websites that don't have strong anti-bot measures, the default `BrowserConfig` (which you get by simply instantiating `AsyncWebCrawler()` without a custom config) might work perfectly fine. It typically launches a headless Chromium browser with a generic user-agent. For quick tests or very straightforward tasks, this is often all you need.

    *   2.1.2. **Key scenarios demanding `BrowserConfig` customization:**
        You'll need to customize `BrowserConfig` when your crawling tasks become more complex or when you encounter challenges like:
        *   **Evading Bot Detection:** Many websites employ techniques to identify and block automated crawlers. Customizing user-agents, browser hints, and even browser behavior can help your crawler appear more like a regular human user.
        *   **Testing Geo-Specific Content:** If a website serves different content based on the user's geographic location, you'll need to configure the browser to simulate originating from that specific region (using `GeolocationConfig` within `CrawlerRunConfig`, but also ensuring your browser's IP via a proxy in `BrowserConfig` aligns).
        *   **Using Proxies:** To rotate IP addresses, mask your origin, or access geo-restricted content, configuring proxies is essential.
        *   **Managing Browser Resources and Performance:** For large-scale crawls, controlling browser features (like disabling images or JavaScript) or using different browser modes (like Docker) can significantly impact performance and resource consumption.
        *   **Persistent Sessions and Authenticated Crawling:** If you need to log into a website and maintain that session across multiple crawl operations, `BrowserConfig` provides options for persistent contexts.

*   2.2. **Strategic `BrowserConfig` Customizations**
    *   2.2.1. **Crafting a Believable Browser Identity**
        *   **`user_agent` and `user_agent_mode`:**
            *   **Why faking User-Agents can be crucial:** The User-Agent string is one of the first pieces of information a web server receives. Many sites use it to tailor content or, more critically for crawlers, to identify and block non-standard or known bot User-Agents. Using a common, legitimate browser User-Agent makes your crawler less conspicuous.
            *   **Choosing between a static `user_agent` and `user_agent_mode="random"`:**
                *   **Static `user_agent`:** Use this if you want to consistently mimic a specific browser and OS combination. This can be useful for targeting mobile-specific views or ensuring consistent rendering.
                *   **`user_agent_mode="random"`:** Crawl4ai will use its built-in `ValidUAGenerator` to pick a common, valid User-Agent for each new browser context (or potentially page, depending on strategy details). This can help avoid patterns if a site tracks User-Agents over time. The `user_agent_generator_config` parameter can be used to further customize the random generation if needed, for example, to only generate User-Agents for a specific OS or device type.
            *   **Trade-offs and when to use each:**
                *   Static: More predictable, good for specific targeting.
                *   Random: Better for avoiding simple User-Agent-based blocking over many requests, but ensure the randomness still aligns with common browser profiles.
            *   **Code Example: Setting a specific User-Agent vs. using random generation.**
                ```python
                from crawl4ai import BrowserConfig

                # Specific User-Agent
                config_specific_ua = BrowserConfig(
                    user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                )
                print(f"Specific UA Config: {config_specific_ua.user_agent}")

                # Random User-Agent (default behavior when user_agent_mode="random" or just not set with a static UA)
                config_random_ua = BrowserConfig(user_agent_mode="random")
                # Note: The actual UA is generated when the browser context is created by AsyncWebCrawler
                # We can inspect the generated UA through the browser_hint which is derived from it.
                print(f"Random UA Config (Hint): {config_random_ua.browser_hint}")
                # Example for generating one manually:
                from crawl4ai.user_agent_generator import ValidUAGenerator
                ua_gen = ValidUAGenerator()
                random_ua_example = ua_gen.generate()
                print(f"Example Random UA: {random_ua_example}")
                ```

        *   **`browser_hint` and `sec-ch-ua` headers:**
            *   **How these contribute to a more convincing browser profile:** Modern browsers send Client Hints (like `Sec-CH-UA`, `Sec-CH-UA-Mobile`, `Sec-CH-UA-Platform`) that provide more granular information about the browser than the traditional User-Agent string. Crawl4ai automatically generates a plausible `browser_hint` (which populates `Sec-CH-UA`) based on the `user_agent` to enhance authenticity.
            *   **Ensuring consistency:** It's vital that your Client Hints are consistent with your main User-Agent string. Crawl4ai aims to do this automatically. If you manually set headers, ensure they don't contradict your chosen `user_agent`.

    *   2.2.2. **Headless vs. Headful: The Visibility Trade-off (`headless`)**
        *   **Why use headless mode (`headless=True`, default):**
            *   **Servers & Automation:** Ideal for running crawlers on servers or in automated CI/CD pipelines where no graphical interface is available or needed.
            *   **Speed & Resources:** Generally consumes fewer resources than a full GUI browser, leading to faster crawls, especially at scale.
        *   **When headful mode (`headless=False`) is essential:**
            *   **Debugging:** Visually inspecting what the browser sees is invaluable for debugging issues with page rendering, element selection, or unexpected site behavior.
            *   **Anti-Bot Measures:** Some sophisticated websites can detect headless browsers (e.g., by checking for specific JavaScript properties or rendering inconsistencies). Running in headful mode can sometimes bypass these checks.
        *   **Impact on performance and detectability:** Headless is faster but potentially more detectable. Headful is slower, uses more resources, but can appear more like a real user.
        *   **Decision Guide: Choosing the right mode for your task.**
            *   Start with `headless=True` for production and automated runs.
            *   Switch to `headless=False` when:
                *   Debugging selectors or interactions.
                *   You suspect the site is blocking headless browsers.
                *   You need to manually perform actions like solving a CAPTCHA during a setup phase.
            ```python
            from crawl4ai import BrowserConfig

            # Default: Headless
            config_headless = BrowserConfig() # headless=True is the default
            print(f"Headless mode: {config_headless.headless}")

            # Explicitly Headful for debugging
            config_headful = BrowserConfig(headless=False)
            print(f"Headful mode: {config_headful.headless}")
            ```

    *   2.2.3. **Controlling the Browser's Lifecycle and Environment**
        *   **`browser_mode` ("builtin", "dedicated", "cdp", "docker"):**
            *   **Explaining each mode and its typical use case:**
                *   `"dedicated"` (Default): Launches a fresh, isolated browser instance for the `AsyncWebCrawler`. This is good for most use cases, ensuring no state leaks between different crawler instances if you were to run multiple in the same script (though typically you'd use one `AsyncWebCrawler` and multiple `arun` calls).
                *   `"builtin"`: (More advanced) Intended for scenarios where Crawl4ai manages a long-lived browser process in the background, potentially shared across different crawler objects or Python processes. This can be more resource-efficient for very frequent, short-lived crawl tasks. It leverages `use_managed_browser=True` and a CDP connection to this managed browser.
                *   `"cdp"` (or `use_managed_browser=True` with a `cdp_url`): Allows you to connect Crawl4ai to an *existing* Chrome/Chromium browser instance that has been launched with a remote debugging port. Useful if you want to control a browser you've launched manually or one managed by another tool.
                *   `"docker"`: Facilitates running the browser inside a Docker container. Crawl4ai can manage launching a browser in a container and connecting to it. This is excellent for consistent environments and isolating dependencies. (Requires Docker setup and relevant browser images).
            *   **"dedicated":**
                *   Pros: Simple to understand, good isolation for typical `AsyncWebCrawler` usage.
                *   Cons: Can be resource-intensive if you're instantiating many `AsyncWebCrawler` objects each with its own dedicated browser, instead of reusing one `AsyncWebCrawler` for multiple `arun` calls.
            *   **"cdp" / `use_managed_browser=True`:** This implies that Crawl4ai will try to connect to a browser via the Chrome DevTools Protocol (CDP).
                *   If `cdp_url` is provided in `BrowserConfig`, it uses that.
                *   If `browser_mode` is "builtin" or "docker", Crawl4ai's internal `ManagedBrowser` (or a Docker strategy) would start a browser and provide the `cdp_url` internally.
        *   **`use_persistent_context` and `user_data_dir`:**
            *   **The power of persistent sessions:** When `use_persistent_context=True`, Playwright (the underlying browser automation library) attempts to save and reuse browser state (cookies, local storage, etc.) across sessions, using the directory specified by `user_data_dir`. This is invaluable for:
                *   **Authenticated Crawls:** Log in once (manually or scripted), and subsequent crawls with the same `user_data_dir` can often bypass the login process.
                *   **Maintaining Preferences:** Site preferences, "accept cookies" banners, etc., can be remembered.
            *   **Workflow for authenticated crawling:**
                1.  **Initial Setup Run:**
                    ```python
                    # First run: Login and save session
                    login_browser_config = BrowserConfig(
                        headless=False,  # Often easier to do initial login with a visible browser
                        use_persistent_context=True,
                        user_data_dir="./my_browser_profile" # Choose a path
                    )
                    # ... (code to navigate to login page, fill credentials, submit using crawler.arun() with appropriate js_code)
                    # After successful login, close the crawler. The session is saved in "./my_browser_profile".
                    ```
                2.  **Subsequent Runs:**
                    ```python
                    # Subsequent runs: Reuse the saved profile
                    reuse_browser_config = BrowserConfig(
                        headless=True, # Can now run headless
                        use_persistent_context=True,
                        user_data_dir="./my_browser_profile" # Must be the same path
                    )
                    # ... (crawler.arun() calls to access protected pages will now use the saved session)
                    ```
            *   **Best Practice:** Use distinct `user_data_dir` paths for different websites or different user accounts to keep sessions isolated.
            *   **Note:** `use_persistent_context=True` automatically implies `use_managed_browser=True` because persistent contexts are a feature of Playwright's browser contexts launched via CDP.

    *   2.2.4. **Navigating Networks: Proxies and SSL (`proxy_config`, `ignore_https_errors`)**
        *   **Integrating Proxies with `proxy_config` (referencing `ProxyConfig` object):**
            *   **Why use proxies:**
                *   **IP Rotation:** Avoid rate limits or blocks by distributing requests across multiple IP addresses.
                *   **Geo-Targeting:** Access content specific to a certain geographic region by using a proxy located in that region.
                *   **Anonymity/Privacy:** Mask your crawler's true origin IP (though be mindful of the proxy provider's logging policies).
            *   **How to structure the `proxy_config` dictionary:**
                The `proxy_config` parameter in `BrowserConfig` expects a dictionary compatible with Playwright's proxy settings. Typically, this includes:
                *   `server`: The proxy server address (e.g., `"http://proxy.example.com:8080"` or `"socks5://proxy.example.com:1080"`).
                *   `username` (optional): Username for proxy authentication.
                *   `password` (optional): Password for proxy authentication.
                A `ProxyConfig` object from `crawl4ai.async_configs` can also be used here by converting it to a dictionary with `my_proxy_config.to_dict()`.
            *   **Workflow: Implementing a basic proxy rotation:**
                While Crawl4ai has a more advanced `ProxyRotationStrategy` (covered elsewhere), a simple rotation can be achieved by dynamically creating `BrowserConfig` instances:
                ```python
                # Conceptual: Basic proxy rotation
                proxies = [
                    {"server": "http://proxy1.example.com:8080", "username": "user1", "password": "p1"},
                    {"server": "http://proxy2.example.com:8080", "username": "user2", "password": "p2"},
                ]
                current_proxy_index = 0

                def get_next_proxy_config_dict():
                    nonlocal current_proxy_index
                    proxy_details = proxies[current_proxy_index % len(proxies)]
                    current_proxy_index += 1
                    return proxy_details

                # In your loop or arun_many setup:
                # proxy_dict = get_next_proxy_config_dict()
                # browser_cfg = BrowserConfig(proxy_config=proxy_dict)
                # crawler = AsyncWebCrawler(config=browser_cfg)
                # await crawler.arun(...)
                ```
            *   **Code Example: Configuring a single authenticated proxy.**
                ```python
                from crawl4ai import BrowserConfig

                proxy_settings = {
                    "server": "http://myproxy.service.com:3128",
                    "username": "proxy_user",
                    "password": "proxy_password"
                }
                config_with_proxy = BrowserConfig(proxy_config=proxy_settings)

                # To use with AsyncWebCrawler:
                # async with AsyncWebCrawler(config=config_with_proxy) as crawler:
                #     result = await crawler.arun(url="https://api.ipify.org?format=json") # Check your IP
                #     print(result.html)
                ```
        *   **`ignore_https_errors`:**
            *   **When this might be needed:** Primarily for development or testing environments where you might encounter self-signed SSL certificates or other non-production SSL configurations.
            *   **Warning:** Setting `ignore_https_errors=True` in a production environment or when accessing sensitive sites is **highly discouraged** as it bypasses crucial security checks, making your crawler vulnerable to man-in-the-middle attacks. Use with extreme caution.

    *   2.2.5. **Fine-tuning for Performance (`text_mode`, `light_mode`, `extra_args`)**
        *   **`text_mode=True`:**
            *   **Benefits:** This mode attempts to disable the loading of images, CSS, and fonts, and may also disable JavaScript depending on the underlying strategy implementation. This can significantly speed up page loads and reduce bandwidth consumption, especially for sites where you are primarily interested in textual content.
        *   **`light_mode=True`:**
            *   **How it differs:** `light_mode` is a more aggressive optimization. It not only includes `text_mode` behaviors but also enables a set of browser launch arguments (`BROWSER_DISABLE_OPTIONS` in `browser_manager.py`) designed to disable various background features, rendering optimizations, and GPU acceleration. This is aimed at achieving maximum performance gains, especially in resource-constrained environments or for very large-scale crawls where every millisecond counts.
        *   **`extra_args`:**
            *   **Unlocking advanced browser capabilities and optimizations:** This parameter allows you to pass a list of custom command-line arguments directly to the browser when it's launched. This is a powerful way to enable or disable specific browser features not covered by other `BrowserConfig` options.
            *   **Common and useful flags:**
                *   `"--disable-gpu"`: Can resolve issues on systems without proper GPU drivers or in headless environments.
                *   `"--no-sandbox"`: Often required when running Chrome/Chromium inside Docker containers, especially as root.
                *   `"--disable-extensions"`: Prevents any installed browser extensions from interfering with the crawl.
                *   `"--disable-dev-shm-usage"`: Can prevent crashes in Docker due to limited shared memory.
            *   **Where to find lists of available browser arguments:** Search for "Chromium command line switches" or "Firefox command line options" for comprehensive lists.
            *   **Code Example:**
                ```python
                from crawl4ai import BrowserConfig

                performance_config = BrowserConfig(
                    light_mode=True, # Includes text_mode and other optimizations
                    extra_args=["--disable-blink-features=AutomationControlled"] # Example: Hiding automation flags
                )
                # Use this config with AsyncWebCrawler
                ```

*   2.3. **Best Practices for `BrowserConfig`**
    *   2.3.1. **Start simple, add complexity as needed:** Don't over-configure from the outset. Begin with defaults and only add customizations as specific needs or problems arise.
    *   2.3.2. **Prioritize realistic browser profiles for stealth:** If evading bot detection is a goal, ensure your `user_agent`, `browser_hint` (implicitly handled by `user_agent`), and other settings present a common and consistent browser profile.
    *   2.3.3. **Use persistent contexts for authenticated sessions:** Leverage `use_persistent_context=True` and `user_data_dir` for sites requiring login, to avoid re-authenticating on every run.
    *   2.3.4. **Be mindful of resource consumption:** Headful mode, multiple "dedicated" browser instances, and not using `light_mode` or `text_mode` can consume more resources. Optimize for your environment and scale.

*   2.4. **Troubleshooting Common `BrowserConfig` Issues**
    *   2.4.1. **Browser not launching or crashing:**
        *   Check Playwright installation: Run `playwright install` or `crawl4ai-setup`.
        *   Missing system dependencies: Especially on Linux, ensure all required libraries for the browser (e.g., Chromium dependencies) are installed. `crawl4ai-doctor` might help.
        *   `extra_args` conflicts: Some launch arguments might conflict or be invalid.
        *   Resource limits: Particularly in Docker or VMs, ensure sufficient CPU/memory. Consider `--disable-dev-shm-usage` if using Docker.
    *   2.4.2. **Pages not rendering correctly (potential `user_agent` or JS issues):**
        *   Try `headless=False` to visually inspect.
        *   Ensure `javascript_enabled=True` in `CrawlerRunConfig` (default) if the site relies heavily on JS.
        *   Experiment with different `user_agent` strings; some sites serve different content or block based on UA.
    *   2.4.3. **Proxy connection failures:**
        *   Verify proxy server address, port, username, and password.
        *   Test the proxy outside of Crawl4ai (e.g., with `curl` or in a browser) to ensure it's working.
        *   Check for firewall issues blocking connections to the proxy.
    *   2.4.4. **Debugging Tip: Always try `headless=False` first.** This is the single most useful step for diagnosing many browser-related issues, as it lets you see exactly what the browser is doing (or not doing).

## 3. Tailoring Crawls with `CrawlerRunConfig`: Precision in Every Operation

*   3.1. **The Purpose of `CrawlerRunConfig`: Granular Control per Crawl**
    *   3.1.1. **Why it's distinct from `BrowserConfig`:**
        While `BrowserConfig` sets up the *global environment* for the browser (how it launches, its identity, network settings), `CrawlerRunConfig` dictates the *specifics for a single `arun()` operation*. This separation is crucial because you might use the same browser instance (configured once with `BrowserConfig`) to crawl multiple URLs, each requiring different processing steps. For example, one URL might need a screenshot, another might need JavaScript execution, and a third might target a specific CSS selector for content extraction.

    *   3.1.2. **How it empowers you to customize each `arun()` or tasks within `arun_many()`:**
        By passing a `CrawlerRunConfig` object to `crawler.arun()` (or as part of the task definition in `crawler.arun_many()`), you gain fine-grained control over:
        *   What part of the page to focus on (`css_selector`, `target_elements`).
        *   What content to exclude (`excluded_tags`, `excluded_selector`).
        *   How content is extracted and transformed (`extraction_strategy`, `markdown_generator`).
        *   Page interactions (`js_code`, `wait_for`).
        *   Media capture (`screenshot`, `pdf`).
        *   Link and media filtering.
        *   Caching behavior for that specific URL.
        *   And much more.
        This allows for highly tailored and efficient crawling workflows.

*   3.2. **Strategies for Effective Content Extraction**
    *   3.2.1. **Scoping Your Extraction (`css_selector`, `target_elements`)**
        *   **`css_selector`:**
            *   **Impact:** This parameter is powerful. When set, Crawl4ai attempts to isolate the HTML content to *only the element(s) matching this CSS selector* **before** most other processing (like cleaning, Markdown generation, or structured extraction) occurs. This means the `cleaned_html` and subsequently the `markdown` output will be derived *only* from this selected portion.
            *   **Use Case:** You want to extract only the main article body from a news website, ignoring headers, footers, sidebars, and ads. Setting `css_selector=".article-content"` would achieve this.
            *   **Benefit:** Significantly reduces noise and focuses all downstream processing on the relevant content, which can improve the quality of Markdown and structured data, and also speed up LLM-based extractions by providing less context.
        *   **`target_elements`:**
            *   **How it differs:** Unlike `css_selector` which pre-filters the raw HTML, `target_elements` (a list of CSS selectors) primarily influences *downstream processing*, particularly Markdown generation and structured data extraction strategies like `JsonCssExtractionStrategy`. The initial `cleaned_html` (if `css_selector` is not also used) will still represent the broader page content. However, when generating Markdown or extracting structured fields, only the content within these `target_elements` will be considered.
            *   **Use Case:** You want to generate Markdown primarily from the main article body (`<article>`) but also need to extract the author's name from a `<div class="author-bio">` and the publication date from a `<time>` element, which might be outside the main article. You could set `target_elements=["article", ".author-bio", "time"]`.
            *   **Benefit:** Allows for more nuanced content selection for different purposes. You can get a broad `cleaned_html` (useful for general context) while focusing Markdown generation and specific data extraction on distinct parts of the page.
        *   **Decision Guide: `css_selector` for pre-filtering raw HTML vs. `target_elements` for post-cleaning focus.**
            *   Use `css_selector` when you are confident that *all* relevant information for *all* downstream tasks (Markdown, structured extraction, etc.) is contained within a single, selectable region of the page. This is the most aggressive filtering.
            *   Use `target_elements` when you need to generate Markdown or extract data from *multiple, potentially disparate sections* of the page, or when your `extraction_strategy` needs to "see" more of the page structure to correctly identify fields that might be outside the main content block.
            *   You *can* use them together: `css_selector` would first limit the HTML, and then `target_elements` would further refine which parts of that limited HTML are used for specific downstream tasks.
        *   **Code Example: Illustrating the difference in output.**
            ```python
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, NoExtractionStrategy, DefaultMarkdownGenerator

            sample_html = """
            <html><body>
                <header><h1>Site Title</h1><nav><a>Home</a></nav></header>
                <main id='content'>
                    <article class='main-story'><h2>Article Heading</h2><p>Main article text.</p></article>
                    <aside class='sidebar'><p>Sidebar content.</p></aside>
                </main>
                <footer><p>Copyright info</p></footer>
            </body></html>
            """

            async def run_example():
                async with AsyncWebCrawler() as crawler:
                    # Scenario 1: Using css_selector
                    config_css = CrawlerRunConfig(css_selector="article.main-story")
                    result_css = await crawler.arun(url=f"raw://{sample_html}", config=config_css)
                    print(f"--- With css_selector='article.main-story' ---")
                    print(f"Cleaned HTML (snippet):\n{result_css.cleaned_html[:200]}\n") # Will be only the article
                    print(f"Markdown:\n{result_css.markdown.raw_markdown}\n")

                    # Scenario 2: Using target_elements
                    # Note: DefaultMarkdownGenerator implicitly uses target_elements if set.
                    # If no target_elements, it uses the whole cleaned_html (or content from css_selector if that's set).
                    config_target = CrawlerRunConfig(
                        target_elements=["article.main-story", "aside.sidebar"],
                        # To make the effect clear, let's use a custom Markdown generator
                        # that explicitly respects target_elements for its input.
                        # The default one would also work similarly.
                        markdown_generator=DefaultMarkdownGenerator()
                    )
                    result_target = await crawler.arun(url=f"raw://{sample_html}", config=config_target)
                    print(f"--- With target_elements=['article.main-story', 'aside.sidebar'] ---")
                    print(f"Cleaned HTML (snippet):\n{result_target.cleaned_html[:200]}\n") # Will be the whole page
                    print(f"Markdown (focused on targets):\n{result_target.markdown.raw_markdown}\n")
                    # The markdown here will primarily be from the article and sidebar combined.

                    # Scenario 3: Using both css_selector and target_elements
                    config_both = CrawlerRunConfig(
                        css_selector="main#content", # First, limit to main
                        target_elements=["article.main-story"] # Then, for markdown, only the article within main
                    )
                    result_both = await crawler.arun(url=f"raw://{sample_html}", config=config_both)
                    print(f"--- With css_selector='main#content' AND target_elements=['article.main-story'] ---")
                    print(f"Cleaned HTML (snippet):\n{result_both.cleaned_html[:200]}\n") # Will be main#content
                    print(f"Markdown (focused on article within main):\n{result_both.markdown.raw_markdown}\n")


            await run_example()
            ```

    *   3.2.2. **Refining Content by Exclusion (`excluded_tags`, `excluded_selector`)**
        *   **How `excluded_tags` globally removes unwanted tag types:** This parameter takes a list of HTML tag names (e.g., `['script', 'style', 'nav', 'footer', 'header', 'form', 'button', 'input', 'textarea', 'select', 'option']`). Before any other processing, Crawl4ai will remove all occurrences of these tags and their content from the HTML. This is a blunt but effective way to strip common non-content elements.
        *   **Using `excluded_selector` for more specific CSS-based exclusions:** If you need to remove elements based on their class, ID, or other attributes (e.g., ad banners with class `.ad-banner`, comment sections in `<div id="comments">`), provide a CSS selector string. All matching elements will be removed. This is more targeted than `excluded_tags`.
        *   **Impact on `cleaned_html` and subsequent Markdown/extraction:** Both `excluded_tags` and `excluded_selector` modify the HTML *before* it becomes the `cleaned_html` and before Markdown generation or structured data extraction. This means the excluded content will not appear in any downstream outputs.
        *   **Code Example: Removing navigation and footer before Markdown generation.**
            ```python
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

            sample_html_nav_footer = """
            <html><body>
                <nav><a>Home</a> <a>About</a></nav>
                <article><p>Main content here.</p></article>
                <div class="advertisement"><p>Buy now!</p></div>
                <footer><p>&copy; 2024</p></footer>
            </body></html>
            """

            async def run_exclusion_example():
                config_exclusions = CrawlerRunConfig(
                    excluded_tags=['nav', 'footer'],
                    excluded_selector=".advertisement"
                )
                async with AsyncWebCrawler() as crawler:
                    result = await crawler.arun(url=f"raw://{sample_html_nav_footer}", config=config_exclusions)
                    print("--- HTML after exclusions ---")
                    print(result.cleaned_html)
                    print("\n--- Markdown after exclusions ---")
                    print(result.markdown.raw_markdown)
            
            await run_exclusion_example()
            # Expected output will not contain <nav>, <footer>, or <div class="advertisement">
            ```

    *   3.2.3. **Choosing Your Extraction Toolkit (`extraction_strategy`, `chunking_strategy`, `markdown_generator`, `only_text`)**
        *   **The default pipeline:** If you don't specify these, Crawl4ai uses:
            *   `WebScrapingStrategy` (which handles basic HTML cleaning, link/media extraction).
            *   `DefaultMarkdownGenerator` (which converts the `cleaned_html` to Markdown).
            *   `NoExtractionStrategy` (meaning `result.extracted_content` will be `None`).
        *   **When to use `only_text=True`:** If your sole goal is to get a plain text representation of the page's main content, and you don't need Markdown, HTML structure, or structured data, setting `only_text=True` can be a quick and efficient option. It typically tries to extract the "body" text and may perform some basic cleaning. The result will be in `result.markdown.raw_markdown` (despite the name, it will be plain text).
        *   **Plugging in `LLMExtractionStrategy`:**
            *   **Why:** This strategy is powerful when:
                *   The data you want is not easily selectable with CSS or XPath (e.g., it's embedded in prose).
                *   The website structure is inconsistent across pages.
                *   You need to infer or transform data based on context.
            *   **Workflow:**
                1.  Define a Pydantic model representing the schema of the data you want to extract.
                2.  Instantiate an `LLMConfig` with your LLM provider details.
                3.  Instantiate `LLMExtractionStrategy(schema=YourPydanticModel.model_json_schema(), llm_config=your_llm_config, instruction="Your specific extraction instructions...")`.
                4.  Pass this strategy to `CrawlerRunConfig(extraction_strategy=your_llm_extraction_strategy)`.
                The extracted data will be available as a JSON string in `result.extracted_content`.
            *   (Cross-reference to `LLMConfig` section for LLM-specific settings like `provider`, `api_token`, `temperature`).
        *   **Custom `chunking_strategy`:**
            *   By default, `LLMExtractionStrategy` might send the entire relevant HTML (or Markdown, depending on its `input_format`) to the LLM. If this content is too large for the LLM's context window, you can provide a `chunking_strategy` (e.g., `RegexChunking`) to `LLMExtractionStrategy`. This strategy will break the input into smaller, manageable chunks before sending them to the LLM.
            *   When to use: For very long documents where you still want to apply LLM extraction across the entire content.
        *   **Custom `markdown_generator`:**
            *   If the `DefaultMarkdownGenerator` doesn't produce Markdown in the exact style or with the specific conversions you need, you can implement your own class inheriting from `MarkdownGenerationStrategy` and pass an instance to `CrawlerRunConfig(markdown_generator=YourCustomMarkdownGenerator())`.
        *   **Code Example: Using `CrawlerRunConfig` with `LLMExtractionStrategy` for structured data from an article.**
            ```python
            from crawl4ai import (
                AsyncWebCrawler, CrawlerRunConfig, LLMConfig, 
                LLMExtractionStrategy, NoExtractionStrategy
            )
            from pydantic import BaseModel, Field
            import json
            import os

            # Define Pydantic schema for extraction
            class ArticleInfo(BaseModel):
                headline: str = Field(..., description="The main headline of the article")
                author: str = Field(None, description="The author of the article, if available")
                publication_date: str = Field(None, description="The publication date, if available")

            sample_article_html = """
            <html><body>
                <article>
                    <h1>Amazing Discovery in AI</h1>
                    <p class='byline'>By Dr. AI Expert on 2024-05-24</p>
                    <p>Scientists today announced a breakthrough...</p>
                </article>
            </body></html>
            """

            async def run_llm_extraction():
                # Configure LLM (using OpenAI for this example)
                # Ensure OPENAI_API_KEY is set in your environment
                llm_conf = LLMConfig(provider="openai/gpt-4o-mini", api_token=os.getenv("OPENAI_API_KEY"))
                
                extraction_strategy = LLMExtractionStrategy(
                    llm_config=llm_conf,
                    schema=ArticleInfo.model_json_schema(),
                    instruction="Extract the headline, author, and publication date from the article content."
                )

                config_llm_extract = CrawlerRunConfig(
                    extraction_strategy=extraction_strategy,
                    # LLMExtractionStrategy defaults to "markdown" input, so no need to change input_format
                    # unless you want to feed it raw HTML, then set extraction_strategy.input_format = "html"
                )

                async with AsyncWebCrawler() as crawler:
                    result = await crawler.arun(url=f"raw://{sample_article_html}", config=config_llm_extract)
                    if result.success and result.extracted_content:
                        extracted_data = json.loads(result.extracted_content)
                        # LLMExtractionStrategy often returns a list of extracted items
                        if isinstance(extracted_data, list) and extracted_data:
                             article_info = ArticleInfo(**extracted_data[0]) # Assuming one main article
                             print(f"Headline: {article_info.headline}")
                             print(f"Author: {article_info.author}")
                             print(f"Date: {article_info.publication_date}")
                        elif isinstance(extracted_data, dict) : # Sometimes it might be a single object
                             article_info = ArticleInfo(**extracted_data)
                             print(f"Headline: {article_info.headline}")
                             print(f"Author: {article_info.author}")
                             print(f"Date: {article_info.publication_date}")
                    else:
                        print(f"Extraction failed or no content: {result.error_message}")
            
            # await run_llm_extraction() # Uncomment to run, requires OPENAI_API_KEY
            ```

    *   3.2.4. **Attribute Handling (`keep_data_attributes`, `keep_attrs`)**
        *   **Why `keep_data_attributes=True` can be useful:** HTML `data-*` attributes are often used by JavaScript frameworks to store state or custom metadata. By default, many cleaning processes might strip these. If this data is important for your extraction or understanding of the page, set `keep_data_attributes=True`.
        *   **Using `keep_attrs` to preserve specific essential attributes:** `keep_attrs` takes a list of attribute names (e.g., `['href', 'src', 'id', 'class', 'title']`). During the HTML cleaning process, only these specified attributes (and `data-*` attributes if `keep_data_attributes` is true) will be retained on tags. All other attributes will be removed. This helps in producing cleaner, more focused HTML for downstream tasks.
            *   Default important attributes like `href` for `<a>` tags and `src` for `<img>` tags are usually kept by the default scraping strategy (`WebScrapingStrategy`) logic, but `keep_attrs` provides explicit control.
            ```python
            from crawl4ai import CrawlerRunConfig

            # Keep only 'id' and 'data-custom' attributes
            config_attrs = CrawlerRunConfig(
                keep_attrs=['id'], 
                keep_data_attributes=True # This would keep 'data-custom'
            )
            # Example of how it affects cleaned_html:
            # HTML: <div id="main" class="container" data-custom="value" style="color:red">Content</div>
            # Cleaned (conceptual): <div id="main" data-custom="value">Content</div>
            ```

*   3.3. **Managing Page Dynamics and Interactions**
    *   3.3.1. **Interacting with Dynamic Pages (`js_code`, `wait_for`, `scan_full_page`, `scroll_delay`)**
        *   **`js_code`:**
            *   **Executing arbitrary JavaScript:** This is your primary tool for interacting with page elements like clicking buttons, filling forms, expanding sections, or triggering custom JavaScript functions defined on the page.
            *   **Single strings vs. lists of JS commands:**
                *   A single string: For a simple, one-off action.
                *   A list of strings: For a sequence of actions. Crawl4ai will execute them in order.
            *   **Code Example: Clicking a "Load More" button multiple times (conceptual).**
                ```python
                # Conceptual - actual selector depends on the target site
                js_load_more_multiple = [
                    "document.querySelector('.load-more-button').click();",
                    "await new Promise(r => setTimeout(r, 2000));", # Wait 2s for content
                    "document.querySelector('.load-more-button').click();",
                    "await new Promise(r => setTimeout(r, 2000));", # Wait again
                    "document.querySelector('.load-more-button').click();"
                ]
                config_load_more = CrawlerRunConfig(js_code=js_load_more_multiple)
                # result = await crawler.arun(url="some-infinite-scroll-page.com", config=config_load_more)
                ```
        *   **`wait_for`:**
            *   **Ensuring critical content is present:** Many dynamic pages load content asynchronously. `wait_for` tells Crawl4ai to pause and wait until a specific condition is met before proceeding with content extraction or further `js_code` execution.
            *   **CSS selectors vs. JS expressions:**
                *   `wait_for="css:.my-element"`: Waits until an element matching the CSS selector `.my-element` appears in the DOM.
                *   `wait_for="js:() => window.myAppDataLoaded === true"`: Waits until the provided JavaScript expression evaluates to `true`. This is powerful for waiting on custom application states.
            *   **Impact on reliability:** Using `wait_for` dramatically increases the reliability of crawls on dynamic sites by preventing premature content extraction before necessary elements are loaded.
            *   **Code Example: Waiting for a specific `div` with ID `#results-container` to appear.**
                ```python
                config_wait_for_div = CrawlerRunConfig(
                    js_code="document.querySelector('#search-button').click();", # Perform a search
                    wait_for="css:#results-container" # Wait for results to load
                )
                # result = await crawler.arun(url="search-page.com", config=config_wait_for_div)
                ```
        *   **`scan_full_page` and `scroll_delay`:**
            *   **How this combination helps:**
                *   `scan_full_page=True`: Instructs Crawl4ai to attempt to scroll through the entire page, from top to bottom. This is designed to trigger lazy-loaded images or content that only appears as the user scrolls.
                *   `scroll_delay` (float, seconds): Specifies the pause duration between each scroll step during `scan_full_page`. A small delay (e.g., 0.2 to 0.5 seconds) gives the browser time to load newly visible content.
            *   **Tuning `scroll_delay`:** If images or content are still missing, try increasing `scroll_delay`. If the page loads quickly, a smaller delay might suffice.

    *   3.3.2. **Controlling Time (`page_timeout`, `wait_for_timeout`, `delay_before_return_html`, `mean_delay`, `max_range`)**
        *   **`page_timeout` and `wait_for_timeout`:**
            *   `page_timeout` (milliseconds, default from `config.PAGE_TIMEOUT` e.g., 60000): The maximum time allowed for the initial page navigation (the `page.goto()` call) to complete.
            *   `wait_for_timeout` (milliseconds): If `wait_for` is specified, this is the maximum time to wait for that condition to be met. If not set, it often defaults to `page_timeout`.
            *   **Purpose:** These prevent your crawler from hanging indefinitely on slow-loading pages or if a `wait_for` condition is never satisfied.
        *   **`delay_before_return_html` (float, seconds, default 0.1):**
            *   Sometimes, even after a page signals "load" or a `wait_for` condition is met, there might be final JavaScript rendering updates. This parameter introduces a small, fixed delay just before the HTML content is grabbed, potentially capturing these last-moment changes.
        *   **`mean_delay` & `max_range` (for `arun_many`):**
            *   These parameters are primarily used by dispatchers like `MemoryAdaptiveDispatcher` when you call `crawler.arun_many()`.
            *   `mean_delay` (seconds, default 0.1): The average base delay between consecutive requests to the *same domain*.
            *   `max_range` (seconds, default 0.3): A random amount of additional delay (between 0 and `max_range`) is added to `mean_delay`.
            *   **Purpose:** This introduces jitter and helps in polite crawling, making your requests less predictable and reducing the load on the target server.

    *   3.3.3. **Handling Embedded Content (`process_iframes`)**
        *   **When to set `process_iframes=True`:** If the content you need to extract is located inside an `<iframe>` on the page, setting this to `True` will instruct Crawl4ai to attempt to locate, access, and extract content from within iframes.
        *   **Limitations and complexities:**
            *   **Cross-Origin Restrictions:** Browsers enforce security policies that can prevent access to the content of iframes from a different domain unless specific CORS headers are set.
            *   **Nested Iframes:** Deeply nested iframes can be challenging to navigate.
            *   **Performance:** Processing iframes adds overhead and can slow down crawls.
            *   Currently, Crawl4ai's default iframe processing is basic and might merge content. For highly specific iframe interactions, you might need custom `js_code` targeting the iframe's content document.

*   3.4. **Media and Link Management Strategies**
    *   3.4.1. **Capturing Visuals and Documents (`screenshot`, `pdf`, `capture_mhtml`)**
        *   **Use cases:**
            *   `screenshot=True`: Captures a PNG image of the viewport (or full page if configured). Useful for visual verification, archiving page appearance, or when image-based analysis is needed. Result in `result.screenshot` (base64 string).
            *   `pdf=True`: Generates a PDF representation of the page. Good for archiving articles or creating printable versions. Result in `result.pdf` (bytes).
            *   `capture_mhtml=True`: Saves the page as an MHTML (.mht) archive. This format bundles all page resources (HTML, CSS, images, JS) into a single file, allowing for offline viewing with near-perfect fidelity. Result in `result.mhtml` (string).
        *   **How `scan_full_page` and `wait_for_images` can improve capture quality:**
            *   `scan_full_page=True`: Ensures lazy-loaded content is visible before capture.
            *   `wait_for_images=True`: Attempts to wait for images to fully load before taking a screenshot or PDF, leading to more complete visuals.

    *   3.4.2. **Curating Media (`image_score_threshold`, `exclude_external_images`, `exclude_all_images`)**
        *   **`image_score_threshold` (int, default from `config.IMAGE_SCORE_THRESHOLD` e.g., 3):**
            *   Crawl4ai internally scores images based on heuristics (size, alt text, proximity to content). This threshold filters out images with scores below the specified value. Higher values mean more stringent filtering (fewer, more "important" images).
        *   **`exclude_external_images=True`:** If set, images hosted on domains different from the crawled page's domain will be excluded from `result.media["images"]`. Useful for focusing on first-party content.
        *   **`exclude_all_images=True`:** If you don't need any image data at all, setting this to `True` will skip all image processing and `result.media["images"]` will be empty. This can improve performance.

    *   3.4.3. **Managing Links (`exclude_external_links`, `exclude_social_media_links`, `exclude_domains`, `exclude_internal_links`)**
        *   **Strategies for cleaning up the `links` output:**
            *   `exclude_external_links=True`: Only internal links (links to the same base domain) will be included in `result.links["internal"]`. `result.links["external"]` will be empty.
            *   `exclude_social_media_links=True`: Removes links pointing to common social media domains (Facebook, Twitter, LinkedIn, etc., defined in `config.SOCIAL_MEDIA_DOMAINS`) from both internal and external link lists.
            *   `exclude_domains=['ads.example.com', 'tracker.net']`: Provide a list of specific domains. Any link pointing to these domains will be excluded.
            *   `exclude_internal_links=True`: Only external links will be included in `result.links["external"]`. `result.links["internal"]` will be empty. Useful if you're only interested in outgoing links.

*   3.5. **Caching and Session Persistence (`cache_mode`, `session_id`)**
    *   3.5.1. **`cache_mode`: Optimizing for Speed and Freshness**
        *   This enum (`from crawl4ai import CacheMode`) controls how Crawl4ai interacts with its local cache for a given `arun()` call.
        *   `CacheMode.ENABLED` (Default if not set explicitly, but `CrawlerRunConfig` defaults to `BYPASS` if no `cache_mode` is passed in `__init__`):
            *   Reads from cache if a fresh entry exists for the URL.
            *   If not, fetches from the network and writes the result to the cache.
            *   **Use When:** Good for development to iterate quickly on parsing/extraction logic without re-fetching, or for crawling relatively static content.
        *   `CacheMode.BYPASS`:
            *   Ignores the cache completely. Always fetches the URL from the network.
            *   Does *not* write the result to the cache.
            *   **Use When:** You always need the absolute latest version of a page, or when debugging fetching/rendering issues.
        *   `CacheMode.READ_ONLY`:
            *   Only reads from the cache if an entry exists.
            *   Does *not* fetch from the network if the URL is not in the cache.
            *   Does *not* write to the cache.
            *   **Use When:** You want to run your processing logic strictly against a pre-existing cached dataset without making any network requests.
        *   `CacheMode.WRITE_ONLY`:
            *   Always fetches the URL from the network.
            *   Always writes (or overwrites) the result to the cache.
            *   Does *not* read from the cache before fetching.
            *   **Use When:** You want to populate or refresh your cache with the latest content.
        *   `CacheMode.DISABLED`:
            *   Completely disables any interaction with the cache system for this run. No reads, no writes.
            *   This is stronger than `BYPASS` as `BYPASS` might still involve some cache system overhead (e.g., checking if it should bypass).
            *   **Use When:** You want to ensure the cache system is not touched at all, perhaps for performance testing of raw fetching.
        *   **Decision Guide: Choosing the right cache mode.**
            *   **Development/Iteration:** `ENABLED` (to speed up repeated runs while changing extraction logic).
            *   **Production (Dynamic Content):** `BYPASS` or `ENABLED` with appropriate cache expiry (not directly settable via `CacheMode` but by cache implementation).
            *   **Production (Static/Archival Content):** `ENABLED` or `WRITE_ONLY` (for initial population) followed by `READ_ONLY` or `ENABLED`.
            *   **Testing against fixed data:** `READ_ONLY`.
            *   **Cache warming:** `WRITE_ONLY`.

    *   3.5.2. **`session_id`: Orchestrating Multi-Step Crawls**
        *   **How `session_id` allows sequential `arun()` calls to reuse the same browser page and context:**
            When you provide the same `session_id` string to multiple `arun()` calls within the *same* `AsyncWebCrawler` instance, Crawl4ai will reuse the existing browser page and its context (cookies, local storage, current URL, DOM state) for those calls, instead of opening a new page/tab for each.
        *   **Workflow: Simulating a login and subsequent data fetch.**
            1.  **First `arun()` (Establish Session & Login):**
                ```python
                # login_config = CrawlerRunConfig(
                #     url="https://example.com/login",
                #     session_id="my_secure_session",
                #     js_code=[
                #         "document.querySelector('#username').value = 'user';",
                #         "document.querySelector('#password').value = 'pass';",
                #         "document.querySelector('button[type=submit]').click();"
                #     ],
                #     wait_for="css:.user-dashboard" # Wait for a post-login element
                # )
                # login_result = await crawler.arun(config=login_config)
                ```
            2.  **Second `arun()` (Access Protected Page - using same `session_id`):**
                ```python
                # dashboard_config = CrawlerRunConfig(
                #     url="https://example.com/dashboard", # Navigate to a new page in the same session
                #     session_id="my_secure_session", # Crucial: same session_id
                #     # No js_code needed if already logged in, or add JS for dashboard interactions
                # )
                # dashboard_result = await crawler.arun(config=dashboard_config)
                ```
            3.  **Third `arun()` (Perform further actions - using same `session_id` and `js_only=True`):**
                If you just want to execute more JavaScript on the *current page* of the session without navigating:
                ```python
                # click_button_config = CrawlerRunConfig(
                #     session_id="my_secure_session",
                #     js_code="document.querySelector('#load-user-data-button').click();",
                #     wait_for="css:.user-data-loaded",
                #     js_only=True # Tells Crawl4ai not to navigate, just run JS on the current page
                # )
                # data_result = await crawler.arun(config=click_button_config)
                ```
        *   **Important: `js_only=True`**
            *   When `js_only=True` is set in `CrawlerRunConfig`, Crawl4ai will *not* perform a `page.goto(url)` operation. Instead, it will execute the provided `js_code` (if any) on the *current page* associated with the `session_id`.
            *   The `url` parameter in `CrawlerRunConfig` is effectively ignored when `js_only=True`.
            *   This is very useful for multi-step interactions on the same page (e.g., clicking multiple "load more" buttons, filling out different parts of a form sequentially).
        * **Cleaning Up:** Remember to kill the session when done to free up browser resources:
            ```python
            # await crawler.kill_session("my_secure_session")
            ```

*   3.6. **Best Practices for `CrawlerRunConfig`**
    *   3.6.1. **Test selectors and JS snippets in your browser's developer console first:** This saves a lot of time and helps ensure your selectors are correct and your JS code behaves as expected before integrating it into Crawl4ai.
    *   3.6.2. **Start with broader selectors and refine if necessary:** It's often easier to start with a more general `css_selector` or `target_elements` and then narrow it down if you're getting too much noise, rather than starting too specific and missing content.
    *   3.6.3. **Use `cache_mode=CacheMode.BYPASS` when testing changes** to selectors, JS code, or extraction strategies to ensure you're always working with fresh page content.
    *   3.6.4. **Combine `js_code` with appropriate `wait_for` conditions for reliability:** Don't assume JS actions complete instantly. Always wait for a clear indicator (an element appearing, a JS variable changing) that the action has had its desired effect.

*   3.7. **Troubleshooting Common `CrawlerRunConfig` Issues**
    *   3.7.1. **Content not being extracted as expected:**
        *   **Selector Issues:** Double-check your `css_selector` or selectors within your `extraction_strategy`. Test them in the browser devtools.
        *   **Dynamic Content Not Loaded:** The content might be loaded by JavaScript after the initial page load. Use `wait_for`, `js_code` to trigger loading, or `scan_full_page`. Try with `headless=False` in `BrowserConfig` to see what the browser is actually rendering.
    *   3.7.2. **Timeouts:**
        *   **Page taking too long:** Increase `page_timeout`.
        *   **`wait_for` condition never met:** Your selector might be wrong, the JS condition might never become true, or the element simply doesn't appear within the `wait_for_timeout`. Debug with `headless=False`.
    *   3.7.3. **JavaScript errors:**
        *   Set `log_console=True` in `BrowserConfig` (or the `arun` call directly if supported) to see browser console messages, which can reveal JS errors.
        *   Test your `js_code` snippets in the browser console.
    *   3.7.4. **`extraction_strategy` not yielding desired output:**
        *   **For `JsonCssExtractionStrategy`:** Verify your schema selectors.
        *   **For `LLMExtractionStrategy`:** Refine your Pydantic schema, improve your `instruction`, adjust `LLMConfig` parameters (like `temperature`), or provide better/more context if using `chunking_strategy`. Ensure the `input_format` for the strategy ("markdown" or "html") matches the type of content that will yield the best results from the LLM.

## 4. Configuring LLM Interactions with `LLMConfig`

*   4.1. **Purpose: Centralized LLM Settings**
    *   4.1.1. **Why `LLMConfig` is essential when using `LLMExtractionStrategy`, `LLMContentFilter`, or other LLM-powered components.**
        When your crawling workflow involves interacting with Large Language Models (e.g., for extracting structured data from unstructured text using `LLMExtractionStrategy`, or for filtering relevant content using `LLMContentFilter`), `LLMConfig` provides a dedicated and centralized place to manage all settings related to these interactions. This includes specifying which LLM provider and model to use, API keys, and parameters that control the LLM's generation behavior (like temperature, max tokens, etc.).

    *   4.1.2. **How it promotes consistency in LLM calls.**
        By encapsulating LLM settings in a separate object, you ensure that:
        *   All LLM-powered components in your Crawl4ai setup can share the same configuration if desired, leading to consistent behavior.
        *   You can easily switch LLM providers or models by changing the `LLMConfig` in one place, without needing to modify every strategy that uses an LLM.
        *   LLM-specific details are kept separate from the core browser and crawl run configurations, improving code organization.

*   4.2. **Core `LLMConfig` Parameters and Their Impact**
    *   4.2.1. **Provider Setup (`provider`, `api_token`, `base_url`)**
        *   **Choosing the right `provider` (e.g., "openai/gpt-4o-mini", "ollama/llama3", "groq/llama3-70b-8192"):**
            *   Crawl4ai leverages the [LiteLLM](https://litellm.ai/) library, which supports a vast range of LLM providers (OpenAI, Azure OpenAI, Anthropic, Cohere, Google Gemini, Ollama, Groq, and many more). The `provider` string typically follows the format `"provider_name/model_name"`.
            *   **Considerations for choosing a provider/model:**
                *   **Cost:** Different models and providers have varying pricing structures.
                *   **Model Capabilities:** Some models excel at specific tasks (e.g., instruction following, summarization, code generation).
                *   **Context Window Size:** The maximum amount of text the model can process at once.
                *   **Speed/Latency:** How quickly the model responds.
                *   **Availability & Rate Limits:** Ensure the provider can handle your expected load.
                *   **Open vs. Closed Source:** Ollama allows running open-source models locally, while others are API-based.
        *   **`api_token`: How to securely provide API keys (direct string vs. `env:YOUR_ENV_VAR`).**
            *   **Direct String:** You can pass the API key directly: `api_token="sk-..."`. **Not recommended for production code.**
            *   **Environment Variable (Recommended):** Use the `env:` prefix to tell Crawl4ai to read the key from an environment variable: `api_token="env:OPENAI_API_KEY"`. This is much more secure as it keeps secrets out of your codebase. Crawl4ai automatically looks for common environment variables like `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`, `GEMINI_API_KEY`, etc., based on the `provider` if `api_token` is not explicitly set.
        *   **`base_url`: When to use this for self-hosted models (like local Ollama) or custom API gateways.**
            *   If you are running an LLM locally (e.g., using Ollama, which defaults to `http://localhost:11434`), or if you are routing API calls through a custom gateway or proxy, you'll need to set the `base_url` to point to the correct endpoint.
            *   For many cloud providers, LiteLLM knows the default `base_url`, so you often don't need to set it.
        *   **Code Example: Configuring for OpenAI vs. a local Ollama instance.**
            ```python
            from crawl4ai import LLMConfig
            import os

            # OpenAI Configuration (assumes OPENAI_API_KEY is set in environment)
            openai_config = LLMConfig(
                provider="openai/gpt-4o-mini",
                # api_token=os.getenv("OPENAI_API_KEY") # Or let Crawl4ai find it
            )
            print(f"OpenAI Provider: {openai_config.provider}")

            # Local Ollama Configuration (Llama3 running via Ollama)
            ollama_config = LLMConfig(
                provider="ollama/llama3", 
                base_url="http://localhost:11434", # Default Ollama endpoint
                api_token="ollama" # Standard token for Ollama if no specific auth
            )
            print(f"Ollama Provider: {ollama_config.provider}, Base URL: {ollama_config.base_url}")
            
            # Groq Configuration (Llama3-70b via Groq, fast inference)
            groq_config = LLMConfig(
                provider="groq/llama3-70b-8192",
                api_token=os.getenv("GROQ_API_KEY") # Needs GROQ_API_KEY env var
            )
            print(f"Groq Provider: {groq_config.provider}")
            ```

    *   4.2.2. **Fine-tuning LLM Generation (`temperature`, `max_tokens`, `top_p`, etc.)**
        These parameters control the behavior of the LLM when it generates text.
        *   **`temperature` (float, typically 0.0 to 2.0):**
            *   Controls the randomness of the output.
            *   Lower values (e.g., 0.0 - 0.3): More deterministic, focused, and factual. Good for precise data extraction or when you want predictable output based on a strict schema.
            *   Higher values (e.g., 0.7 - 1.0+): More creative, diverse, and potentially surprising. Better for tasks like summarization, brainstorming, or generating varied text.
        *   **`max_tokens` (int):**
            *   The maximum number of tokens (words/sub-words) the LLM should generate in its response.
            *   Crucial for managing costs (as most APIs charge per token) and ensuring the output doesn't become excessively long.
            *   Set it based on the expected length of your desired output (e.g., for a short summary vs. a detailed extraction).
        *   **`top_p` (float, typically 0.0 to 1.0):**
            *   An alternative to `temperature` for controlling randomness, known as nucleus sampling. The model considers only the tokens whose cumulative probability mass exceeds `top_p`.
            *   A common value is 0.9. Lower values make the output more focused.
            *   Usually, you'd use either `temperature` or `top_p`, not both simultaneously (or set one to its neutral default, e.g., `top_p=1.0` if using `temperature`).
        *   **Other parameters (`frequency_penalty`, `presence_penalty`, `stop`, `n`):**
            *   `frequency_penalty` (float): Penalizes tokens that have already appeared frequently, encouraging the model to use different words.
            *   `presence_penalty` (float): Penalizes tokens that have appeared at all, encouraging novelty.
            *   `stop` (string or list of strings): Sequences where the API will stop generating further tokens.
            *   `n` (int): How many completions to generate for each prompt.
            *   **When to use:** These are more advanced and used for specific fine-tuning, e.g., reducing repetition or generating multiple candidate outputs. Consult your LLM provider's documentation for details on how they interpret these.
        *   **Use Case: Adjusting parameters for extracting a strict JSON schema vs. generating a summary.**
            *   **Strict JSON Schema Extraction:** `temperature=0.1`, `top_p=1.0` (or not set), `max_tokens` appropriate for the schema size.
            *   **Creative Summary Generation:** `temperature=0.7`, `top_p=0.9`, `max_tokens` set to desired summary length.

*   4.3. **Workflow: Integrating `LLMConfig` in Your Crawl**
    *   4.3.1. **Step 1: Instantiate `LLMConfig` with your desired settings.**
        ```python
        from crawl4ai import LLMConfig
        import os
        
        my_llm_config = LLMConfig(
            provider="openai/gpt-4o-mini",
            api_token=os.getenv("OPENAI_API_KEY"),
            temperature=0.2,
            max_tokens=1024
        )
        ```
    *   4.3.2. **Step 2: Pass the `LLMConfig` instance to an LLM-dependent strategy.**
        For example, if using `LLMExtractionStrategy`:
        ```python
        from crawl4ai.extraction_strategy import LLMExtractionStrategy
        from pydantic import BaseModel

        class MyData(BaseModel):
            name: str
            value: int

        llm_extraction_strategy = LLMExtractionStrategy(
            llm_config=my_llm_config,
            schema=MyData.model_json_schema(),
            instruction="Extract name and value."
        )
        ```
    *   4.3.3. **Step 3: Include that strategy in your `CrawlerRunConfig`.**
        ```python
        from crawl4ai import CrawlerRunConfig

        my_run_config = CrawlerRunConfig(
            extraction_strategy=llm_extraction_strategy
            # ... other run config settings
        )
        ```
    *   **Code Example: A complete flow showing `LLMConfig` -> `LLMExtractionStrategy` -> `CrawlerRunConfig` -> `arun()`.**
        ```python
        from crawl4ai import AsyncWebCrawler, LLMConfig, LLMExtractionStrategy, CrawlerRunConfig
        from pydantic import BaseModel, Field
        import json
        import os

        class Product(BaseModel):
            product_name: str = Field(description="The name of the product")
            price: float = Field(description="The price of the product")

        sample_product_page_html = """
        <html><body>
            <div class='product-details'>
                <h2>Awesome Gadget X1000</h2>
                <p class='price-tag'>Price: $99.99</p>
                <p>This gadget does amazing things...</p>
            </div>
        </body></html>
        """

        async def run_full_llm_flow():
            # 1. LLMConfig
            llm_conf = LLMConfig(
                provider="openai/gpt-4o-mini", 
                api_token=os.getenv("OPENAI_API_KEY"), # Ensure this is set
                temperature=0.1
            )

            # 2. LLMExtractionStrategy
            product_extraction_strategy = LLMExtractionStrategy(
                llm_config=llm_conf,
                schema=Product.model_json_schema(),
                instruction="From the provided HTML, extract the product name and its price."
            )

            # 3. CrawlerRunConfig
            product_run_config = CrawlerRunConfig(
                extraction_strategy=product_extraction_strategy,
                # LLMExtractionStrategy expects HTML input by default if input_format is not changed
                input_format="html" # Explicitly telling the strategy to use HTML
            )

            # 4. AsyncWebCrawler and arun()
            async with AsyncWebCrawler() as crawler:
                result = await crawler.arun(
                    url=f"raw://{sample_product_page_html}", 
                    config=product_run_config
                )

                if result.success and result.extracted_content:
                    try:
                        extracted_data_list = json.loads(result.extracted_content)
                        if extracted_data_list: # LLMExtractionStrategy often returns a list
                            product_info = Product(**extracted_data_list[0])
                            print(f"Product: {product_info.product_name}, Price: ${product_info.price}")
                        else:
                            print("LLM returned no data.")
                    except json.JSONDecodeError:
                        print(f"Failed to parse LLM JSON output: {result.extracted_content}")
                    except Exception as e:
                        print(f"Error processing extracted data: {e}")
                else:
                    print(f"Crawl or extraction failed: {result.error_message}")
        
        # if os.getenv("OPENAI_API_KEY"):
        #     await run_full_llm_flow()
        # else:
        #     print("OPENAI_API_KEY not set. Skipping LLMConfig example.")
        ```

*   4.4. **Best Practices for `LLMConfig`**
    *   4.4.1. **Use environment variables for API keys:** Never hardcode API keys in your scripts. Use `api_token="env:YOUR_KEY_NAME"`.
    *   4.4.2. **Start with conservative `max_tokens`:** This helps manage costs, especially during testing. Increase it only if necessary for the desired output length.
    *   4.4.3. **Test prompts and parameters iteratively:** LLM behavior can be sensitive to prompting and parameters. Start with simple prompts and gradually refine them. Test with low `temperature` for predictability first.
    *   4.4.4. **Be aware of rate limits:** Different LLM providers have different rate limits. If you're making many calls, implement appropriate delays or use a queueing system to avoid hitting these limits. Crawl4ai's built-in backoff in `perform_completion_with_backoff` helps, but sustained high volume might still be an issue.

*   4.5. **Troubleshooting `LLMConfig` and LLM Interactions**
    *   4.5.1. **Authentication errors (invalid API key, incorrect provider string):**
        *   Double-check your `api_token` and ensure the environment variable is correctly set and accessible.
        *   Verify the `provider` string matches one supported by LiteLLM and that you have the necessary access/credits for that provider.
        *   If using `base_url`, ensure it's correct and the local LLM server (like Ollama) is running.
    *   4.5.2. **LLM not following instructions or schema (if `extraction_type="schema"`):**
        *   **Prompt Engineering:** This is key. Your `instruction` needs to be very clear, specific, and unambiguous. Provide examples within the prompt if necessary.
        *   **Parameter Tuning:** Adjust `temperature`. For schema extraction, very low (e.g., 0.0 or 0.1) is usually best.
        *   **Model Choice:** Some models are better at instruction-following or JSON generation than others. Experiment if one model isn't working.
        *   **Schema Complexity:** If your Pydantic schema is very complex, the LLM might struggle. Try simplifying it or breaking down the extraction into multiple steps/prompts.
        *   **Input Content:** Ensure the `input_format` for your `LLMExtractionStrategy` ("markdown" or "html") provides the LLM with the most useful version of the content. Sometimes, clean Markdown is better; other times, the raw HTML structure helps.
    *   4.5.3. **Rate limit errors from the LLM provider:**
        *   The `perform_completion_with_backoff` utility in Crawl4ai attempts to handle transient rate limits with exponential backoff.
        *   If you consistently hit rate limits, you may need to reduce the concurrency of your LLM calls (e.g., process fewer chunks in parallel) or request a higher rate limit from your provider.
    *   4.5.4. **Unexpectedly high costs (monitor token usage):**
        *   Keep `max_tokens` as low as feasible for your task.
        *   Be mindful of input token count, especially if using `LLMExtractionStrategy` on large chunks of text. Optimize `chunk_size` in your `chunking_strategy`.
        *   Monitor your LLM provider's billing dashboard regularly.

## 5. Specialized Configuration Objects: `GeolocationConfig`, `ProxyConfig`, `HTTPCrawlerConfig`

These objects provide targeted configuration for specific advanced crawling needs.

*   5.1. **Simulating Location with `GeolocationConfig`**
    *   5.1.1. **Purpose: Why you might need to make the browser appear from a specific geographic location.**
        Websites can serve different content, prices, or even different site versions based on the user's perceived geographic location (often determined by IP address, but also potentially by browser geolocation APIs). `GeolocationConfig` allows you to override the browser's reported GPS coordinates.
    *   5.1.2. **Use Cases:**
        *   **Accessing Geo-Restricted Websites or Content:** Some sites block access or show limited content to users outside specific regions.
        *   **Testing Localization and Internationalization:** Verify that your website correctly displays language, currency, and content for different locales.
        *   **Scraping Geo-Specific Data:** Collect data that varies by location, like local search results, store availability, or regional pricing.
    *   5.1.3. **How to use:**
        1.  Instantiate `GeolocationConfig` with the desired `latitude`, `longitude`, and optionally `accuracy` (in meters).
            ```python
            from crawl4ai.async_configs import GeolocationConfig
            paris_location = GeolocationConfig(latitude=48.8566, longitude=2.3522, accuracy=50.0)
            ```
        2.  Pass this object to the `geolocation` parameter of `CrawlerRunConfig`.
            ```python
            from crawl4ai import CrawlerRunConfig
            run_config_paris = CrawlerRunConfig(geolocation=paris_location)
            ```
        *   **Important Note:** For `GeolocationConfig` to be truly effective in making a website *believe* you are in that location, you usually also need to route your traffic through a **proxy server located in that same geographic region**. Setting GPS coordinates alone might not be enough if your IP address still points to your actual location.
    *   5.1.4. **Interaction with browser permissions (Playwright handles this implicitly when geolocation is set).**
        When you set geolocation via Playwright (which Crawl4ai uses under the hood), it typically also grants the necessary browser permission for the page to access this spoofed location information, mimicking a user clicking "Allow" on a location access prompt.
    *   **Code Example: Crawling a site as if from Paris, France (assuming a Paris proxy is also configured in `BrowserConfig`).**
        ```python
        from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, GeolocationConfig

        async def crawl_from_paris():
            # Assume proxy_for_paris is configured in BrowserConfig
            # For this example, we'll just show GeolocationConfig
            paris_browser_config = BrowserConfig(
                # proxy_config={"server": "http://paris-proxy.example.com:8080"} # Illustrative
            )
            
            paris_location = GeolocationConfig(latitude=48.8566, longitude=2.3522, accuracy=100.0)
            
            # Also good to set locale and timezone to match
            paris_run_config = CrawlerRunConfig(
                geolocation=paris_location,
                locale="fr-FR",
                timezone_id="Europe/Paris"
            )

            async with AsyncWebCrawler(config=paris_browser_config) as crawler:
                # A site that shows location-based info
                result = await crawler.arun(url="https://www.iplocation.net/", config=paris_run_config)
                if result.success:
                    print("--- Page content (should reflect Paris if proxy and geo are working) ---")
                    print(result.markdown.raw_markdown[:500]) 
                else:
                    print(f"Crawl failed: {result.error_message}")
        
        # await crawl_from_paris()
        ```

*   5.2. **Detailed Proxy Setup with `ProxyConfig`**
    *   5.2.1. **When to use `ProxyConfig` object vs. the simpler `proxy` string in `BrowserConfig`.**
        *   The `proxy` parameter directly in `BrowserConfig` (e.g., `BrowserConfig(proxy="http://user:pass@host:port")`) is a simpler way to set a proxy for Playwright, but it's a Playwright-level string.
        *   The `proxy_config` parameter in `BrowserConfig` expects a dictionary like `{"server": "...", "username": "...", ...}` which Playwright also accepts.
        *   The `crawl4ai.async_configs.ProxyConfig` object is a Pydantic model that helps structure these details, especially useful if you are:
            *   Programmatically constructing proxy configurations.
            *   Building a custom `ProxyRotationStrategy` that needs to manage a list of `ProxyConfig` objects.
            *   Needing to store or pass around proxy details in a typed way.
            *   It also includes an `ip` field, which can be useful for internal tracking or verification, though it's not directly used by Playwright's connection mechanism.
        When passing to `BrowserConfig(proxy_config=...)`, you'd typically use `my_proxy_config_object.to_dict()`.
    *   5.2.2. **Key parameters of `ProxyConfig` object: `server`, `username`, `password`, `ip`.**
        *   `server` (str): The proxy server URL (e.g., `"http://127.0.0.1:8080"`, `"socks5://myproxy.com:1080"`).
        *   `username` (Optional[str]): Username for proxy authentication.
        *   `password` (Optional[str]): Password for proxy authentication.
        *   `ip` (Optional[str]): The IP address of the proxy. This is more for your internal tracking or if your proxy provider gives you an outbound IP to verify against; Playwright itself primarily uses the `server` field for connection.
    *   5.2.3. **How `ProxyConfig` instances are typically managed by a `ProxyRotationStrategy`.**
        If you're using a `ProxyRotationStrategy` (detailed in its own documentation section), that strategy would typically hold a list of `ProxyConfig` objects. Its `get_next_proxy()` method would return one of these `ProxyConfig` objects, which would then be used to configure the `proxy_config` (via its dictionary representation) for a `BrowserConfig` or directly within a `CrawlerRunConfig` if the strategy involves per-run proxy changes.
    *   **Code Example: Creating `ProxyConfig` objects.**
        ```python
        from crawl4ai.async_configs import ProxyConfig, BrowserConfig

        # Create ProxyConfig objects
        proxy1 = ProxyConfig(
            server="http://proxy1.example.com:8000", 
            username="user1", 
            password="password1",
            ip="1.2.3.4" # For your reference
        )
        proxy2 = ProxyConfig(
            server="socks5://proxy2.example.com:1080",
            ip="5.6.7.8"
        )

        print(f"Proxy 1 Server: {proxy1.server}")
        
        # To use with BrowserConfig:
        # browser_cfg = BrowserConfig(proxy_config=proxy1.to_dict())
        # Or if you have a list and a rotation strategy:
        # rotation_strategy = RoundRobinProxyStrategy(proxies=[proxy1, proxy2])
        # next_proxy_obj = await rotation_strategy.get_next_proxy()
        # if next_proxy_obj:
        #     browser_cfg = BrowserConfig(proxy_config=next_proxy_obj.to_dict())
        ```

*   5.3. **Lightweight Crawling with `HTTPCrawlerConfig`**
    *   5.3.1. **Understanding the `AsyncHTTPCrawlerStrategy`:**
        *   **When it's a better choice:** The default `AsyncPlaywrightCrawlerStrategy` uses a full browser (Playwright), which is powerful but resource-intensive. For tasks that don't require JavaScript execution, complex DOM interactions, or browser rendering, the `AsyncHTTPCrawlerStrategy` is a much lighter and faster alternative. It makes direct HTTP requests using the `requests` library (via `httpx` for async).
        *   Ideal for:
            *   Scraping static HTML sites.
            *   Accessing APIs that return JSON, XML, or other text-based data.
            *   Downloading files directly.
        *   **Trade-offs:**
            *   Cannot execute JavaScript. Content rendered by client-side JS will be missed.
            *   No DOM interaction capabilities (like clicking buttons).
            *   Doesn't handle complex browser features like cookies or sessions automatically in the same way Playwright does (though you can manage headers manually).
    *   5.3.2. **Purpose of `HTTPCrawlerConfig`: Tailoring direct HTTP requests.**
        When you use `AsyncHTTPCrawlerStrategy`, the `HTTPCrawlerConfig` object allows you to specify details for the HTTP request itself, such as the method, headers, and body data.
    *   5.3.3. **Key Parameters of `HTTPCrawlerConfig`:**
        *   `method` (str, default "GET"): The HTTP method (e.g., "GET", "POST", "PUT", "DELETE").
        *   `headers` (Optional[Dict[str, str]]): Custom HTTP headers to send with the request.
        *   `data` (Optional[Dict[str, Any]]): Dictionary of data to be form-urlencoded and sent in the request body (typically for "POST" requests with `Content-Type: application/x-www-form-urlencoded`).
        *   `json` (Optional[Dict[str, Any]]): Dictionary of data to be JSON-encoded and sent in the request body (typically for "POST" or "PUT" requests with `Content-Type: application/json`).
        *   `follow_redirects` (bool, default True): Whether `httpx` should automatically follow HTTP redirects (3xx status codes).
        *   `verify_ssl` (bool, default True): Whether to verify SSL certificates. Set to `False` with caution, similar to `ignore_https_errors` in `BrowserConfig`.
    *   5.3.4. **Workflow:**
        1.  Instantiate `AsyncHTTPCrawlerStrategy`.
            ```python
            from crawl4ai.async_crawler_strategy import AsyncHTTPCrawlerStrategy
            http_strategy = AsyncHTTPCrawlerStrategy()
            ```
        2.  Create an `AsyncWebCrawler` instance, passing this strategy.
            ```python
            # crawler = AsyncWebCrawler(crawler_strategy=http_strategy)
            ```
        3.  When calling `crawler.arun()`, if you need to customize the HTTP request (e.g., for a POST), create an `HTTPCrawlerConfig` and pass it via `CrawlerRunConfig`.
            ```python
            from crawl4ai.async_configs import HTTPCrawlerConfig, CrawlerRunConfig
            
            # http_post_config = HTTPCrawlerConfig(
            #     method="POST",
            #     json={"key": "value"},
            #     headers={"X-Custom-Header": "MyValue"}
            # )
            # run_config_http = CrawlerRunConfig(
            #     # Note: When using AsyncHTTPCrawlerStrategy, its specific config
            #     # is often passed directly to arun or its strategy methods,
            #     # rather than through CrawlerRunConfig's generic 'experimental' field.
            #     # However, let's assume for consistency or future enhancement
            #     # it could be passed like this:
            #     experimental={"http_crawler_config": http_post_config.to_dict()}
            # )
            # For current direct use with arun():
            # result = await crawler.arun(
            #     url="https://api.example.com/submit",
            #     method="POST", # Pass directly to arun when using AsyncHTTPCrawlerStrategy
            #     json_data={"key": "value"}, # Pass directly
            #     headers={"X-Custom-Header": "MyValue"} # Pass directly
            # )
            ```
            **Correction/Clarification:** `AsyncHTTPCrawlerStrategy.crawl()` directly accepts `method`, `headers`, `data`, `json_data`, etc. as keyword arguments. `HTTPCrawlerConfig` is more of a Pydantic model to structure these, but they are passed directly to `arun` when the active strategy is `AsyncHTTPCrawlerStrategy`. `CrawlerRunConfig` is less relevant for these HTTP-specific parameters when *not* using a browser-based strategy.

    *   **Code Example: Fetching data from a JSON API using `AsyncHTTPCrawlerStrategy`.**
        ```python
        from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
        from crawl4ai.async_crawler_strategy import AsyncHTTPCrawlerStrategy
        import json

        async def fetch_json_api():
            # 1. Use AsyncHTTPCrawlerStrategy
            http_strategy = AsyncHTTPCrawlerStrategy()
            
            # 2. Create Crawler with this strategy
            async with AsyncWebCrawler(crawler_strategy=http_strategy) as crawler:
                # 3. Call arun, passing HTTP-specific params directly
                result = await crawler.arun(
                    url="https://jsonplaceholder.typicode.com/todos/1",
                    method="GET" # Default, but explicit here
                )

                if result.success:
                    print(f"Status Code: {result.status_code}")
                    try:
                        # The 'html' field will contain the raw response body
                        todo_data = json.loads(result.html) 
                        print("Fetched TODO Data:")
                        print(todo_data)
                    except json.JSONDecodeError:
                        print(f"Failed to parse JSON response: {result.html[:200]}")
                else:
                    print(f"API call failed: {result.error_message}")

        # await fetch_json_api()
        ```

## 6. Efficiently Managing Configurations: `clone()`, `dump()`, and `load()`

*   6.1. **The Rationale: Why Manage Configurations Programmatically?**
    Manually creating and managing numerous configuration objects with slight variations can quickly become tedious, error-prone, and lead to code duplication. Crawl4ai provides `clone()`, `dump()`, and `load()` methods on its configuration objects (`BrowserConfig`, `CrawlerRunConfig`, `LLMConfig`, etc.) to address these challenges. Programmatic management offers:
    *   **Reduced Repetition:** Define base configurations once and create variations easily.
    *   **Modularity and Reusability:** Store and load common configurations, promoting a "don't repeat yourself" (DRY) approach.
    *   **Persistence:** Save configurations to files (JSON, YAML) for later use, version control, or sharing across different scripts or team members.
    *   **Dynamic Configuration:** Load or modify configurations at runtime based on external inputs or application logic.
    *   **Improved Readability:** Complex setups can be broken down into smaller, named configurations, making the overall code easier to understand.

*   6.2. **`clone(**kwargs)`: Creating Variations with Ease**
    *   6.2.1. **How it works:** The `clone()` method, available on configuration objects like `BrowserConfig` and `CrawlerRunConfig`, performs a *deep copy* of the original configuration object. You can then pass keyword arguments to `clone()` to override specific attributes in the newly created copy. The original object remains unchanged.
    *   6.2.2. **Use Cases:**
        *   **Creating slightly different `CrawlerRunConfig` objects:**
            *   For different sections of a website (e.g., product pages vs. blog posts) that share most crawl settings but require different `extraction_strategy` or `css_selector`.
            *   For A/B testing different `wait_for` conditions or `js_code` snippets.
        *   **Generating multiple `BrowserConfig` instances:**
            *   For testing with different user agents, proxy settings, or headless modes while keeping other browser settings consistent.
    *   6.2.3. **Code Example:**
        ```python
        from crawl4ai import BrowserConfig, CrawlerRunConfig, CacheMode
        # from crawl4ai.extraction_strategy import SomeExtractionStrategy # Placeholder

        # --- BrowserConfig Cloning ---
        base_browser_config = BrowserConfig(
            headless=True,
            user_agent="MyDefaultAgent/1.0"
        )

        # Clone for debugging (headful)
        debug_browser_config = base_browser_config.clone(headless=False, verbosity=True)
        print(f"Base headless: {base_browser_config.headless}, Debug headless: {debug_browser_config.headless}")

        # Clone for a specific mobile UA
        mobile_browser_config = base_browser_config.clone(
            user_agent="Mozilla/5.0 (iPhone; CPU iPhone OS 13_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Mobile/15E148 Safari/604.1"
        )
        print(f"Mobile UA: {mobile_browser_config.user_agent}")

        # --- CrawlerRunConfig Cloning ---
        base_run_config = CrawlerRunConfig(
            cache_mode=CacheMode.ENABLED,
            word_count_threshold=50,
            screenshot=False
        )

        # Config for scraping articles, needs specific extraction and screenshot
        # Assuming ArticleExtractionStrategy is a defined class
        # article_strategy = SomeExtractionStrategy(type="article") 
        article_run_config = base_run_config.clone(
            # extraction_strategy=article_strategy, 
            screenshot=True,
            css_selector="main.article-body"
        )
        print(f"Article config screenshot: {article_run_config.screenshot}, CSS: {article_run_config.css_selector}")

        # Config for scraping product listings, different strategy, no screenshot
        # Assuming ProductListExtractionStrategy is a defined class
        # product_list_strategy = SomeExtractionStrategy(type="product_list")
        product_list_run_config = base_run_config.clone(
            # extraction_strategy=product_list_strategy,
            css_selector="ul.product-grid"
        )
        print(f"Product list screenshot: {product_list_run_config.screenshot}, CSS: {product_list_run_config.css_selector}")
        ```

*   6.3. **`dump()` and `load(data: dict)`: Persistence and Portability**
    *   6.3.1. **`dump()`:**
        *   **How it serializes:** The `dump()` method converts the configuration object's state into a Python dictionary. This dictionary is designed to be JSON-serializable, meaning it contains only basic Python types (strings, numbers, booleans, lists, dictionaries) and representations of nested configuration objects.
        *   **What can be serialized:**
            *   Basic attributes (strings, ints, bools).
            *   Nested Crawl4ai configuration objects (e.g., a `GeolocationConfig` within a `CrawlerRunConfig` will also be `dump`ed).
            *   Enum members are typically serialized to their string values.
        *   **Limitations:** `dump()` primarily serializes the *configurable parameters* of the object. It generally cannot serialize:
            *   Arbitrary Python objects assigned to attributes (e.g., custom, non-Crawl4ai class instances like a complex `extraction_strategy` instance that isn't just a basic Crawl4ai strategy). If you need to persist such complex objects, you'd typically handle their serialization and deserialization separately (e.g., using `pickle` with caution, or by re-instantiating them based on some stored identifier).
            *   Runtime state that isn't part of the initial configuration.
    *   6.3.2. **`load(data: dict)`:**
        *   **How it reconstructs:** This is a *static method* on the configuration class (e.g., `BrowserConfig.load(my_dict)`). It takes a dictionary (usually one produced by `dump()`) and creates a new instance of the configuration object, populating it with the values from the dictionary.
        *   **Ensuring dictionary structure:** The input dictionary should have keys that correspond to the parameters of the configuration object's `__init__` method or its settable attributes. Nested config objects in the dictionary will also be reconstructed using their respective `load()` methods.
    *   6.3.3. **Workflow: Saving and Loading Configurations**
        1.  **Create and Configure:** Instantiate and set up your config object.
            ```python
            # my_browser_config = BrowserConfig(user_agent="TestAgent/1.0", headless=False)
            ```
        2.  **Dump to Dictionary:**
            ```python
            # config_dict = my_browser_config.dump()
            ```
        3.  **Save to File (e.g., JSON):**
            ```python
            import json
            # with open("browser_settings.json", "w") as f:
            #     json.dump(config_dict, f, indent=4)
            ```
        4.  **Later, Load from File:**
            ```python
            # with open("browser_settings.json", "r") as f:
            #     loaded_dict_from_file = json.load(f)
            ```
        5.  **Reconstruct Object using `load()`:**
            ```python
            # loaded_browser_config = BrowserConfig.load(loaded_dict_from_file)
            # print(f"Loaded User-Agent: {loaded_browser_config.user_agent}")
            ```
    *   **Code Example: Saving a `BrowserConfig` to JSON and then loading it back.**
        ```python
        from crawl4ai import BrowserConfig
        import json
        import os

        # 1. Create and configure
        original_browser_config = BrowserConfig(
            user_agent="MyPersistentAgent/2.0", 
            headless=True,
            extra_args=["--incognito"],
            proxy_config={"server": "http://testproxy.com:1234"}
        )
        print(f"Original Config: {original_browser_config.user_agent}, Headless: {original_browser_config.headless}")

        # 2. Dump to dictionary
        config_as_dict = original_browser_config.dump()
        print(f"\nDumped Dictionary:\n{json.dumps(config_as_dict, indent=2)}")

        # 3. Save to JSON file
        file_path = "my_saved_browser_config.json"
        with open(file_path, "w") as f:
            json.dump(config_as_dict, f, indent=2)
        print(f"\nSaved config to {file_path}")

        # 4. Load from JSON file
        with open(file_path, "r") as f:
            loaded_dict = json.load(f)
        
        # 5. Reconstruct object using load()
        loaded_config = BrowserConfig.load(loaded_dict)
        print(f"\nLoaded Config from file: {loaded_config.user_agent}, Headless: {loaded_config.headless}")
        print(f"Loaded Proxy Server: {loaded_config.proxy_config.get('server') if loaded_config.proxy_config else 'None'}")

        # Clean up
        os.remove(file_path)
        ```

*   6.4. **Best Practices for Configuration Management**
    *   6.4.1. **Define base configurations:** For settings that are common across many crawls (e.g., a standard `BrowserConfig` for your organization, or a default `CrawlerRunConfig` for a type of website), define them once.
    *   6.4.2. **Use `clone()` for variations:** When you need slight modifications for specific tasks, use `base_config.clone(param_to_override=new_value)`. This keeps your code DRY and makes it clear what's changing.
    *   6.4.3. **Store complex/reused configurations externally:** For configurations that are elaborate or used across multiple scripts/projects, save them as JSON or YAML files and load them using `ConfigClass.load()`. This decouples configuration from code.
    *   6.4.4. **Consider versioning your configuration files:** If your external configuration files evolve, use a version control system (like Git) to track changes, just as you would with your code. This helps in managing different setups or rolling back if needed.

## 7. Advanced Scenarios: Combining Configuration Objects for Powerful Workflows

*   7.1. **Introduction: The Synergy of Configuration Objects**
    The true power of Crawl4ai's configuration system shines when you combine different configuration objects (`BrowserConfig`, `CrawlerRunConfig`, `LLMConfig`, `GeolocationConfig`, etc.) to tackle complex, real-world crawling challenges. Each object controls a specific aspect of the crawl, and their interplay allows for highly tailored and sophisticated behavior. This section explores several scenarios to illustrate this synergy.

*   7.2. **Scenario 1: Geo-Targeted Content Extraction with Specific Browser Identity and Proxies**
    *   **Objective:** Crawl a news website that serves different content based on the user's country, appearing as a mobile user from Germany, and routing traffic through a German proxy server.
    *   **`BrowserConfig` Elements:**
        *   `user_agent`: A User-Agent string for a common mobile browser in Germany (e.g., Chrome on Android).
            *   *Why:* To make the server believe the request is from a mobile device.
        *   `proxy_config`: Details of a proxy server located in Germany.
            *   *Why:* The IP address is a primary way websites determine location.
        *   `channel` (if Chromium-based): Could be set to "chrome" to ensure Chrome-specific behavior if the UA is Chrome.
    *   **`CrawlerRunConfig` Elements:**
        *   `geolocation`: An instance of `GeolocationConfig` with latitude/longitude for a city in Germany (e.g., Berlin).
            *   *Why:* To provide GPS coordinates that match the desired location, for sites using browser geolocation APIs.
        *   `locale`: Set to "de-DE".
            *   *Why:* To set the `Accept-Language` header and JavaScript `navigator.language` to German, further reinforcing the German user profile.
        *   `timezone_id`: Set to "Europe/Berlin".
            *   *Why:* To make the browser's reported timezone consistent with Germany.
        *   `extraction_strategy`: An appropriate strategy to extract news headlines and summaries.
    *   **Workflow Explanation:**
        1.  The `BrowserConfig` launches a browser that routes its traffic through the German proxy, making all network requests appear to originate from Germany. Its User-Agent string identifies it as a German mobile user.
        2.  The `CrawlerRunConfig` then instructs this browser context to report German GPS coordinates, set its language to German, and use a German timezone.
        3.  When `arun()` navigates to the news URL, the website should (if it performs geo-targeting) serve the German version of its content.
        4.  The specified `extraction_strategy` then processes this German-specific content.
    *   **Code Example: Setting up this combined configuration.**
        ```python
        from crawl4ai import (
            AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, 
            GeolocationConfig, CacheMode
        )
        # Assume an appropriate extraction strategy, e.g., for news articles
        # from crawl4ai.extraction_strategy import SomeArticleExtractionStrategy 

        async def crawl_german_news():
            german_proxy = {
                "server": "http://your-german-proxy.com:port", # Replace with actual proxy
                # "username": "proxy_user", # If authenticated
                # "password": "proxy_pass"  # If authenticated
            }

            browser_cfg_german = BrowserConfig(
                user_agent="Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Mobile Safari/537.36", # Example Android Chrome
                proxy_config=german_proxy,
                headless=True 
            )

            geo_config_berlin = GeolocationConfig(latitude=52.5200, longitude=13.4050, accuracy=100.0)
            
            # article_strategy = SomeArticleExtractionStrategy() # Replace with actual strategy

            run_cfg_german_news = CrawlerRunConfig(
                geolocation=geo_config_berlin,
                locale="de-DE",
                timezone_id="Europe/Berlin",
                # extraction_strategy=article_strategy,
                cache_mode=CacheMode.BYPASS # Ensure fresh content for geo-testing
            )

            async with AsyncWebCrawler(config=browser_cfg_german) as crawler:
                # Use a site that shows location or IP for testing, e.g., ipinfo.io
                result = await crawler.arun(url="https://ipinfo.io/json", config=run_cfg_german_news)
                
                if result.success:
                    print("--- Geo-Targeted Crawl Result (ipinfo.io) ---")
                    print(result.html) # Should show German IP and location details
                    # For a real news site, you'd inspect result.markdown or result.extracted_content
                else:
                    print(f"Crawl failed: {result.error_message}")

        # await crawl_german_news() # Uncomment to run with a real proxy
        ```

*   7.3. **Scenario 2: High-Volume Data Extraction from API-like Endpoints (No JS) with Rate Limiting**
    *   **Objective:** Efficiently scrape data from a list of 1000 product API endpoints (e.g., `api.example.com/product/{id}`) that return JSON and are known to be static (no JavaScript rendering needed). Ensure polite crawling to avoid overwhelming the server.
    *   **Strategy Choice:** `AsyncHTTPCrawlerStrategy` is ideal here for speed and low overhead.
    *   **`HTTPCrawlerConfig` Elements (if needed per request, often passed directly to `arun` with HTTP strategy):**
        *   `headers`: If the API requires specific headers like an `Authorization` token or `Accept: application/json`.
        *   `method`: Likely "GET" for fetching product data.
    *   **`CrawlerRunConfig` Elements:**
        *   `extraction_strategy`: `NoExtractionStrategy` if the API returns clean JSON directly in `result.html`. If it returns HTML containing JSON (e.g., in a `<script>` tag), you might need a custom extractor or a simple regex in post-processing.
        *   `cache_mode`: `CacheMode.ENABLED` might be good if product data doesn't change extremely frequently, or `CacheMode.BYPASS` if always fresh data is paramount.
    *   **Dispatcher & Rate Limiting:**
        *   Use `crawler.arun_many()` with its default `MemoryAdaptiveDispatcher`.
        *   Configure the `CrawlerRunConfig` (passed to `arun_many`) with `mean_delay` and `max_range` to introduce delays between requests to the *same domain*.
        *   The `MemoryAdaptiveDispatcher` itself can also be configured with a `RateLimiter` instance for more global control if needed, but per-domain delays via `CrawlerRunConfig` are often sufficient for politeness.
    *   **Workflow Explanation:**
        1.  Instantiate `AsyncWebCrawler` with `AsyncHTTPCrawlerStrategy`.
        2.  Prepare a list of product API URLs.
        3.  Create a `CrawlerRunConfig` that includes `mean_delay` and `max_range` for polite crawling.
        4.  Call `crawler.arun_many(urls=product_urls, config=run_config_with_delay)`.
        5.  The dispatcher will manage concurrency (based on memory by default) and inter-request delays.
        6.  Each result's `html` attribute will contain the raw JSON response from the API.
    *   **Code Example: Fetching data from a list of URLs using `AsyncHTTPCrawlerStrategy`.**
        ```python
        from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
        from crawl4ai.async_crawler_strategy import AsyncHTTPCrawlerStrategy
        from crawl4ai.async_dispatchers import MemoryAdaptiveDispatcher, RateLimiter # For more advanced control
        import json
        import asyncio

        # Sample product IDs
        product_ids = list(range(1, 21)) # Let's do 20 for a quick demo
        api_urls = [f"https://jsonplaceholder.typicode.com/todos/{pid}" for pid in product_ids]

        async def fetch_product_apis():
            http_strategy = AsyncHTTPCrawlerStrategy()
            
            # Configure run config for politeness
            # This will apply per-domain delays managed by the dispatcher
            run_config_polite = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                mean_delay=0.5,  # Average 0.5s delay between requests to jsonplaceholder.typicode.com
                max_range=0.3,   # Add random 0-0.3s to that
                # No specific extraction_strategy needed as API returns JSON directly in result.html
            )
            
            # Optional: Configure the dispatcher itself if more control than CrawlerRunConfig's delay offers
            # custom_dispatcher = MemoryAdaptiveDispatcher(
            #     rate_limiter=RateLimiter(base_delay=(0.5, 1.0)) # Global rate limiting
            # )

            async with AsyncWebCrawler(crawler_strategy=http_strategy) as crawler:
                print(f"Fetching {len(api_urls)} URLs...")
                results_stream = await crawler.arun_many(
                    urls=api_urls, 
                    config=run_config_polite,
                    # dispatcher=custom_dispatcher # If using custom dispatcher
                )
                
                all_product_data = []
                async for result_container in results_stream: # Assuming stream=True in run_config_polite
                    result = result_container.result # Access the CrawlResult
                    if result.success:
                        try:
                            product_data = json.loads(result.html)
                            all_product_data.append(product_data)
                            print(f"Fetched: {product_data.get('title', 'N/A')[:30]}...")
                        except json.JSONDecodeError:
                            print(f"Error parsing JSON for {result.url}: {result.html[:100]}")
                    else:
                        print(f"Failed {result.url}: {result.error_message}")
                
                print(f"\nSuccessfully fetched {len(all_product_data)} product details.")
                # print("Sample of first product:", all_product_data[0] if all_product_data else "None")

        # await fetch_product_apis()
        ```
        *Note: For `arun_many`, `CrawlerRunConfig`'s `mean_delay` and `max_range` are hints for the dispatcher's internal per-domain rate limiting. The `RateLimiter` object passed to the dispatcher provides more explicit global control.*

*   7.4. **Scenario 3: Multi-Step Authenticated Crawl with LLM-based Data Summarization**
    *   **Objective:**
        1.  Log into a website.
        2.  Navigate to a user-specific dashboard page.
        3.  Extract structured data (e.g., a list of recent orders) from the dashboard.
        4.  Use an LLM to generate a brief summary of these orders.
    *   **`BrowserConfig` Elements:**
        *   `use_persistent_context=True`, `user_data_dir="my_site_profile"`: To save and reuse login cookies/session.
        *   `headless=False` (recommended for initial login script development).
    *   **`CrawlerRunConfig` (Step 1: Login):**
        *   `url`: Login page URL.
        *   `session_id`: A unique ID, e.g., "my_dashboard_session".
        *   `js_code`: JavaScript to fill username, password, and click submit.
        *   `wait_for`: CSS selector or JS condition confirming successful login (e.g., visibility of a dashboard element or URL change).
        *   `cache_mode=CacheMode.BYPASS` (to ensure login is attempted).
    *   **`CrawlerRunConfig` (Step 2: Navigate & Extract Data - using same `session_id`):**
        *   `url`: Dashboard page URL.
        *   `session_id`: Must be "my_dashboard_session".
        *   `extraction_strategy`: An instance of `JsonCssExtractionStrategy` (or `LLMExtractionStrategy`) configured to extract order details.
        *   `cache_mode=CacheMode.BYPASS` (to get fresh dashboard data).
    *   **`LLMConfig` (for summarization, if using an LLM strategy for it):**
        *   `provider`, `api_token`.
        *   `temperature`, `max_tokens` suitable for summarization.
    *   **Post-processing or an `LLMSummarizationStrategy`:**
        *   If summarization is a separate step: After getting `extracted_content` (list of orders), manually call an LLM with this data.
        *   If using a hypothetical `LLMSummarizationStrategy`: This strategy would take the extracted order data (perhaps from a previous `extraction_strategy` or directly from the page content if simple enough) and use the LLM to summarize it. This would be part of the `CrawlerRunConfig` for Step 2.
    *   **Workflow Explanation:**
        1.  The first `arun()` call uses `js_code` to log in. The session (cookies) is stored due to `use_persistent_context`.
        2.  The second `arun()` call reuses the `session_id`. Playwright/Crawl4ai uses the stored cookies, allowing access to the dashboard. The `extraction_strategy` then pulls the order data.
        3.  The extracted order data (JSON string from `result.extracted_content`) is parsed.
        4.  This data is then passed to an LLM for summarization (either via another `LLMExtractionStrategy` configured for summarization or a direct API call).
    *   **Code Example: Focusing on the `CrawlerRunConfig` aspects.**
        ```python
        from crawl4ai import (
            AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, LLMConfig,
            LLMExtractionStrategy, CacheMode
        )
        from pydantic import BaseModel, Field
        import json
        import os

        # --- Schemas ---
        class OrderItem(BaseModel):
            item_name: str
            quantity: int
            price: float

        class DashboardData(BaseModel):
            user_name: str
            recent_orders: list[OrderItem]

        # --- Mock HTML ---
        LOGIN_PAGE_HTML = "<html><body><form><input name='user'><input name='pass' type='password'><button type='submit'>Login</button></form></body></html>"
        DASHBOARD_HTML_TEMPLATE = """
        <html><body><div id='dashboard'>
            Welcome, {user_name}!
            <h2>Recent Orders</h2>
            <ul id='order-list'>
                <li><span>Order 1: Widget A (2) @ $10.00</span></li>
                <li><span>Order 2: Gadget B (1) @ $25.50</span></li>
            </ul>
        </div></body></html>
        """

        async def run_authenticated_llm_summary():
            session_id = "auth_crawl_session"
            user_data_dir = "./auth_browser_profile" # For session persistence
            
            # For real use, ensure OPENAI_API_KEY is set
            if not os.getenv("OPENAI_API_KEY"):
                print("OPENAI_API_KEY not set. Skipping authenticated LLM summary example.")
                return

            # Browser config with persistence
            browser_cfg = BrowserConfig(
                use_persistent_context=True, 
                user_data_dir=user_data_dir,
                headless=True # Set to False to observe login if needed
            )

            # LLM Config for extraction & summarization
            llm_conf = LLMConfig(provider="openai/gpt-4o-mini", api_token=os.getenv("OPENAI_API_KEY"), temperature=0.2)

            # Strategy to extract orders from dashboard
            order_extraction_strategy = LLMExtractionStrategy(
                llm_config=llm_conf,
                schema=DashboardData.model_json_schema(),
                instruction="Extract the username and all recent orders from the dashboard HTML. For each order, get item name, quantity, and price.",
                input_format="html" # Feed raw HTML for LLM to parse structure
            )

            async with AsyncWebCrawler(config=browser_cfg) as crawler:
                # Step 1: Simulate Login (replace with actual login logic for a real site)
                # For this example, we'll just navigate to a mock "login successful" page
                # In a real scenario, js_code would fill and submit the login form.
                print("Simulating login...")
                login_config = CrawlerRunConfig(
                    url=f"raw://{LOGIN_PAGE_HTML.replace('{user_name}', 'TestUser')}", # Mock successful login state
                    session_id=session_id,
                    wait_for="css:body" # Just wait for body to exist on this mock page
                )
                login_result = await crawler.arun(config=login_config)
                if not login_result.success:
                    print(f"Login step failed: {login_result.error_message}")
                    return
                print("Login step simulated/completed.")

                # Step 2: Navigate to dashboard and extract orders
                print("Navigating to dashboard and extracting orders...")
                dashboard_html = DASHBOARD_HTML_TEMPLATE.replace("{user_name}", "TestUser") # Mock dashboard
                dashboard_config = CrawlerRunConfig(
                    url=f"raw://{dashboard_html}", # Use mock dashboard HTML
                    session_id=session_id,
                    extraction_strategy=order_extraction_strategy,
                    cache_mode=CacheMode.BYPASS
                )
                dashboard_result = await crawler.arun(config=dashboard_config)

                if not dashboard_result.success or not dashboard_result.extracted_content:
                    print(f"Dashboard data extraction failed: {dashboard_result.error_message}")
                    await crawler.kill_session(session_id)
                    return
                
                print("Orders extracted successfully.")
                extracted_data = json.loads(dashboard_result.extracted_content)
                
                # LLMExtractionStrategy might return a list, take the first element.
                dashboard_info = DashboardData(**(extracted_data[0] if isinstance(extracted_data, list) else extracted_data))
                print(f"Welcome, {dashboard_info.user_name}!")
                for order in dashboard_info.recent_orders:
                    print(f" - {order.item_name} (x{order.quantity}) at ${order.price}")

                # Step 3: Summarize orders using another LLM call (can be part of a more complex strategy or separate)
                if dashboard_info.recent_orders:
                    print("Summarizing orders...")
                    orders_text = "\n".join([f"- {o.item_name} (x{o.quantity}) for ${o.price}" for o in dashboard_info.recent_orders])
                    
                    summarization_prompt = f"Summarize these orders for {dashboard_info.user_name}:\n{orders_text}\n\nSummary:"
                    
                    # Using a generic completion method for simplicity, could also be another LLMExtractionStrategy
                    from crawl4ai.utils import perform_completion_with_backoff # Assuming direct LiteLLM call
                    summary_response = await perform_completion_with_backoff(
                        provider=llm_conf.provider,
                        prompt=summarization_prompt, # Note: LiteLLM uses 'messages' array usually
                        messages=[{"role": "user", "content": summarization_prompt}],
                        api_key=llm_conf.api_token,
                        base_url=llm_conf.base_url,
                        max_tokens=100
                    )
                    summary_text = summary_response.choices[0].message.content
                    print(f"\nOrder Summary:\n{summary_text}")

                # Clean up session
                await crawler.kill_session(session_id)
                # And remove profile dir if it was for temp use
                # import shutil; shutil.rmtree(user_data_dir, ignore_errors=True)
        
        # await run_authenticated_llm_summary() # Uncomment to run
        ```

*   7.5. **Scenario 4: Dynamic Content Scraping with Robust Error Handling and Fallbacks**
    *   **Objective:** Scrape product details from an e-commerce site where some product attributes (e.g., "discounted price," "stock level") might load dynamically or not be present for all items. The goal is to get as much data as possible and handle missing pieces gracefully.
    *   **`BrowserConfig` Elements:**
        *   Standard setup, potentially with `headless=False` during development for observation.
    *   **`CrawlerRunConfig` Elements (and Python control flow):**
        *   **Initial Load & Wait:**
            *   `url`: The product page URL.
            *   `wait_for`: A selector for a core element that *must* be present (e.g., product title or main image).
        *   **Attempting to Trigger Dynamic Content (if applicable):**
            *   `js_code`: May include clicks on tabs (e.g., "Specifications," "Reviews") or scrolls if certain data is lazy-loaded upon such interactions.
            *   Further `wait_for` calls after each interaction to allow content to load.
        *   **Extraction Strategy (e.g., `JsonCssExtractionStrategy` or `LLMExtractionStrategy`):**
            *   The schema should define fields as `Optional` where data might be missing (e.g., `discounted_price: Optional[float] = None`).
            *   For CSS-based extraction, selectors for optional fields should be robust enough not to break if the element isn't found (the strategy should handle this by returning `None` for that field).
        *   **Python-Level Fallbacks/Retries (Conceptual):**
            While `CrawlerRunConfig` itself doesn't have direct retry logic for parts of an extraction, you can structure your Python code around `arun()`:
            ```python
            # Conceptual Python-level retry for an optional element
            # result = await crawler.arun(config=initial_config)
            # extracted_data = json.loads(result.extracted_content)[0]
            # if not extracted_data.get("stock_level"):
            #     print("Stock level not found, trying to click 'Check Stock' button...")
            #     retry_config = initial_config.clone(
            #         js_code="document.querySelector('#check-stock-btn')?.click();",
            #         wait_for="css:.stock-info-loaded", # Wait for stock info to appear
            #         js_only=True, # Operate on the same page
            #         session_id="product_page_session" # Ensure same page
            #     )
            #     stock_result = await crawler.arun(config=retry_config)
            #     # Re-extract or merge results
            ```
    *   **Workflow Explanation:**
        1.  Load the main page and wait for essential static elements.
        2.  If certain data is known to be dynamic (e.g., loaded on a tab click), use `js_code` to trigger that interaction, followed by another `wait_for`.
        3.  Use an extraction strategy with an optional schema.
        4.  If key optional data is missing, and there's a known interaction to reveal it (like clicking a button), you can make a subsequent `arun()` call (with `js_only=True` and the same `session_id`) to perform that action and then attempt to re-extract or extract just that missing piece.
    *   **Code Example (Conceptual - focusing on the idea of layered attempts):**
        ```python
        from crawl4ai import (
            AsyncWebCrawler, BrowserConfig, CrawlerRunConfig,
            JsonCssExtractionStrategy, CacheMode # Example using CSS strategy
        )
        import json

        # Define a schema where some fields are optional
        PRODUCT_SCHEMA = {
            "name": "Product Info",
            "baseSelector": "div.product-main", # Assuming a main product container
            "fields": [
                {"name": "title", "selector": "h1.product-title", "type": "text"},
                {"name": "price", "selector": ".price-current", "type": "text"},
                # Optional field: discount might not always be there
                {"name": "discounted_price", "selector": ".price-discounted", "type": "text", "default": None},
                # Optional field: stock might load after a click
                {"name": "stock_status", "selector": ".stock-status-display", "type": "text", "default": "Unknown"}
            ]
        }
        
        # Mock HTMLs
        INITIAL_HTML = """
        <div class='product-main'>
            <h1 class='product-title'>Super Widget</h1>
            <span class='price-current'>$100</span>
            <!-- Discounted price and stock are not initially visible -->
            <button id='show-details-btn'>Show More Details</button>
            <div id='extra-details' style='display:none;'>
                 <span class='price-discounted'>$80</span>
                 <span class='stock-status-display'>In Stock</span>
            </div>
        </div>
        """
        HTML_AFTER_CLICK = INITIAL_HTML.replace("style='display:none;'", "style='display:block;'")


        async def crawl_dynamic_product():
            session_id = "dynamic_product_session"
            extraction_strategy = JsonCssExtractionStrategy(PRODUCT_SCHEMA)

            async with AsyncWebCrawler(config=BrowserConfig(headless=True)) as crawler:
                # --- Attempt 1: Initial Load ---
                print("--- Attempt 1: Initial Load ---")
                config_attempt1 = CrawlerRunConfig(
                    url=f"raw://{INITIAL_HTML}",
                    session_id=session_id,
                    extraction_strategy=extraction_strategy,
                    cache_mode=CacheMode.BYPASS
                )
                result1 = await crawler.arun(config=config_attempt1)
                data1 = {}
                if result1.success and result1.extracted_content:
                    data1_list = json.loads(result1.extracted_content)
                    if data1_list: data1 = data1_list[0]
                print(f"Initial Data: {data1}")

                # --- Attempt 2: Click button and re-evaluate (or re-extract if strategy supports it) ---
                # If some data is missing (e.g., stock_status is 'Unknown' or discounted_price is None)
                # and we know an action can reveal it.
                if data1.get("stock_status") == "Unknown" or not data1.get("discounted_price"):
                    print("\n--- Attempt 2: Clicking 'Show More Details' ---")
                    
                    # For this raw HTML example, we'll just "navigate" to the state after click
                    # In a real scenario, js_code would click the button.
                    config_attempt2 = CrawlerRunConfig(
                        url=f"raw://{HTML_AFTER_CLICK}", # Simulating state after click
                        session_id=session_id, # Maintain session
                        # js_code="document.getElementById('show-details-btn')?.click();", # Real interaction
                        # wait_for="css:#extra-details[style*='display:block']", # Wait for it to be visible
                        js_only=False, # Set to True if js_code is used on existing page
                        extraction_strategy=extraction_strategy, # Re-extract
                        cache_mode=CacheMode.BYPASS
                    )
                    result2 = await crawler.arun(config=config_attempt2)
                    data2 = {}
                    if result2.success and result2.extracted_content:
                        data2_list = json.loads(result2.extracted_content)
                        if data2_list: data2 = data2_list[0]
                    print(f"Data after interaction: {data2}")
                    # In a real app, you'd merge data1 and data2 intelligently
                
                await crawler.kill_session(session_id)

        # await crawl_dynamic_product()
        ```
        This conceptual example shows how you might chain `arun` calls with different `CrawlerRunConfig`s (sharing a `session_id`) to handle dynamic content revealing steps. More robust solutions might involve custom retry logic in Python or more sophisticated `wait_for` JS expressions.


## 8. Conclusion and Further Exploration

*   8.1. **Recap of the power and flexibility offered by Crawl4ai's configuration objects.**
    Throughout this guide, we've explored how `BrowserConfig`, `CrawlerRunConfig`, `LLMConfig`, and other specialized configuration objects in Crawl4ai provide a powerful and flexible framework for tailoring your web crawling and scraping tasks. From defining browser identity and environment to controlling per-page interactions, content extraction, media handling, and LLM integration, these objects give you granular control over every aspect of the crawl. The separation of concerns and methods like `clone()`, `dump()`, and `load()` further enhance reusability and manageability of your configurations.

*   8.2. **Encouragement to experiment with different combinations.**
    The true strength of Crawl4ai's configuration system lies in the ability to combine these objects and their parameters in creative ways to solve unique challenges. Don't hesitate to experiment:
    *   Try different `user_agent` strings with varying `headless` modes.
    *   Combine `css_selector` with `target_elements` for precise content focus.
    *   Use `js_code` and `wait_for` to navigate complex SPAs.
    *   Integrate `LLMExtractionStrategy` with fine-tuned `LLMConfig` settings for difficult extractions.
    *   Leverage `session_id` for multi-step workflows.
    The more you experiment, the better you'll understand how to harness the full potential of Crawl4ai for your specific needs.

*   8.3. **Pointers to other relevant documentation sections.**
    This guide has focused on the "how" and "why" of using configuration objects. For more details on specific areas, please refer to:
    *   **API Reference / "Foundational Memory" Document for `config_objects`:** For an exhaustive list of all parameters, their types, and default values.
    *   **Documentation on Specific Strategies:** Deep dives into `LLMExtractionStrategy`, `JsonCssExtractionStrategy`, `AsyncHTTPCrawlerStrategy`, various `MarkdownGenerationStrategy` and `ContentFilterStrategy` options.
    *   **Advanced Browser Management:** Detailed guides on `use_persistent_context`, `user_data_dir`, Docker integration, and managing browser profiles.
    *   **`arun_many()` and Dispatchers:** For understanding how to efficiently crawl multiple URLs in parallel and customize dispatch behavior with `MemoryAdaptiveDispatcher`, `SemaphoreDispatcher`, and `RateLimiter`.
    *   **Hooks and Custom Callbacks:** For advanced customization of the crawling lifecycle.

By mastering these configuration objects, you can build robust, efficient, and highly customized web crawlers with Crawl4ai. Happy crawling!
```

---


## Deep Crawling - Memory
Source: crawl4ai_deep_crawling_memory_content.llm.md

Okay, I have read and processed the information. I will now generate the detailed "Foundational Memory" document for the `deep_crawling` component of `crawl4ai`, based on the outline from the previous step and the provided code snippets.

```markdown
# Detailed Foundational Memory for crawl4ai - deep_crawling Component

**Target Document Type:** memory
**Target Output Filename Suggestion:** `llm_memory_deep_crawling.md`
**Library Version Context:** 0.6.3 (from `crawl4ai/__version__.py`)
**Outline Generation Date:** 2024-05-24
---

## 1. Introduction to Deep Crawling

*   1.1. Purpose: The `deep_crawling` component provides functionalities for recursively crawling web pages starting from an initial URL. It includes strategies for different traversal orders (BFS, DFS, Best-First), mechanisms for filtering which URLs to visit, and methods for scoring URLs to prioritize crawling.
*   1.2. Core Concepts:
    *   1.2.1. Definition of Deep Crawling in Crawl4ai context: The process of discovering and fetching multiple web pages by following links from an initial set of URLs, adhering to specified depth, page limits, and filtering/scoring rules.
    *   1.2.2. Key Abstractions:
        *   `DeepCrawlStrategy`: Defines the algorithm for traversing linked web pages (e.g., BFS, DFS).
        *   `URLFilter`: Determines whether a discovered URL should be considered for crawling.
        *   `URLScorer`: Assigns a score to URLs to influence crawling priority, especially in strategies like Best-First.

## 2. `DeepCrawlStrategy` Interface and Implementations

*   **2.1. `DeepCrawlStrategy` (Abstract Base Class)**
    *   Source: `crawl4ai/deep_crawling/base_strategy.py`
    *   2.1.1. Purpose: Defines the abstract base class for all deep crawling strategies, outlining the core methods required for traversal logic, resource management, URL validation, and link discovery.
    *   2.1.2. Key Abstract Methods:
        *   `async def _arun_batch(self, start_url: str, crawler: AsyncWebCrawler, config: CrawlerRunConfig) -> List[CrawlResult]`:
            *   Description: Core logic for batch (non-streaming) deep crawling. Processes URLs level by level (or according to strategy) and returns all results once the crawl is complete or limits are met.
        *   `async def _arun_stream(self, start_url: str, crawler: AsyncWebCrawler, config: CrawlerRunConfig) -> AsyncGenerator[CrawlResult, None]`:
            *   Description: Core logic for streaming deep crawling. Processes URLs and yields `CrawlResult` objects as they become available.
        *   `async def shutdown(self) -> None`:
            *   Description: Cleans up any resources used by the deep crawl strategy, such as signaling cancellation events.
        *   `async def can_process_url(self, url: str, depth: int) -> bool`:
            *   Description: Validates a given URL and current depth against configured filters and limits to decide if it should be processed.
        *   `async def link_discovery(self, result: CrawlResult, source_url: str, current_depth: int, visited: Set[str], next_level: List[tuple], depths: Dict[str, int]) -> None`:
            *   Description: Extracts links from a `CrawlResult`, validates them using `can_process_url`, optionally scores them, and appends valid URLs (and their parent references) to the `next_level` list. Updates the `depths` dictionary for newly discovered URLs.
    *   2.1.3. Key Concrete Methods:
        *   `async def arun(self, start_url: str, crawler: AsyncWebCrawler, config: Optional[CrawlerRunConfig] = None) -> RunManyReturn`:
            *   Description: Main entry point for initiating a deep crawl. It checks if a `CrawlerRunConfig` is provided and then delegates to either `_arun_stream` or `_arun_batch` based on the `config.stream` flag.
        *   `def __call__(self, start_url: str, crawler: AsyncWebCrawler, config: CrawlerRunConfig)`:
            *   Description: Makes the strategy instance callable, directly invoking the `arun` method.
    *   2.1.4. Attributes:
        *   `_cancel_event (asyncio.Event)`: Event to signal cancellation of the crawl.
        *   `_pages_crawled (int)`: Counter for the number of pages successfully crawled.

*   **2.2. `BFSDeepCrawlStrategy`**
    *   Source: `crawl4ai/deep_crawling/bfs_strategy.py`
    *   2.2.1. Purpose: Implements a Breadth-First Search (BFS) deep crawling strategy, exploring all URLs at the current depth level before moving to the next.
    *   2.2.2. Inheritance: `DeepCrawlStrategy`
    *   2.2.3. Initialization (`__init__`)
        *   2.2.3.1. Signature:
            ```python
            def __init__(
                self,
                max_depth: int,
                filter_chain: FilterChain = FilterChain(),
                url_scorer: Optional[URLScorer] = None,
                include_external: bool = False,
                score_threshold: float = -float('inf'),
                max_pages: int = float('inf'),
                logger: Optional[logging.Logger] = None,
            ):
            ```
        *   2.2.3.2. Parameters:
            *   `max_depth (int)`: Maximum depth to crawl relative to the `start_url`.
            *   `filter_chain (FilterChain`, default: `FilterChain()`)`: A `FilterChain` instance to apply to discovered URLs.
            *   `url_scorer (Optional[URLScorer]`, default: `None`)`: An optional `URLScorer` to score URLs. If provided, URLs below `score_threshold` are skipped, and for crawls exceeding `max_pages`, higher-scored URLs are prioritized.
            *   `include_external (bool`, default: `False`)`: If `True`, allows crawling of URLs from external domains.
            *   `score_threshold (float`, default: `-float('inf')`)`: Minimum score (if `url_scorer` is used) for a URL to be processed.
            *   `max_pages (int`, default: `float('inf')`)`: Maximum total number of pages to crawl.
            *   `logger (Optional[logging.Logger]`, default: `None`)`: An optional logger instance. If `None`, a default logger is created.
    *   2.2.4. Key Implemented Methods:
        *   `_arun_batch(...)`: Implements BFS traversal by processing URLs level by level. It collects all results from a level before discovering links for the next level. All results are returned as a list upon completion.
        *   `_arun_stream(...)`: Implements BFS traversal, yielding `CrawlResult` objects as soon as they are processed within a level. Link discovery for the next level happens after all URLs in the current level are processed and their results yielded.
        *   `can_process_url(...)`: Validates URL format, applies the `filter_chain`, and checks depth limits. For the start URL (depth 0), filtering is bypassed.
        *   `link_discovery(...)`: Extracts internal (and optionally external) links, normalizes them, checks against `visited` set and `can_process_url`. If a `url_scorer` is present and `max_pages` limit is a concern, it scores and sorts valid links, selecting the top ones within `remaining_capacity`.
        *   `shutdown(...)`: Sets an internal `_cancel_event` to signal graceful termination and records the end time in `stats`.
    *   2.2.5. Key Attributes/Properties:
        *   `stats (TraversalStats)`: [Read-only] - Instance of `TraversalStats` tracking the progress and statistics of the crawl.
        *   `max_depth (int)`: Maximum crawl depth.
        *   `filter_chain (FilterChain)`: The filter chain used.
        *   `url_scorer (Optional[URLScorer])`: The URL scorer used.
        *   `include_external (bool)`: Flag for including external URLs.
        *   `score_threshold (float)`: URL score threshold.
        *   `max_pages (int)`: Maximum pages to crawl.

*   **2.3. `DFSDeepCrawlStrategy`**
    *   Source: `crawl4ai/deep_crawling/dfs_strategy.py`
    *   2.3.1. Purpose: Implements a Depth-First Search (DFS) deep crawling strategy, exploring as far as possible along each branch before backtracking.
    *   2.3.2. Inheritance: `BFSDeepCrawlStrategy` (Note: Leverages much of the `BFSDeepCrawlStrategy`'s infrastructure but overrides traversal logic to use a stack.)
    *   2.3.3. Initialization (`__init__`)
        *   2.3.3.1. Signature: (Same as `BFSDeepCrawlStrategy`)
            ```python
            def __init__(
                self,
                max_depth: int,
                filter_chain: FilterChain = FilterChain(),
                url_scorer: Optional[URLScorer] = None,
                include_external: bool = False,
                score_threshold: float = -float('inf'),
                max_pages: int = infinity,
                logger: Optional[logging.Logger] = None,
            ):
            ```
        *   2.3.3.2. Parameters: Same as `BFSDeepCrawlStrategy`.
    *   2.3.4. Key Overridden/Implemented Methods:
        *   `_arun_batch(...)`: Implements DFS traversal using a LIFO stack. Processes one URL at a time, discovers its links, and adds them to the stack (typically in reverse order of discovery to maintain a natural DFS path). Collects all results in a list.
        *   `_arun_stream(...)`: Implements DFS traversal using a LIFO stack, yielding `CrawlResult` for each processed URL as it becomes available. Discovered links are added to the stack for subsequent processing.

*   **2.4. `BestFirstCrawlingStrategy`**
    *   Source: `crawl4ai/deep_crawling/bff_strategy.py`
    *   2.4.1. Purpose: Implements a Best-First Search deep crawling strategy, prioritizing URLs based on scores assigned by a `URLScorer`. It uses a priority queue to manage URLs to visit.
    *   2.4.2. Inheritance: `DeepCrawlStrategy`
    *   2.4.3. Initialization (`__init__`)
        *   2.4.3.1. Signature:
            ```python
            def __init__(
                self,
                max_depth: int,
                filter_chain: FilterChain = FilterChain(),
                url_scorer: Optional[URLScorer] = None,
                include_external: bool = False,
                max_pages: int = float('inf'),
                logger: Optional[logging.Logger] = None,
            ):
            ```
        *   2.4.3.2. Parameters:
            *   `max_depth (int)`: Maximum depth to crawl.
            *   `filter_chain (FilterChain`, default: `FilterChain()`)`: Chain of filters to apply.
            *   `url_scorer (Optional[URLScorer]`, default: `None`)`: Scorer to rank URLs. Crucial for this strategy; if not provided, URLs might effectively be processed in FIFO order (score 0).
            *   `include_external (bool`, default: `False`)`: Whether to include external links.
            *   `max_pages (int`, default: `float('inf')`)`: Maximum number of pages to crawl.
            *   `logger (Optional[logging.Logger]`, default: `None`)`: Logger instance.
    *   2.4.4. Key Implemented Methods:
        *   `_arun_batch(...)`: Aggregates results from `_arun_best_first` into a list.
        *   `_arun_stream(...)`: Yields results from `_arun_best_first` as they are generated.
        *   `_arun_best_first(...)`: Core logic for best-first traversal. Uses an `asyncio.PriorityQueue` where items are `(score, depth, url, parent_url)`. URLs are processed in batches (default size 10) from the priority queue. Discovered links are scored and added to the queue.
    *   2.4.5. Key Attributes/Properties:
        *   `stats (TraversalStats)`: [Read-only] - Traversal statistics object.
        *   `BATCH_SIZE (int)`: [Class constant, default: 10] - Number of URLs to process concurrently from the priority queue.

## 3. URL Filtering Mechanisms

*   **3.1. `URLFilter` (Abstract Base Class)**
    *   Source: `crawl4ai/deep_crawling/filters.py`
    *   3.1.1. Purpose: Defines the abstract base class for all URL filters, providing a common interface for deciding whether a URL should be processed.
    *   3.1.2. Key Abstract Methods:
        *   `apply(self, url: str) -> bool`:
            *   Description: Abstract method that must be implemented by subclasses. It takes a URL string and returns `True` if the URL passes the filter (should be processed), and `False` otherwise.
    *   3.1.3. Key Attributes/Properties:
        *   `name (str)`: [Read-only] - The name of the filter, typically the class name.
        *   `stats (FilterStats)`: [Read-only] - An instance of `FilterStats` to track how many URLs were processed, passed, and rejected by this filter.
        *   `logger (logging.Logger)`: [Read-only] - A logger instance specific to this filter, initialized lazily.
    *   3.1.4. Key Concrete Methods:
        *   `_update_stats(self, passed: bool) -> None`: Updates the `stats` object (total, passed, rejected counts).

*   **3.2. `FilterChain`**
    *   Source: `crawl4ai/deep_crawling/filters.py`
    *   3.2.1. Purpose: Manages a sequence of `URLFilter` instances. A URL must pass all filters in the chain to be considered valid.
    *   3.2.2. Initialization (`__init__`)
        *   3.2.2.1. Signature:
            ```python
            def __init__(self, filters: List[URLFilter] = None):
            ```
        *   3.2.2.2. Parameters:
            *   `filters (List[URLFilter]`, default: `None`)`: An optional list of `URLFilter` instances to initialize the chain with. If `None`, an empty chain is created.
    *   3.2.3. Key Public Methods:
        *   `add_filter(self, filter_: URLFilter) -> FilterChain`:
            *   Description: Adds a new `URLFilter` instance to the end of the chain.
            *   Returns: `(FilterChain)` - The `FilterChain` instance itself, allowing for method chaining.
        *   `async def apply(self, url: str) -> bool`:
            *   Description: Applies each filter in the chain to the given URL. If any filter returns `False` (rejects the URL), this method immediately returns `False`. If all filters pass, it returns `True`. Handles both synchronous and asynchronous `apply` methods of individual filters.
            *   Returns: `(bool)` - `True` if the URL passes all filters, `False` otherwise.
    *   3.2.4. Key Attributes/Properties:
        *   `filters (Tuple[URLFilter, ...])`: [Read-only] - An immutable tuple containing the `URLFilter` instances in the chain.
        *   `stats (FilterStats)`: [Read-only] - An instance of `FilterStats` tracking the aggregated statistics for the entire chain (total URLs processed, passed, and rejected by the chain as a whole).

*   **3.3. `URLPatternFilter`**
    *   Source: `crawl4ai/deep_crawling/filters.py`
    *   3.3.1. Purpose: Filters URLs based on whether they match a list of specified string patterns. Supports glob-style wildcards and regular expressions.
    *   3.3.2. Inheritance: `URLFilter`
    *   3.3.3. Initialization (`__init__`)
        *   3.3.3.1. Signature:
            ```python
            def __init__(
                self,
                patterns: Union[str, Pattern, List[Union[str, Pattern]]],
                use_glob: bool = True, # Deprecated, glob is always used for strings if not regex
                reverse: bool = False,
            ):
            ```
        *   3.3.3.2. Parameters:
            *   `patterns (Union[str, Pattern, List[Union[str, Pattern]]])`: A single pattern string/compiled regex, or a list of such patterns. String patterns are treated as glob patterns by default unless they are identifiable as regex (e.g., start with `^`, end with `$`, contain `\d`).
            *   `use_glob (bool`, default: `True`)`: [Deprecated] This parameter's functionality is now implicitly handled by pattern detection.
            *   `reverse (bool`, default: `False`)`: If `True`, the filter rejects URLs that match any of the patterns. If `False` (default), it accepts URLs that match any pattern and rejects those that don't match any.
    *   3.3.4. Key Implemented Methods:
        *   `apply(self, url: str) -> bool`:
            *   Description: Checks if the URL matches any of the configured patterns. Simple suffix/prefix/domain patterns are checked first for performance. For more complex patterns, it uses `fnmatch.translate` (for glob-like strings) or compiled regex objects. The outcome is affected by the `reverse` flag.
    *   3.3.5. Internal Categorization:
        *   `PATTERN_TYPES`: A dictionary mapping pattern types (SUFFIX, PREFIX, DOMAIN, PATH, REGEX) to integer constants.
        *   `_simple_suffixes (Set[str])`: Stores simple suffix patterns (e.g., `.html`).
        *   `_simple_prefixes (Set[str])`: Stores simple prefix patterns (e.g., `/blog/`).
        *   `_domain_patterns (List[Pattern])`: Stores compiled regex for domain-specific patterns (e.g., `*.example.com`).
        *   `_path_patterns (List[Pattern])`: Stores compiled regex for more general path patterns.

*   **3.4. `ContentTypeFilter`**
    *   Source: `crawl4ai/deep_crawling/filters.py`
    *   3.4.1. Purpose: Filters URLs based on their expected content type, primarily by inferring it from the file extension in the URL.
    *   3.4.2. Inheritance: `URLFilter`
    *   3.4.3. Initialization (`__init__`)
        *   3.4.3.1. Signature:
            ```python
            def __init__(
                self,
                allowed_types: Union[str, List[str]],
                check_extension: bool = True,
                ext_map: Dict[str, str] = _MIME_MAP, # _MIME_MAP is internal
            ):
            ```
        *   3.4.3.2. Parameters:
            *   `allowed_types (Union[str, List[str]])`: A single MIME type string (e.g., "text/html") or a list of allowed MIME types. Can also be partial types like "image/" to allow all image types.
            *   `check_extension (bool`, default: `True`)`: If `True` (default), the filter attempts to determine the content type by looking at the URL's file extension. If `False`, all URLs pass this filter (unless `allowed_types` is empty).
            *   `ext_map (Dict[str, str]`, default: `ContentTypeFilter._MIME_MAP`)`: A dictionary mapping file extensions to their corresponding MIME types. A comprehensive default map is provided.
    *   3.4.4. Key Implemented Methods:
        *   `apply(self, url: str) -> bool`:
            *   Description: Extracts the file extension from the URL. If `check_extension` is `True` and an extension is found, it checks if the inferred MIME type (or the extension itself if MIME type is unknown) is among the `allowed_types`. If no extension is found, it typically allows the URL (assuming it might be an HTML page or similar).
    *   3.4.5. Static Methods:
        *   `_extract_extension(url: str) -> str`: [Cached] Extracts the file extension from a URL path, handling query parameters and fragments.
    *   3.4.6. Class Variables:
        *   `_MIME_MAP (Dict[str, str])`: A class-level dictionary mapping common file extensions to MIME types.

*   **3.5. `DomainFilter`**
    *   Source: `crawl4ai/deep_crawling/filters.py`
    *   3.5.1. Purpose: Filters URLs based on a whitelist of allowed domains or a blacklist of blocked domains. Supports subdomain matching.
    *   3.5.2. Inheritance: `URLFilter`
    *   3.5.3. Initialization (`__init__`)
        *   3.5.3.1. Signature:
            ```python
            def __init__(
                self,
                allowed_domains: Union[str, List[str]] = None,
                blocked_domains: Union[str, List[str]] = None,
            ):
            ```
        *   3.5.3.2. Parameters:
            *   `allowed_domains (Union[str, List[str]]`, default: `None`)`: A single domain string or a list of domain strings. If provided, only URLs whose domain (or a subdomain thereof) is in this list will pass.
            *   `blocked_domains (Union[str, List[str]]`, default: `None`)`: A single domain string or a list of domain strings. URLs whose domain (or a subdomain thereof) is in this list will be rejected.
    *   3.5.4. Key Implemented Methods:
        *   `apply(self, url: str) -> bool`:
            *   Description: Extracts the domain from the URL. First, checks if the domain is in `_blocked_domains` (rejects if true). Then, if `_allowed_domains` is specified, checks if the domain is in that list (accepts if true). If `_allowed_domains` is not specified and the URL was not blocked, it passes.
    *   3.5.5. Static Methods:
        *   `_normalize_domains(domains: Union[str, List[str]]) -> Set[str]`: Converts input domains to a set of lowercase strings.
        *   `_is_subdomain(domain: str, parent_domain: str) -> bool`: Checks if `domain` is a subdomain of (or equal to) `parent_domain`.
        *   `_extract_domain(url: str) -> str`: [Cached] Extracts the domain name from a URL.

*   **3.6. `ContentRelevanceFilter`**
    *   Source: `crawl4ai/deep_crawling/filters.py`
    *   3.6.1. Purpose: Filters URLs by fetching their `<head>` section, extracting text content (title, meta tags), and scoring its relevance against a given query using the BM25 algorithm.
    *   3.6.2. Inheritance: `URLFilter`
    *   3.6.3. Initialization (`__init__`)
        *   3.6.3.1. Signature:
            ```python
            def __init__(
                self,
                query: str,
                threshold: float,
                k1: float = 1.2,
                b: float = 0.75,
                avgdl: int = 1000,
            ):
            ```
        *   3.6.3.2. Parameters:
            *   `query (str)`: The query string to assess relevance against.
            *   `threshold (float)`: The minimum BM25 score required for the URL to be considered relevant and pass the filter.
            *   `k1 (float`, default: `1.2`)`: BM25 k1 parameter (term frequency saturation).
            *   `b (float`, default: `0.75`)`: BM25 b parameter (length normalization).
            *   `avgdl (int`, default: `1000`)`: Assumed average document length for BM25 calculations (typically based on the head content).
    *   3.6.4. Key Implemented Methods:
        *   `async def apply(self, url: str) -> bool`:
            *   Description: Asynchronously fetches the HTML `<head>` content of the URL using `HeadPeeker.peek_html`. Extracts title and meta description/keywords. Calculates the BM25 score of this combined text against the `query`. Returns `True` if the score is >= `threshold`.
    *   3.6.5. Helper Methods:
        *   `_build_document(self, fields: Dict) -> str`: Constructs a weighted document string from title and meta tags.
        *   `_tokenize(self, text: str) -> List[str]`: Simple whitespace tokenizer.
        *   `_bm25(self, document: str) -> float`: Calculates the BM25 score.

*   **3.7. `SEOFilter`**
    *   Source: `crawl4ai/deep_crawling/filters.py`
    *   3.7.1. Purpose: Filters URLs by performing a quantitative SEO quality assessment based on the content of their `<head>` section (e.g., title length, meta description presence, canonical tags, robots meta tags, schema.org markup).
    *   3.7.2. Inheritance: `URLFilter`
    *   3.7.3. Initialization (`__init__`)
        *   3.7.3.1. Signature:
            ```python
            def __init__(
                self,
                threshold: float = 0.65,
                keywords: List[str] = None,
                weights: Dict[str, float] = None,
            ):
            ```
        *   3.7.3.2. Parameters:
            *   `threshold (float`, default: `0.65`)`: The minimum aggregated SEO score (typically 0.0 to 1.0 range, though individual factor weights can exceed 1) required for the URL to pass.
            *   `keywords (List[str]`, default: `None`)`: A list of keywords to check for presence in the title.
            *   `weights (Dict[str, float]`, default: `None`)`: A dictionary to override default weights for various SEO factors (e.g., `{"title_length": 0.2, "canonical": 0.15}`).
    *   3.7.4. Key Implemented Methods:
        *   `async def apply(self, url: str) -> bool`:
            *   Description: Asynchronously fetches the HTML `<head>` content. Calculates scores for individual SEO factors (title length, keyword presence, meta description, canonical tag, robots meta tag, schema.org presence, URL quality). Aggregates these scores using the defined `weights`. Returns `True` if the total score is >= `threshold`.
    *   3.7.5. Helper Methods (Scoring Factors):
        *   `_score_title_length(self, title: str) -> float`
        *   `_score_keyword_presence(self, text: str) -> float`
        *   `_score_meta_description(self, desc: str) -> float`
        *   `_score_canonical(self, canonical: str, original: str) -> float`
        *   `_score_schema_org(self, html: str) -> float`
        *   `_score_url_quality(self, parsed_url) -> float`
    *   3.7.6. Class Variables:
        *   `DEFAULT_WEIGHTS (Dict[str, float])`: Default weights for each SEO factor.

*   **3.8. `FilterStats` Data Class**
    *   Source: `crawl4ai/deep_crawling/filters.py`
    *   3.8.1. Purpose: A data class to track statistics for URL filtering operations, including total URLs processed, passed, and rejected.
    *   3.8.2. Fields:
        *   `_counters (array.array)`: An array of unsigned integers storing counts for `[total, passed, rejected]`.
    *   3.8.3. Properties:
        *   `total_urls (int)`: Returns the total number of URLs processed.
        *   `passed_urls (int)`: Returns the number of URLs that passed the filter.
        *   `rejected_urls (int)`: Returns the number of URLs that were rejected.

## 4. URL Scoring Mechanisms

*   **4.1. `URLScorer` (Abstract Base Class)**
    *   Source: `crawl4ai/deep_crawling/scorers.py`
    *   4.1.1. Purpose: Defines the abstract base class for all URL scorers. Scorers assign a numerical value to URLs, which can be used to prioritize crawling.
    *   4.1.2. Key Abstract Methods:
        *   `_calculate_score(self, url: str) -> float`:
            *   Description: Abstract method to be implemented by subclasses. It takes a URL string and returns a raw numerical score.
    *   4.1.3. Key Concrete Methods:
        *   `score(self, url: str) -> float`:
            *   Description: Calculates the final score for a URL by calling `_calculate_score` and multiplying the result by the scorer's `weight`. It also updates the internal `ScoringStats`.
            *   Returns: `(float)` - The weighted score.
    *   4.1.4. Key Attributes/Properties:
        *   `weight (ctypes.c_float)`: [Read-write] - The weight assigned to this scorer. The raw score calculated by `_calculate_score` will be multiplied by this weight. Default is 1.0. Stored as `ctypes.c_float` for memory efficiency.
        *   `stats (ScoringStats)`: [Read-only] - An instance of `ScoringStats` that tracks statistics for this scorer (number of URLs scored, total score, min/max scores).

*   **4.2. `KeywordRelevanceScorer`**
    *   Source: `crawl4ai/deep_crawling/scorers.py`
    *   4.2.1. Purpose: Scores URLs based on the presence and frequency of specified keywords within the URL string itself.
    *   4.2.2. Inheritance: `URLScorer`
    *   4.2.3. Initialization (`__init__`)
        *   4.2.3.1. Signature:
            ```python
            def __init__(self, keywords: List[str], weight: float = 1.0, case_sensitive: bool = False):
            ```
        *   4.2.3.2. Parameters:
            *   `keywords (List[str])`: A list of keyword strings to search for in the URL.
            *   `weight (float`, default: `1.0`)`: The weight to apply to the calculated score.
            *   `case_sensitive (bool`, default: `False`)`: If `True`, keyword matching is case-sensitive. Otherwise, both the URL and keywords are converted to lowercase for matching.
    *   4.2.4. Key Implemented Methods:
        *   `_calculate_score(self, url: str) -> float`:
            *   Description: Counts how many of the provided `keywords` are present in the `url`. The score is the ratio of matched keywords to the total number of keywords (0.0 to 1.0).
    *   4.2.5. Helper Methods:
        *   `_url_bytes(self, url: str) -> bytes`: [Cached] Converts URL to bytes, lowercasing if not case-sensitive.

*   **4.3. `PathDepthScorer`**
    *   Source: `crawl4ai/deep_crawling/scorers.py`
    *   4.3.1. Purpose: Scores URLs based on their path depth (number of segments in the URL path). It favors URLs closer to an `optimal_depth`.
    *   4.3.2. Inheritance: `URLScorer`
    *   4.3.3. Initialization (`__init__`)
        *   4.3.3.1. Signature:
            ```python
            def __init__(self, optimal_depth: int = 3, weight: float = 1.0):
            ```
        *   4.3.3.2. Parameters:
            *   `optimal_depth (int`, default: `3`)`: The path depth considered ideal. URLs at this depth get the highest score.
            *   `weight (float`, default: `1.0`)`: The weight to apply to the calculated score.
    *   4.3.4. Key Implemented Methods:
        *   `_calculate_score(self, url: str) -> float`:
            *   Description: Calculates the path depth of the URL. The score is `1.0 / (1.0 + abs(depth - optimal_depth))`, meaning URLs at `optimal_depth` score 1.0, and scores decrease as depth deviates. Uses a lookup table for common small differences for speed.
    *   4.3.5. Static Methods:
        *   `_quick_depth(path: str) -> int`: [Cached] Efficiently calculates path depth without full URL parsing.

*   **4.4. `ContentTypeScorer`**
    *   Source: `crawl4ai/deep_crawling/scorers.py`
    *   4.4.1. Purpose: Scores URLs based on their inferred content type, typically derived from the file extension.
    *   4.4.2. Inheritance: `URLScorer`
    *   4.4.3. Initialization (`__init__`)
        *   4.4.3.1. Signature:
            ```python
            def __init__(self, type_weights: Dict[str, float], weight: float = 1.0):
            ```
        *   4.4.3.2. Parameters:
            *   `type_weights (Dict[str, float])`: A dictionary mapping file extensions (e.g., "html", "pdf") or MIME type patterns (e.g., "text/html", "image/") to scores. Patterns ending with '$' are treated as exact extension matches.
            *   `weight (float`, default: `1.0`)`: The weight to apply to the calculated score.
    *   4.4.4. Key Implemented Methods:
        *   `_calculate_score(self, url: str) -> float`:
            *   Description: Extracts the file extension from the URL. Looks up the score in `type_weights` first by exact extension match (if pattern ends with '$'), then by general extension. If no direct match, it might try matching broader MIME type categories if defined in `type_weights`. Returns 0.0 if no match found.
    *   4.4.5. Static Methods:
        *   `_quick_extension(url: str) -> str`: [Cached] Efficiently extracts file extension.

*   **4.5. `FreshnessScorer`**
    *   Source: `crawl4ai/deep_crawling/scorers.py`
    *   4.5.1. Purpose: Scores URLs based on dates found within the URL string, giving higher scores to more recent dates.
    *   4.5.2. Inheritance: `URLScorer`
    *   4.5.3. Initialization (`__init__`)
        *   4.5.3.1. Signature:
            ```python
            def __init__(self, weight: float = 1.0, current_year: int = [datetime.date.today().year]): # Actual default is dynamic
            ```
        *   4.5.3.2. Parameters:
            *   `weight (float`, default: `1.0`)`: The weight to apply to the calculated score.
            *   `current_year (int`, default: `datetime.date.today().year`)`: The reference year to calculate freshness against.
    *   4.5.4. Key Implemented Methods:
        *   `_calculate_score(self, url: str) -> float`:
            *   Description: Uses a regex to find year patterns (YYYY) in the URL. If multiple years are found, it uses the latest valid year. The score is higher for years closer to `current_year`, using a predefined lookup for small differences or a decay function for larger differences. If no year is found, a default score (0.5) is returned.
    *   4.5.5. Helper Methods:
        *   `_extract_year(self, url: str) -> Optional[int]`: [Cached] Extracts the most recent valid year from the URL.

*   **4.6. `DomainAuthorityScorer`**
    *   Source: `crawl4ai/deep_crawling/scorers.py`
    *   4.6.1. Purpose: Scores URLs based on a predefined list of domain authority weights. This allows prioritizing or de-prioritizing URLs from specific domains.
    *   4.6.2. Inheritance: `URLScorer`
    *   4.6.3. Initialization (`__init__`)
        *   4.6.3.1. Signature:
            ```python
            def __init__(
                self,
                domain_weights: Dict[str, float],
                default_weight: float = 0.5,
                weight: float = 1.0,
            ):
            ```
        *   4.6.3.2. Parameters:
            *   `domain_weights (Dict[str, float])`: A dictionary mapping domain names (e.g., "example.com") to their authority scores (typically between 0.0 and 1.0).
            *   `default_weight (float`, default: `0.5`)`: The score to assign to URLs whose domain is not found in `domain_weights`.
            *   `weight (float`, default: `1.0`)`: The overall weight to apply to the calculated score.
    *   4.6.4. Key Implemented Methods:
        *   `_calculate_score(self, url: str) -> float`:
            *   Description: Extracts the domain from the URL. If the domain is in `_domain_weights`, its corresponding score is returned. Otherwise, `_default_weight` is returned. Prioritizes top domains for faster lookup.
    *   4.6.5. Static Methods:
        *   `_extract_domain(url: str) -> str`: [Cached] Efficiently extracts the domain from a URL.

*   **4.7. `CompositeScorer`**
    *   Source: `crawl4ai/deep_crawling/scorers.py`
    *   4.7.1. Purpose: Combines the scores from multiple `URLScorer` instances. Each constituent scorer contributes its weighted score to the final composite score.
    *   4.7.2. Inheritance: `URLScorer`
    *   4.7.3. Initialization (`__init__`)
        *   4.7.3.1. Signature:
            ```python
            def __init__(self, scorers: List[URLScorer], normalize: bool = True):
            ```
        *   4.7.3.2. Parameters:
            *   `scorers (List[URLScorer])`: A list of `URLScorer` instances to be combined.
            *   `normalize (bool`, default: `True`)`: If `True`, the final composite score is normalized by dividing the sum of weighted scores by the number of scorers. This can help keep scores in a more consistent range.
    *   4.7.4. Key Implemented Methods:
        *   `_calculate_score(self, url: str) -> float`:
            *   Description: Iterates through all scorers in its list, calls their `score(url)` method (which applies individual weights), and sums up these scores. If `normalize` is `True`, divides the total sum by the number of scorers.
    *   4.7.5. Key Concrete Methods (overrides `URLScorer.score`):
        *   `score(self, url: str) -> float`:
            *   Description: Calculates the composite score and updates its own `ScoringStats`. Note: The individual scorers' stats are updated when their `score` methods are called internally.

*   **4.8. `ScoringStats` Data Class**
    *   Source: `crawl4ai/deep_crawling/scorers.py`
    *   4.8.1. Purpose: A data class to track statistics for URL scoring operations, including the number of URLs scored, total score, and min/max scores.
    *   4.8.2. Fields:
        *   `_urls_scored (int)`: Count of URLs scored.
        *   `_total_score (float)`: Sum of all scores.
        *   `_min_score (Optional[float])`: Minimum score encountered.
        *   `_max_score (Optional[float])`: Maximum score encountered.
    *   4.8.3. Key Methods:
        *   `update(self, score: float) -> None`: Updates the statistics with a new score.
        *   `get_average(self) -> float`: Calculates and returns the average score.
        *   `get_min(self) -> float`: Lazily initializes and returns the minimum score.
        *   `get_max(self) -> float`: Lazily initializes and returns the maximum score.

## 5. `DeepCrawlDecorator`

*   Source: `crawl4ai/deep_crawling/base_strategy.py`
*   5.1. Purpose: A decorator class that transparently adds deep crawling functionality to the `AsyncWebCrawler.arun` method if a `deep_crawl_strategy` is specified in the `CrawlerRunConfig`.
*   5.2. Initialization (`__init__`)
    *   5.2.1. Signature:
        ```python
        def __init__(self, crawler: AsyncWebCrawler):
        ```
    *   5.2.2. Parameters:
        *   `crawler (AsyncWebCrawler)`: The `AsyncWebCrawler` instance whose `arun` method is to be decorated.
*   5.3. `__call__` Method
    *   5.3.1. Signature:
        ```python
        @wraps(original_arun)
        async def wrapped_arun(url: str, config: CrawlerRunConfig = None, **kwargs):
        ```
    *   5.3.2. Functionality: This method wraps the original `arun` method of the `AsyncWebCrawler`.
        *   It checks if `config` is provided, has a `deep_crawl_strategy` set, and if `DeepCrawlDecorator.deep_crawl_active` context variable is `False` (to prevent recursion).
        *   If these conditions are met:
            *   It sets `DeepCrawlDecorator.deep_crawl_active` to `True`.
            *   It calls the `arun` method of the specified `config.deep_crawl_strategy`.
            *   It handles potential streaming results from the strategy by wrapping them in an async generator.
            *   Finally, it resets `DeepCrawlDecorator.deep_crawl_active` to `False`.
        *   If the conditions are not met, it calls the original `arun` method of the crawler.
*   5.4. Class Variable:
    *   `deep_crawl_active (ContextVar)`:
        *   Purpose: A `contextvars.ContextVar` used as a flag to indicate if a deep crawl is currently in progress for the current asynchronous context. This prevents the decorator from re-triggering deep crawling if the strategy itself calls the crawler's `arun` or `arun_many` methods.
        *   Default Value: `False`.

## 6. `TraversalStats` Data Model

*   Source: `crawl4ai/models.py`
*   6.1. Purpose: A data class for storing and tracking statistics related to a deep crawl traversal.
*   6.2. Fields:
    *   `start_time (datetime)`: The timestamp (Python `datetime` object) when the traversal process began. Default: `datetime.now()`.
    *   `end_time (Optional[datetime])`: The timestamp when the traversal process completed. Default: `None`.
    *   `urls_processed (int)`: The total number of URLs that were successfully fetched and processed. Default: `0`.
    *   `urls_failed (int)`: The total number of URLs that resulted in an error during fetching or processing. Default: `0`.
    *   `urls_skipped (int)`: The total number of URLs that were skipped (e.g., due to filters, already visited, or depth limits). Default: `0`.
    *   `total_depth_reached (int)`: The maximum depth reached from the start URL during the crawl. Default: `0`.
    *   `current_depth (int)`: The current depth level being processed by the crawler (can fluctuate during the crawl, especially for BFS). Default: `0`.

## 7. Configuration for Deep Crawling (`CrawlerRunConfig`)

*   Source: `crawl4ai/async_configs.py`
*   7.1. Purpose: `CrawlerRunConfig` is the primary configuration object passed to `AsyncWebCrawler.arun()` and `AsyncWebCrawler.arun_many()`. It contains various settings that control the behavior of a single crawl run, including those specific to deep crawling.
*   7.2. Relevant Fields:
    *   `deep_crawl_strategy (Optional[DeepCrawlStrategy])`:
        *   Type: `Optional[DeepCrawlStrategy]` (where `DeepCrawlStrategy` is the ABC from `crawl4ai.deep_crawling.base_strategy`)
        *   Default: `None`
        *   Description: Specifies the deep crawling strategy instance (e.g., `BFSDeepCrawlStrategy`, `DFSDeepCrawlStrategy`, `BestFirstCrawlingStrategy`) to be used for the crawl. If `None`, deep crawling is disabled, and only the initial URL(s) will be processed.
    *   *Note: Parameters like `max_depth`, `max_pages`, `filter_chain`, `url_scorer`, `score_threshold`, and `include_external` are not direct attributes of `CrawlerRunConfig` for deep crawling. Instead, they are passed to the constructor of the chosen `DeepCrawlStrategy` instance, which is then assigned to `CrawlerRunConfig.deep_crawl_strategy`.*

## 8. Utility Functions

*   **8.1. `normalize_url_for_deep_crawl(url: str, source_url: str) -> str`**
    *   Source: `crawl4ai/deep_crawling/utils.py` (or `crawl4ai/utils.py` if it's a general utility)
    *   8.1.1. Purpose: Normalizes a URL found during deep crawling. This typically involves resolving relative URLs against the `source_url` to create absolute URLs and removing URL fragments (`#fragment`).
    *   8.1.2. Signature: `def normalize_url_for_deep_crawl(url: str, source_url: str) -> str:`
    *   8.1.3. Parameters:
        *   `url (str)`: The URL string to be normalized.
        *   `source_url (str)`: The URL of the page where the `url` was discovered. This is used as the base for resolving relative paths.
    *   8.1.4. Returns: `(str)` - The normalized, absolute URL without fragments.

*   **8.2. `efficient_normalize_url_for_deep_crawl(url: str, source_url: str) -> str`**
    *   Source: `crawl4ai/deep_crawling/utils.py` (or `crawl4ai/utils.py`)
    *   8.2.1. Purpose: Provides a potentially more performant version of URL normalization specifically for deep crawling scenarios, likely employing optimizations to avoid repeated or complex parsing operations. (Note: Based on the provided code, this appears to be the same as `normalize_url_for_deep_crawl` if only one is present, or it might contain specific internal optimizations not exposed differently at the API level but used by strategies).
    *   8.2.2. Signature: `def efficient_normalize_url_for_deep_crawl(url: str, source_url: str) -> str:`
    *   8.2.3. Parameters:
        *   `url (str)`: The URL string to be normalized.
        *   `source_url (str)`: The URL of the page where the `url` was discovered.
    *   8.2.4. Returns: `(str)` - The normalized, absolute URL, typically without fragments.

## 9. PDF Processing Integration (`crawl4ai.processors.pdf`)
    *   9.1. Overview of PDF processing in Crawl4ai: While not directly part of the `deep_crawling` package, PDF processing components can be used in conjunction if a deep crawl discovers PDF URLs and they need to be processed. The `PDFCrawlerStrategy` can fetch PDFs, and `PDFContentScrapingStrategy` can extract content from them.
    *   **9.2. `PDFCrawlerStrategy`**
        *   Source: `crawl4ai/processors/pdf/__init__.py`
        *   9.2.1. Purpose: An `AsyncCrawlerStrategy` designed to "crawl" PDF files. In practice, this usually means downloading the PDF content. It returns a minimal `AsyncCrawlResponse` that signals to a `ContentScrapingStrategy` (like `PDFContentScrapingStrategy`) that the content is a PDF.
        *   9.2.2. Inheritance: `AsyncCrawlerStrategy`
        *   9.2.3. Initialization (`__init__`)
            *   9.2.3.1. Signature: `def __init__(self, logger: AsyncLogger = None):`
            *   9.2.3.2. Parameters:
                *   `logger (AsyncLogger`, default: `None`)`: An optional logger instance.
        *   9.2.4. Key Methods:
            *   `async def crawl(self, url: str, **kwargs) -> AsyncCrawlResponse`:
                *   Description: For a PDF URL, this method typically signifies that the URL points to a PDF. It constructs an `AsyncCrawlResponse` with a `Content-Type` header of `application/pdf` and a placeholder HTML. The actual PDF processing (downloading and content extraction) is usually handled by a subsequent scraping strategy.
    *   **9.3. `PDFContentScrapingStrategy`**
        *   Source: `crawl4ai/processors/pdf/__init__.py`
        *   9.3.1. Purpose: A `ContentScrapingStrategy` specialized in extracting text, images (optional), and metadata from PDF files. It uses a `PDFProcessorStrategy` (like `NaivePDFProcessorStrategy`) internally.
        *   9.3.2. Inheritance: `ContentScrapingStrategy`
        *   9.3.3. Initialization (`__init__`)
            *   9.3.3.1. Signature:
                ```python
                def __init__(self,
                             save_images_locally: bool = False,
                             extract_images: bool = False,
                             image_save_dir: str = None,
                             batch_size: int = 4,
                             logger: AsyncLogger = None):
                ```
            *   9.3.3.2. Parameters:
                *   `save_images_locally (bool`, default: `False`)`: If `True`, extracted images will be saved to the local disk.
                *   `extract_images (bool`, default: `False`)`: If `True`, attempts to extract images from the PDF.
                *   `image_save_dir (str`, default: `None`)`: The directory where extracted images will be saved if `save_images_locally` is `True`.
                *   `batch_size (int`, default: `4`)`: The number of PDF pages to process in parallel batches (if the underlying processor supports it).
                *   `logger (AsyncLogger`, default: `None`)`: An optional logger instance.
        *   9.3.4. Key Methods:
            *   `scrape(self, url: str, html: str, **params) -> ScrapingResult`:
                *   Description: Takes the URL (which should point to a PDF or a local PDF path) and processes it. It downloads the PDF if it's a remote URL, then uses the internal `pdf_processor` to extract content. It formats the extracted text into basic HTML and collects image and link information.
            *   `async def ascrape(self, url: str, html: str, **kwargs) -> ScrapingResult`:
                *   Description: Asynchronous version of the `scrape` method, typically by running the synchronous `scrape` method in a separate thread.
        *   9.3.5. Helper Methods:
            *   `_get_pdf_path(self, url: str) -> str`: Downloads a PDF from a URL to a temporary file if it's not a local path.
    *   **9.4. `NaivePDFProcessorStrategy`**
        *   Source: `crawl4ai/processors/pdf/processor.py`
        *   9.4.1. Purpose: A concrete implementation of `PDFProcessorStrategy` that uses `PyPDF2` (or similar libraries if extended) to extract text, images, and metadata from PDF documents page by page or in batches.
        *   9.4.2. Initialization (`__init__`)
            *   Signature: `def __init__(self, image_dpi: int = 144, image_quality: int = 85, extract_images: bool = True, save_images_locally: bool = False, image_save_dir: Optional[Path] = None, batch_size: int = 4)`
            *   Parameters: [Details parameters for image extraction quality, saving, and batch processing size.]
        *   9.4.3. Key Methods:
            *   `process(self, pdf_path: Path) -> PDFProcessResult`:
                *   Description: Processes a single PDF file sequentially, page by page. Extracts metadata, text, and optionally images from each page.
            *   `process_batch(self, pdf_path: Path) -> PDFProcessResult`:
                *   Description: Processes a PDF file by dividing its pages into batches and processing these batches in parallel using a thread pool, potentially speeding up extraction for large PDFs.
        *   9.4.4. Helper Methods:
            *   `_process_page(self, page, image_dir: Optional[Path]) -> PDFPage`: Processes a single PDF page object.
            *   `_extract_images(self, page, image_dir: Optional[Path]) -> List[Dict]`: Extracts images from a page.
            *   `_extract_links(self, page) -> List[str]`: Extracts hyperlinks from a page.
            *   `_extract_metadata(self, pdf_path: Path, reader=None) -> PDFMetadata`: Extracts metadata from the PDF.
    *   **9.5. PDF Data Models**
        *   Source: `crawl4ai/processors/pdf/processor.py`
        *   9.5.1. `PDFMetadata`:
            *   Purpose: Stores metadata extracted from a PDF document.
            *   Fields:
                *   `title (Optional[str])`: The title of the PDF.
                *   `author (Optional[str])`: The author(s) of the PDF.
                *   `producer (Optional[str])`: The software used to produce the PDF.
                *   `created (Optional[datetime])`: The creation date of the PDF.
                *   `modified (Optional[datetime])`: The last modification date of the PDF.
                *   `pages (int)`: The total number of pages in the PDF. Default: `0`.
                *   `encrypted (bool)`: `True` if the PDF is encrypted, `False` otherwise. Default: `False`.
                *   `file_size (Optional[int])`: The size of the PDF file in bytes. Default: `None`.
        *   9.5.2. `PDFPage`:
            *   Purpose: Stores content extracted from a single page of a PDF document.
            *   Fields:
                *   `page_number (int)`: The page number (1-indexed).
                *   `raw_text (str)`: The raw text extracted from the page. Default: `""`.
                *   `markdown (str)`: Markdown representation of the page content. Default: `""`.
                *   `html (str)`: Basic HTML representation of the page content. Default: `""`.
                *   `images (List[Dict])`: A list of dictionaries, each representing an extracted image with details like format, path/data, dimensions. Default: `[]`.
                *   `links (List[str])`: A list of hyperlink URLs found on the page. Default: `[]`.
                *   `layout (List[Dict])`: Information about the layout of text elements on the page (e.g., coordinates). Default: `[]`.
        *   9.5.3. `PDFProcessResult`:
            *   Purpose: Encapsulates the results of processing a PDF document.
            *   Fields:
                *   `metadata (PDFMetadata)`: The metadata of the processed PDF.
                *   `pages (List[PDFPage])`: A list of `PDFPage` objects, one for each page processed.
                *   `processing_time (float)`: The time taken to process the PDF, in seconds. Default: `0.0`.
                *   `version (str)`: The version of the PDF processor. Default: `"1.1"`.

## 10. Version Information (`crawl4ai.__version__`)
*   Source: `crawl4ai/__version__.py`
*   10.1. `__version__ (str)`: A string representing the current installed version of the `crawl4ai` library (e.g., "0.6.3").

## 11. Asynchronous Configuration (`crawl4ai.async_configs`)
    *   11.1. Overview: The `crawl4ai.async_configs` module contains configuration classes used throughout the library, including those relevant for network requests like proxies (`ProxyConfig`) and general crawler/browser behavior.
    *   **11.2. `ProxyConfig`**
        *   Source: `crawl4ai/async_configs.py` (and `crawl4ai/proxy_strategy.py`)
        *   11.2.1. Purpose: Represents the configuration for a single proxy server, including its address, port, and optional authentication credentials.
        *   11.2.2. Initialization (`__init__`)
            *   11.2.2.1. Signature:
                ```python
                def __init__(
                    self,
                    server: str,
                    username: Optional[str] = None,
                    password: Optional[str] = None,
                    ip: Optional[str] = None,
                ):
                ```
            *   11.2.2.2. Parameters:
                *   `server (str)`: The proxy server URL (e.g., "http://proxy.example.com:8080", "socks5://proxy.example.com:1080").
                *   `username (Optional[str]`, default: `None`)`: The username for proxy authentication, if required.
                *   `password (Optional[str]`, default: `None`)`: The password for proxy authentication, if required.
                *   `ip (Optional[str]`, default: `None`)`: Optionally, the specific IP address of the proxy server. If not provided, it's inferred from the `server` URL.
        *   11.2.3. Key Static Methods:
            *   `from_string(proxy_str: str) -> ProxyConfig`:
                *   Description: Creates a `ProxyConfig` instance from a string representation. Expected format is "ip:port:username:password" or "ip:port".
                *   Returns: `(ProxyConfig)`
            *   `from_dict(proxy_dict: Dict) -> ProxyConfig`:
                *   Description: Creates a `ProxyConfig` instance from a dictionary.
                *   Returns: `(ProxyConfig)`
            *   `from_env(env_var: str = "PROXIES") -> List[ProxyConfig]`:
                *   Description: Loads a list of proxy configurations from a comma-separated string in an environment variable.
                *   Returns: `(List[ProxyConfig])`
        *   11.2.4. Key Methods:
            *   `to_dict(self) -> Dict`: Converts the `ProxyConfig` instance to a dictionary.
            *   `clone(self, **kwargs) -> ProxyConfig`: Creates a copy of the instance, optionally updating attributes with `kwargs`.

    *   **11.3. `ProxyRotationStrategy` (ABC)**
        *   Source: `crawl4ai/proxy_strategy.py`
        *   11.3.1. Purpose: Abstract base class defining the interface for proxy rotation strategies.
        *   11.3.2. Key Abstract Methods:
            *   `async def get_next_proxy(self) -> Optional[ProxyConfig]`: Asynchronously gets the next `ProxyConfig` from the strategy.
            *   `def add_proxies(self, proxies: List[ProxyConfig])`: Adds a list of `ProxyConfig` objects to the strategy's pool.
    *   **11.4. `RoundRobinProxyStrategy`**
        *   Source: `crawl4ai/proxy_strategy.py`
        *   11.4.1. Purpose: A simple proxy rotation strategy that cycles through a list of proxies in a round-robin fashion.
        *   11.4.2. Inheritance: `ProxyRotationStrategy`
        *   11.4.3. Initialization (`__init__`)
            *   11.4.3.1. Signature: `def __init__(self, proxies: List[ProxyConfig] = None):`
            *   11.4.3.2. Parameters:
                *   `proxies (List[ProxyConfig]`, default: `None`)`: An optional initial list of `ProxyConfig` objects.
        *   11.4.4. Key Implemented Methods:
            *   `add_proxies(self, proxies: List[ProxyConfig])`: Adds new proxies to the internal list and reinitializes the cycle.
            *   `async def get_next_proxy(self) -> Optional[ProxyConfig]`: Returns the next proxy from the cycle. Returns `None` if no proxies are available.

## 12. HTML to Markdown Conversion (`crawl4ai.markdown_generation_strategy`)
    *   12.1. `MarkdownGenerationStrategy` (ABC)
        *   Source: `crawl4ai/markdown_generation_strategy.py`
        *   12.1.1. Purpose: Abstract base class defining the interface for strategies that convert HTML content to Markdown.
        *   12.1.2. Key Abstract Methods:
            *   `generate_markdown(self, input_html: str, base_url: str = "", html2text_options: Optional[Dict[str, Any]] = None, content_filter: Optional[RelevantContentFilter] = None, citations: bool = True, **kwargs) -> MarkdownGenerationResult`:
                *   Description: Abstract method to convert the given `input_html` string into a `MarkdownGenerationResult` object.
                *   Parameters:
                    *   `input_html (str)`: The HTML content to convert.
                    *   `base_url (str`, default: `""`)`: The base URL used for resolving relative links within the HTML.
                    *   `html2text_options (Optional[Dict[str, Any]]`, default: `None`)`: Options to pass to the underlying HTML-to-text conversion library.
                    *   `content_filter (Optional[RelevantContentFilter]`, default: `None`)`: An optional filter to apply to the HTML before Markdown conversion, potentially to extract only relevant parts.
                    *   `citations (bool`, default: `True`)`: If `True`, attempts to convert hyperlinks into Markdown citations with a reference list.
                    *   `**kwargs`: Additional keyword arguments.
                *   Returns: `(MarkdownGenerationResult)`
    *   12.2. `DefaultMarkdownGenerator`
        *   Source: `crawl4ai/markdown_generation_strategy.py`
        *   12.2.1. Purpose: The default implementation of `MarkdownGenerationStrategy`. It uses the `CustomHTML2Text` class (an enhanced `html2text.HTML2Text`) for the primary conversion and can optionally apply a `RelevantContentFilter`.
        *   12.2.2. Inheritance: `MarkdownGenerationStrategy`
        *   12.2.3. Initialization (`__init__`)
            *   12.2.3.1. Signature:
                ```python
                def __init__(
                    self,
                    content_filter: Optional[RelevantContentFilter] = None,
                    options: Optional[Dict[str, Any]] = None,
                    content_source: str = "cleaned_html", # "raw_html", "fit_html"
                ):
                ```
            *   12.2.3.2. Parameters:
                *   `content_filter (Optional[RelevantContentFilter]`, default: `None`)`: An instance of a content filter strategy (e.g., `BM25ContentFilter`, `PruningContentFilter`) to be applied to the `input_html` before Markdown conversion. If `None`, no pre-filtering is done.
                *   `options (Optional[Dict[str, Any]]`, default: `None`)`: A dictionary of options to configure the `CustomHTML2Text` converter (e.g., `{"body_width": 0, "ignore_links": False}`).
                *   `content_source (str`, default: `"cleaned_html"`)`: Specifies which HTML source to use for Markdown generation if multiple are available (e.g., from `CrawlResult`). Options: `"cleaned_html"` (default), `"raw_html"`, `"fit_html"`. This parameter is primarily used when the generator is part of a larger crawling pipeline.
        *   12.2.4. Key Methods:
            *   `generate_markdown(self, input_html: str, base_url: str = "", html2text_options: Optional[Dict[str, Any]] = None, content_filter: Optional[RelevantContentFilter] = None, citations: bool = True, **kwargs) -> MarkdownGenerationResult`:
                *   Description: Converts HTML to Markdown. If a `content_filter` is provided (either at init or as an argument), it's applied first to get "fit_html". Then, `CustomHTML2Text` converts the chosen HTML (input_html or fit_html) to raw Markdown. If `citations` is True, links in the raw Markdown are converted to citation format.
                *   Returns: `(MarkdownGenerationResult)`
            *   `convert_links_to_citations(self, markdown: str, base_url: str = "") -> Tuple[str, str]`:
                *   Description: Parses Markdown text, identifies links, replaces them with citation markers (e.g., `[text]^(1)`), and generates a corresponding list of references.
                *   Returns: `(Tuple[str, str])` - A tuple containing the Markdown with citations and the Markdown string of references.

## 13. Content Filtering (`crawl4ai.content_filter_strategy`)
    *   13.1. `RelevantContentFilter` (ABC)
        *   Source: `crawl4ai/content_filter_strategy.py`
        *   13.1.1. Purpose: Abstract base class for strategies that filter HTML content to extract only the most relevant parts, typically before Markdown conversion or further processing.
        *   13.1.2. Key Abstract Methods:
            *   `filter_content(self, html: str) -> List[str]`:
                *   Description: Abstract method that takes an HTML string and returns a list of strings, where each string is a chunk of HTML deemed relevant.
    *   13.2. `BM25ContentFilter`
        *   Source: `crawl4ai/content_filter_strategy.py`
        *   13.2.1. Purpose: Filters HTML content by extracting text chunks and scoring their relevance to a user query (or an inferred page query) using the BM25 algorithm.
        *   13.2.2. Inheritance: `RelevantContentFilter`
        *   13.2.3. Initialization (`__init__`)
            *   13.2.3.1. Signature:
                ```python
                def __init__(
                    self,
                    user_query: Optional[str] = None,
                    bm25_threshold: float = 1.0,
                    language: str = "english",
                ):
                ```
            *   13.2.3.2. Parameters:
                *   `user_query (Optional[str]`, default: `None`)`: The query to compare content against. If `None`, the filter attempts to extract a query from the page's metadata.
                *   `bm25_threshold (float`, default: `1.0`)`: The minimum BM25 score for a text chunk to be considered relevant.
                *   `language (str`, default: `"english"`)`: The language used for stemming tokens.
        *   13.2.4. Key Implemented Methods:
            *   `filter_content(self, html: str, min_word_threshold: int = None) -> List[str]`: Parses HTML, extracts text chunks (paragraphs, list items, etc.), scores them with BM25 against the query, and returns the HTML of chunks exceeding the threshold.
    *   13.3. `PruningContentFilter`
        *   Source: `crawl4ai/content_filter_strategy.py`
        *   13.3.1. Purpose: Filters HTML content by recursively pruning less relevant parts of the DOM tree based on a composite score (text density, link density, tag weights, etc.).
        *   13.3.2. Inheritance: `RelevantContentFilter`
        *   13.3.3. Initialization (`__init__`)
            *   13.3.3.1. Signature:
                ```python
                def __init__(
                    self,
                    user_query: Optional[str] = None,
                    min_word_threshold: Optional[int] = None,
                    threshold_type: str = "fixed", # or "dynamic"
                    threshold: float = 0.48,
                ):
                ```
            *   13.3.3.2. Parameters:
                *   `user_query (Optional[str]`, default: `None`)`: [Not directly used by pruning logic but inherited].
                *   `min_word_threshold (Optional[int]`, default: `None`)`: Minimum word count for an element to be considered for scoring initially (default behavior might be more nuanced).
                *   `threshold_type (str`, default: `"fixed"`)`: Specifies how the `threshold` is applied. "fixed" uses the direct value. "dynamic" adjusts the threshold based on content characteristics.
                *   `threshold (float`, default: `0.48`)`: The score threshold for pruning. Elements below this score are removed.
        *   13.3.4. Key Implemented Methods:
            *   `filter_content(self, html: str, min_word_threshold: int = None) -> List[str]`: Parses HTML, applies the pruning algorithm to the body, and returns the remaining significant HTML blocks as a list of strings.
    *   13.4. `LLMContentFilter`
        *   Source: `crawl4ai/content_filter_strategy.py`
        *   13.4.1. Purpose: Uses a Large Language Model (LLM) to determine the relevance of HTML content chunks based on a given instruction.
        *   13.4.2. Inheritance: `RelevantContentFilter`
        *   13.4.3. Initialization (`__init__`)
            *   13.4.3.1. Signature:
                ```python
                def __init__(
                    self,
                    llm_config: Optional[LLMConfig] = None,
                    instruction: Optional[str] = None,
                    chunk_token_threshold: int = CHUNK_TOKEN_THRESHOLD, # Default from config
                    overlap_rate: float = OVERLAP_RATE,            # Default from config
                    word_token_rate: float = WORD_TOKEN_RATE,        # Default from config
                    verbose: bool = False,
                    logger: Optional[AsyncLogger] = None,
                    ignore_cache: bool = True
                ):
                ```
            *   13.4.3.2. Parameters:
                *   `llm_config (Optional[LLMConfig])`: Configuration for the LLM (provider, API key, model, etc.).
                *   `instruction (Optional[str])`: The instruction given to the LLM to guide content filtering (e.g., "Extract only the main article content, excluding headers, footers, and ads.").
                *   `chunk_token_threshold (int)`: Maximum number of tokens per chunk sent to the LLM.
                *   `overlap_rate (float)`: Percentage of overlap between consecutive chunks.
                *   `word_token_rate (float)`: Estimated ratio of words to tokens, used for chunking.
                *   `verbose (bool`, default: `False`)`: Enables verbose logging for LLM operations.
                *   `logger (Optional[AsyncLogger]`, default: `None`)`: Custom logger instance.
                *   `ignore_cache (bool`, default: `True`)`: If `True`, bypasses any LLM response caching for this operation.
        *   13.4.4. Key Implemented Methods:
            *   `filter_content(self, html: str, ignore_cache: bool = True) -> List[str]`:
                *   Description: Chunks the input HTML. For each chunk, it sends a request to the configured LLM with the chunk and the `instruction`. The LLM is expected to return the relevant part of the chunk. These relevant parts are then collected and returned.
```

---


## Deep Crawling - Reasoning
Source: crawl4ai_deep_crawling_reasoning_content.llm.md

```markdown
# Detailed Outline for crawl4ai - deep_crawling Component

**Target Document Type:** reasoning
**Target Output Filename Suggestion:** `reasoning_deep_crawling.md`
**Library Version Context:** 0.6.3
**Outline Generation Date:** 2025-05-24
---

## 1. Introduction to Deep Crawling with Crawl4ai

Deep crawling is a fundamental capability for comprehensive web data extraction. This section introduces what deep crawling means in the context of Crawl4ai, why it's essential, and provides an overview of how Crawl4ai empowers you to perform sophisticated, multi-page crawls.

*   **1.1. What is Deep Crawling and Why Do You Need It?**
    *   **Explanation of deep crawling:** Deep crawling, unlike single-page scraping, involves systematically discovering and fetching web pages by following hyperlinks from an initial set of "seed" URLs. It's the process of exploring a website's structure to gather information spread across multiple pages. Crawl4ai's deep crawling component automates this exploration, allowing you to define the boundaries and priorities of your crawl.
    *   **Common scenarios requiring deep crawling:**
        *   **Building a comprehensive site index:** Discovering all pages within a website for search engine indexing or sitemap generation. For example, indexing all articles on a news website or all products on an e-commerce site.
        *   **Scraping data from multiple interconnected pages:** Extracting detailed information that isn't available on a single page, such as product specifications from individual product pages linked from a category page.
        *   **Discovering all content within a specific domain or sub-domain:** Ensuring all relevant content under `blog.example.com` is found and processed.
        *   **SEO analysis and site structure understanding:** Mapping out how pages are linked, identifying orphaned pages, or analyzing internal link distribution.
        *   **Monitoring website changes:** Regularly crawling a site to detect new content, updated pages, or broken links.
    *   **Core problems solved by Crawl4ai's `deep_crawling` component:**
        *   **URL Frontier Management:** Efficiently managing the queue of URLs to visit.
        *   **Visited URL Tracking:** Preventing re-crawling of already processed pages.
        *   **Depth Control:** Limiting how many "hops" the crawler takes from the seed URL.
        *   **Scope Management:** Using filters to define which URLs are relevant and should be processed.
        *   **Crawl Prioritization:** Using scorers to decide which URLs are more important to visit next.
        *   **Resource Management:** Providing mechanisms (`max_pages`) to limit the overall crawl size.

*   **1.2. When to Choose Deep Crawling Over Single-Page Crawling**
    *   **Decision factors:**
        *   **Data Distribution:** If the information you need is spread across multiple interlinked pages (e.g., an e-commerce site with category pages, product listing pages, and individual product detail pages), deep crawling is necessary. Single-page crawling is sufficient if all required data is on one page or a known, small set of URLs.
        *   **Link Discovery:** If you need to discover new URLs dynamically based on the content of previously crawled pages, deep crawling is the way to go.
        *   **Site Mapping/Full Site Analysis:** If your goal is to understand the structure of an entire site or a significant portion of it, deep crawling is essential.
    *   **Trade-offs:**
        *   **Comprehensiveness vs. Speed/Resources:** Deep crawling provides more comprehensive data but typically takes longer and consumes more bandwidth and processing resources than single-page crawls.
        *   **Complexity:** Configuring an effective deep crawl (with appropriate strategies, filters, and scorers) can be more complex than a simple single-page fetch.
        *   **Scope Control:** Without proper filters and limits (`max_depth`, `max_pages`), deep crawls can easily become too broad and inefficient.

*   **1.3. Overview of Crawl4ai's Deep Crawling Architecture**
    *   **High-level explanation:**
        Crawl4ai's deep crawling is orchestrated by a `DeepCrawlStrategy` (like `BFSDeepCrawlStrategy`, `DFSDeepCrawlStrategy`, or `BestFirstCrawlingStrategy`). When a page is crawled, this strategy is responsible for:
        1.  Extracting new links from the page.
        2.  Applying a `FilterChain` (a sequence of `URLFilter` instances) to determine if a discovered URL should be considered for further crawling.
        3.  Optionally, using a `URLScorer` (especially with `BestFirstCrawlingStrategy`) to assign a priority to valid URLs, influencing the order in which they are visited.
        4.  Adding valid (and potentially scored) URLs to a frontier (queue or priority queue) for future processing.
        5.  Managing visited URLs to avoid redundant crawls and controlling the depth and extent of the crawl.
    *   **Role of `DeepCrawlDecorator`:**
        This decorator is an internal mechanism that transparently adds deep crawling capabilities to the standard `AsyncWebCrawler.arun()` method when a `deep_crawl_strategy` is specified in `CrawlerRunConfig`. Users typically don't interact with it directly but should be aware that it's the component enabling this extended functionality.
    *   `* Diagram: [Conceptual diagram of the deep crawling workflow:
        Seed URL -> AsyncWebCrawler.arun() -> DeepCrawlDecorator -> (if deep_crawl_strategy in CrawlerRunConfig) -> DeepCrawlStrategy.arun()
        Within DeepCrawlStrategy.arun():
            Fetch Page -> Process Page (Extract Links) -> For each Link:
                -> FilterChain.apply(link) -> (if valid) -> URLScorer.score(link) -> Add to Frontier -> Select Next URL from Frontier -> Fetch Page ... ]`

## 2. Core Concepts: Strategies, Filters, and Scorers

To effectively use deep crawling, understanding its three main pillars is crucial: the strategy dictates *how* you explore, filters decide *what* to explore, and scorers (especially for Best-First) determine *in what order* to explore.

*   **2.1. Understanding `DeepCrawlStrategy`**
    *   **Purpose:** The `DeepCrawlStrategy` is the heart of the deep crawling process. It's an interface (an abstract base class) that defines the logic for traversing a website. Concrete implementations provide different exploration patterns.
    *   **Why different strategies exist:**
        *   **BFS (Breadth-First Search):** Explores websites level by level. Good for a complete, systematic scan up to a certain depth.
        *   **DFS (Depth-First Search):** Explores one branch of a website as deeply as possible before backtracking. Useful for following specific paths.
        *   **Best-First Search:** Uses a scoring mechanism to prioritize URLs, visiting the most "promising" ones first. Ideal for targeted crawling where relevance is key.
    *   **How to select the right strategy for your goal:**
        *   **Comprehensive Site Mapping:** BFS is often preferred.
        *   **Finding Specific Content Quickly (if path is known or can be guided):** DFS can be efficient.
        *   **Targeted Crawling (e.g., based on keywords, freshness, authority):** Best-First is the most powerful.
        *   **Resource Constraints:** BFS can be memory-intensive for wide sites. DFS might be better for deep, narrow sites if `max_depth` is managed. Best-First's resource usage depends on the scorer and queue size.
        *   `* Decision Table:
            | Goal                          | Recommended Strategy | Key Considerations                      |
            |-------------------------------|----------------------|-----------------------------------------|
            | Full site index up to depth X | BFS                  | Memory for wide sites, `max_depth`      |
            | Explore specific section deep | DFS                  | `max_depth`, avoiding traps             |
            | Find most relevant pages      | Best-First           | Scorer quality, `max_pages`             |
            | Quick overview of a site      | BFS with low `max_depth`| Speed vs. completeness                |
            `
*   **2.2. The Role of URL Filters (`URLFilter` & `FilterChain`)**
    *   **Purpose:** Filters are essential for controlling the scope and efficiency of your deep crawl. They decide whether a discovered URL should be added to the crawling queue or discarded. Without filters, a crawler might wander into irrelevant parts of a website, get stuck in "crawler traps" (like infinite calendars), or consume excessive resources.
    *   **How `FilterChain` allows combining multiple filters:** `FilterChain` takes a list of `URLFilter` instances. When a URL is evaluated, it's passed through each filter in the chain sequentially. If *any* filter in the chain rejects the URL (returns `False`), the URL is discarded. It must pass *all* filters to be considered valid. This allows for creating sophisticated, layered filtering logic.
    *   **Benefits of effective filtering:**
        *   **Efficiency:** Reduces the number of pages fetched and processed, saving time and bandwidth.
        *   **Relevance:** Focuses the crawl on content that matches your objectives.
        *   **Resource Management:** Prevents excessive memory usage by keeping the URL frontier manageable.
        *   **Avoiding Traps:** Helps avoid sections of a website that might lead to an infinite number of unique URLs (e.g., calendars, faceted search results with many parameter combinations).

*   **2.3. The Power of URL Scoring (`URLScorer` & `CompositeScorer`)**
    *   **Purpose:** URL scoring is primarily used by the `BestFirstCrawlingStrategy`. It assigns a numerical score to each valid URL, indicating its priority. The strategy then picks URLs from the frontier based on these scores (typically highest score first). This allows the crawler to intelligently prioritize which parts of a website to explore.
    *   **How `CompositeScorer` enables multi-faceted URL evaluation:** Often, a single criterion isn't enough to determine a URL's importance. `CompositeScorer` allows you to combine multiple individual `URLScorer` instances (e.g., one for keyword relevance, one for freshness, one for domain authority). Each individual scorer contributes to an overall score, often with weights you can define, providing a more nuanced and effective prioritization.
    *   **Impact of scoring on crawl efficiency and result quality:**
        *   **Efficiency:** Good scoring can dramatically improve efficiency by guiding the crawler to relevant content much faster, especially if you have a `max_pages` limit.
        *   **Result Quality:** By prioritizing high-value pages, scoring ensures that the most important data is collected even if the crawl is stopped before exploring the entire site. The definition of "high-value" is determined by your scoring logic.

## 3. Deep Crawling Strategies In-Depth

Let's dive into each specific strategy, understanding its mechanics, use cases, and how to configure it.

*   **3.1. Breadth-First Search (`BFSDeepCrawlStrategy`)**
    *   **3.1.1. Understanding BFS Traversal**
        *   **What is BFS?** Breadth-First Search explores a website layer by layer. It starts with the seed URL(s) (level 0), then crawls all pages directly linked from the seeds (level 1), then all pages linked from level 1 pages (level 2), and so on. It uses a FIFO (First-In, First-Out) queue to manage URLs for each level.
        *   **Pros:**
            *   Finds the shortest path to all reachable pages.
            *   Systematic and predictable exploration pattern.
            *   Good for getting a broad overview of a site's structure quickly, especially at shallow depths.
        *   **Cons:**
            *   Can consume significant memory for websites with a large number of links per page (wide sites), as it needs to store all URLs of a given level before moving to the next.
            *   May take a long time to reach content buried deep within the site structure.
        *   **Typical Use Cases:**
            *   Full site mapping up to a certain depth.
            *   Discovering all pages for a small to medium-sized website.
            *   Finding broken links or orphaned pages (when combined with analysis of all discovered URLs).
        *   `* Diagram: [Visual representation of BFS traversal, showing levels and queue behavior.
            Example:
                Level 0: A
                Queue: [A] -> Process A, discover B, C
                Level 1: B, C
                Queue: [B, C] -> Process B, discover D, E. Process C, discover F
                Level 2: D, E, F
                Queue: [D, E, F] -> ...
            ]`
    *   **3.1.2. Practical Usage of `BFSDeepCrawlStrategy`**
        *   **How to instantiate and pass it to `CrawlerRunConfig`:**
            ```python
            from crawl4ai import BFSDeepCrawlStrategy, CrawlerRunConfig

            bfs_strategy = BFSDeepCrawlStrategy(max_depth=3) # Example: crawl up to 3 levels deep
            run_config = CrawlerRunConfig(deep_crawl_strategy=bfs_strategy)
            # ... then pass run_config to crawler.arun() or crawler.arun_many()
            ```
        *   **Key configuration parameters:**
            *   `max_depth (int)`: Crucial for BFS. Defines how many levels deep the crawl will go. A `max_depth` of 0 crawls only the seed URL(s). A `max_depth` of 1 crawls seeds and pages directly linked from them.
            *   `filter_chain (Optional[FilterChain])`: URLs discovered at each level are passed through this chain before being added to the next level's queue.
            *   `url_scorer (Optional[URLScorer])`: While BFS is primarily level-ordered, a scorer *can* be used to influence the processing order *within* a given level if multiple URLs are fetched concurrently in batches. However, it doesn't change the fundamental level-by-level exploration. (This is less common for pure BFS compared to Best-First).
            *   `max_pages (int, default=infinity)`: A global limit on the total number of pages to crawl. The crawl will stop if `max_pages` or `max_depth` is reached, whichever comes first.
            *   `include_external (bool, default=False)`: If `True`, BFS will also explore links to external domains, respecting `max_depth` for those external paths as well. Use with caution and strong `DomainFilter`s.
        *   `* Code Example: [Setting up a BFS crawl to explore 'example.com' up to depth 2, only HTML pages, max 100 pages]`
            ```python
            from crawl4ai import (
                AsyncWebCrawler, CrawlerRunConfig, BrowserConfig,
                BFSDeepCrawlStrategy, FilterChain, DomainFilter, ContentTypeFilter
            )
            import asyncio

            async def bfs_example_crawl():
                # Filters: Only allow 'example.com' and only HTML files
                filters = FilterChain(filters=[
                    DomainFilter(allowed_domains=["example.com"]),
                    ContentTypeFilter(allowed_types=['.html', '.htm'])
                ])

                # BFS Strategy: Max depth 2, max 100 pages, apply filters
                bfs_strategy = BFSDeepCrawlStrategy(
                    max_depth=2,
                    filter_chain=filters,
                    max_pages=100
                )

                run_config = CrawlerRunConfig(
                    deep_crawl_strategy=bfs_strategy,
                    verbose=True
                )

                browser_config = BrowserConfig(headless=True)
                async with AsyncWebCrawler(config=browser_config) as crawler:
                    result_container = await crawler.arun(
                        url="https://example.com",
                        config=run_config
                    )
                    # In batch mode (default for arun without stream=True in strategy),
                    # result_container will be a list of CrawlResult objects
                    for i, result in enumerate(result_container):
                        if result.success:
                            print(f"Crawled {i+1}: {result.url} (Depth: {result.metadata.get('depth')})")
                        else:
                            print(f"Failed {i+1}: {result.url} - {result.error_message}")

            if __name__ == "__main__":
                asyncio.run(bfs_example_crawl())
            ```
    *   **3.1.3. Best Practices for BFS**
        *   **Memory Management:** For very wide sites (many links per page), BFS can consume a lot of memory because it holds all URLs of the current level. If memory is a concern, consider a lower `max_depth` or switching to DFS/Best-First for more targeted exploration.
        *   **Effective `max_depth`:** Choose `max_depth` carefully. A small increase in depth can lead to an exponential increase in pages crawled.
        *   **Filtering:** Always use `DomainFilter` to keep the crawl focused. Add other filters (`ContentTypeFilter`, `URLPatternFilter`) as needed to refine scope.
    *   **3.1.4. Common Pitfalls with BFS**
        *   **Excessive Memory on Large/Wide Sites:** Setting `max_depth` too high without considering site width can lead to out-of-memory errors.
        *   **Crawling Irrelevant Content:** Not using filters can result in crawling large, unwanted sections of a site or even external sites if `include_external` is accidentally enabled without proper domain filtering.
        *   **Time Consumption:** BFS aims for breadth, so reaching very specific, deep content might take longer than with DFS or a well-tuned Best-First strategy.

*   **3.2. Depth-First Search (`DFSDeepCrawlStrategy`)**
    *   **3.2.1. Understanding DFS Traversal**
        *   **What is DFS?** Depth-First Search explores as far as possible along each branch before backtracking. It uses a LIFO (Last-In, First-Out) stack to manage URLs. When it discovers new links on a page, those links are added to the top of the stack, and the crawler immediately proceeds to the newest link.
        *   **Pros:**
            *   Can reach deep content very quickly if it happens to be on the current exploration path.
            *   Potentially lower memory footprint for deep, narrow sites compared to BFS, as it doesn't need to store all URLs at a given level.
        *   **Cons:**
            *   Can get "stuck" exploring a very deep or infinite branch, potentially missing content in other, shallower branches if `max_pages` or another limit is hit.
            *   The order of discovery is less predictable than BFS and may not provide a balanced view of the site quickly.
        *   **Typical Use Cases:**
            *   Following a specific path through a website (e.g., a series of articles, a product configuration wizard).
            *   Exploring a single section of a website as deeply as possible.
            *   When memory is a primary concern and the target content is known to be deep.
        *   `* Diagram: [Visual representation of DFS traversal, showing stack behavior.
            Example:
                Stack: [A] -> Pop A, discover B, C. Push C, then B.
                Stack: [B, C] -> Pop B, discover D, E. Push E, then D.
                Stack: [D, E, C] -> Pop D ... and so on.
            ]`
    *   **3.2.2. Practical Usage of `DFSDeepCrawlStrategy`**
        *   **How to instantiate and pass it to `CrawlerRunConfig`:**
            ```python
            from crawl4ai import DFSDeepCrawlStrategy, CrawlerRunConfig

            dfs_strategy = DFSDeepCrawlStrategy(max_depth=5) # Example: explore up to 5 links deep
            run_config = CrawlerRunConfig(deep_crawl_strategy=dfs_strategy)
            ```
        *   **Key configuration parameters:**
            *   `max_depth (int)`: Critically important for DFS to prevent infinite loops or excessively deep crawls.
            *   `filter_chain (Optional[FilterChain])`: Essential for guiding DFS and preventing it from exploring irrelevant paths.
            *   `url_scorer (Optional[URLScorer])`: Less commonly used with pure DFS, as the stack naturally dictates order. If used, it might influence which of the newly discovered links from a page gets pushed to the stack (and thus processed) first.
            *   `max_pages (int, default=infinity)`: Stops the crawl if this limit is reached.
            *   `include_external (bool, default=False)`: Controls whether DFS follows external links.
        *   `* Code Example: [Setting up a DFS crawl to explore a blog, prioritizing paths under '/blog/archive/']`
            ```python
            from crawl4ai import (
                AsyncWebCrawler, CrawlerRunConfig, BrowserConfig,
                DFSDeepCrawlStrategy, FilterChain, URLPatternFilter
            )
            import asyncio

            async def dfs_example_crawl():
                # Filter to keep crawl within /blog/ subdirectories
                filters = FilterChain(filters=[
                    URLPatternFilter(patterns=["https://example.com/blog/.*"])
                ])

                dfs_strategy = DFSDeepCrawlStrategy(
                    max_depth=10,      # Allow going reasonably deep
                    filter_chain=filters,
                    max_pages=50       # But limit total pages
                )

                run_config = CrawlerRunConfig(
                    deep_crawl_strategy=dfs_strategy,
                    verbose=True
                )

                browser_config = BrowserConfig(headless=True)
                async with AsyncWebCrawler(config=browser_config) as crawler:
                    # Using stream=True in strategy for immediate results
                    dfs_strategy.stream = True # Overriding here for demo
                    async for result in await crawler.arun(
                        url="https://example.com/blog/",
                        config=run_config
                    ):
                        if result.success:
                            print(f"Crawled (DFS): {result.url} (Depth: {result.metadata.get('depth')})")
                        else:
                            print(f"Failed (DFS): {result.url} - {result.error_message}")

            if __name__ == "__main__":
                asyncio.run(dfs_example_crawl())
            ```
    *   **3.2.3. Best Practices for DFS**
        *   **Mandatory `max_depth`:** Always set a reasonable `max_depth` to prevent the crawler from getting lost in very deep or cyclical paths.
        *   **Targeted Filtering:** Use `URLPatternFilter` or other specific filters to guide the DFS along the paths you're interested in.
        *   **Monitor `max_pages`:** If `max_pages` is hit before `max_depth` in many branches, your DFS might not be exploring the site effectively.
    *   **3.2.4. Common Pitfalls with DFS**
        *   **Crawler Traps:** DFS is particularly susceptible to getting stuck in "crawler traps" (e.g., links generating unique URLs infinitely, like calendars or poorly designed filters).
        *   **Missing Broad Content:** If relevant content is spread across many shallow branches, DFS might miss much of it if it goes deep into one branch and hits `max_pages`.
        *   **Order of Discovery:** The order in which pages are discovered can feel random if the site structure isn't well understood or filters aren't guiding the crawl.

*   **3.3. Best-First Search (`BestFirstCrawlingStrategy`)**
    *   **3.3.1. Understanding Best-First Traversal**
        *   **What is Best-First?** This strategy uses a priority queue to manage the URL frontier. Each URL added to the frontier is assigned a score by a `URLScorer`. The crawler always picks the URL with the highest score from the priority queue to process next.
        *   **Pros:**
            *   Highly efficient for targeted crawling when you can define what makes a URL "good" or "relevant."
            *   Focuses crawler resources on the most promising areas of a website first.
            *   Adaptable: By changing the scoring logic, you can radically alter the crawl's focus.
        *   **Cons:**
            *   Effectiveness is *heavily* dependent on the quality and design of the `URLScorer`. A bad scorer leads to a bad crawl.
            *   Can be more complex to configure due to the need to design and implement scoring logic.
            *   Might miss some relevant content if it consistently scores low and `max_pages` is reached.
        *   **Typical Use Cases:**
            *   Finding pages most relevant to a specific set of keywords.
            *   Prioritizing pages from high-authority domains or known good sources.
            *   Crawling recently updated or fresh content first.
            *   Combining multiple factors (e.g., relevance, freshness, authority) for sophisticated prioritization.
        *   `* Diagram: [Visual representation of Best-First traversal.
            1. Seed URL -> Scorer -> Add to PriorityQueue (URL, Score)
            2. Pop highest score URL from PQ -> Fetch & Process -> Discover Links
            3. For each Link: Filter -> (if valid) -> Scorer -> Add to PQ (Link, Score)
            4. Repeat from step 2.
            Show PQ reordering as new items are added with different scores.]`
    *   **3.3.2. Practical Usage of `BestFirstCrawlingStrategy`**
        *   **How to instantiate and pass it to `CrawlerRunConfig`:**
            ```python
            from crawl4ai import (
                BestFirstCrawlingStrategy, CrawlerRunConfig,
                KeywordRelevanceScorer, DomainFilter, FilterChain
            )

            # Scorer: Prioritize URLs with 'ai' and 'ethics'
            keyword_scorer = KeywordRelevanceScorer(keywords=['ai', 'ethics'], weight=1.0)

            # Filter: Only 'example.com'
            domain_filter = DomainFilter(allowed_domains=['example.com'])
            filter_chain = FilterChain(filters=[domain_filter])

            best_first_strategy = BestFirstCrawlingStrategy(
                url_scorer=keyword_scorer,
                filter_chain=filter_chain,
                max_depth=5,
                max_pages=200
            )
            run_config = CrawlerRunConfig(deep_crawl_strategy=best_first_strategy)
            ```
        *   **Crucial role of `url_scorer`:** This is the defining component. You *must* provide a `URLScorer` instance. This could be a single scorer or a `CompositeScorer`.
        *   **Interaction with `filter_chain`:** Filters are applied *before* scoring. Only URLs that pass all filters are then scored and considered for the priority queue.
        *   **Parameters:**
            *   `max_depth (int)`: Still relevant to prevent excessively deep exploration, even if scores guide the way.
            *   `max_pages (int, default=infinity)`: Important for limiting the overall crawl size.
            *   `include_external (bool, default=False)`: If `True`, external URLs that pass filters will also be scored and added to the queue.
        *   `* Code Example: [Setting up a Best-First crawl using CompositeScorer to find recent articles about "AI in finance" from specific financial news domains]`
            ```python
            from crawl4ai import (
                AsyncWebCrawler, CrawlerRunConfig, BrowserConfig,
                BestFirstCrawlingStrategy, FilterChain, DomainFilter,
                KeywordRelevanceScorer, FreshnessScorer, CompositeScorer
            )
            import asyncio
            from datetime import datetime

            async def best_first_example_crawl():
                # Scorers
                keyword_scorer = KeywordRelevanceScorer(
                    keywords=['ai', 'finance', 'fintech'],
                    weight=0.6
                )
                freshness_scorer = FreshnessScorer(
                    current_year=datetime.now().year,
                    weight=0.4
                )
                composite_scorer = CompositeScorer(
                    scorers=[keyword_scorer, freshness_scorer]
                )

                # Filters
                allowed_domains = ["reputablefinance.news", "fintechinsider.com"]
                filters = FilterChain(filters=[
                    DomainFilter(allowed_domains=allowed_domains)
                ])

                best_first_strategy = BestFirstCrawlingStrategy(
                    url_scorer=composite_scorer,
                    filter_chain=filters,
                    max_depth=4,
                    max_pages=100,
                    stream=True # Get results as they come
                )

                run_config = CrawlerRunConfig(
                    deep_crawl_strategy=best_first_strategy,
                    verbose=True
                )
                browser_config = BrowserConfig(headless=True)

                async with AsyncWebCrawler(config=browser_config) as crawler:
                    start_urls = [f"https://{domain}/" for domain in allowed_domains]
                    async for result in await crawler.arun_many(
                        urls=start_urls,
                        config=run_config
                    ):
                        if result.success:
                            print(f"Crawled (Best-First): {result.url} (Score: {result.metadata.get('score', 'N/A')}, Depth: {result.metadata.get('depth')})")

            if __name__ == "__main__":
                asyncio.run(best_first_example_crawl())
            ```
    *   **3.3.3. Best Practices for Best-First**
        *   **Design Effective Scoring:** The success of Best-First hinges on this. Think carefully about what makes a URL valuable for your goal.
        *   **Iterative Refinement:** Test your scorers. Observe the crawl path. Adjust weights and logic in your `CompositeScorer` or custom scorers based on results.
        *   **Balance Complexity and Performance:** While `CompositeScorer` is powerful, very complex scoring logic involving many external calls or heavy computations per URL can slow down the decision-making process.
        *   **Combine with Strong Filters:** Filters reduce the number of URLs that need to be scored, improving efficiency.
    *   **3.3.4. Common Pitfalls with Best-First**
        *   **Poor Scorer Configuration:** If the scorer doesn't align with your goals, the crawl will be misguided and inefficient (e.g., a keyword scorer with irrelevant keywords).
        *   **Score Normalization (if building custom composite logic):** Ensure scores from different components are on a somewhat comparable scale or that weights account for differences. `CompositeScorer` handles weighting but doesn't inherently normalize scores from sub-scorers.
        *   **Ignoring Potentially Valuable Branches:** If a relevant section of a site consistently scores low due to a quirk in the scoring logic, it might be missed if `max_pages` is too restrictive. Consider periodic "exploration" phases or adjusting scores.
        *   **Over-reliance on a Single Metric:** A `CompositeScorer` is often better than relying on just one type of score (e.g., just keywords) which might be too narrow.

## 4. Fine-Tuning Your Crawl: URL Filtering

Filters are your first line of defense against an unmanageable or irrelevant crawl. They ensure that only URLs meeting your criteria are even considered for fetching and further processing.

*   **4.1. The Importance of Effective Filtering**
    *   **Why filter?**
        *   **Save Resources:** Every skipped URL saves bandwidth, processing time, and memory.
        *   **Improve Speed:** A focused crawl finishes faster.
        *   **Enhance Relevance:** Ensures the data you collect is pertinent to your objectives.
        *   **Avoid Crawler Traps:** Prevents the crawler from getting stuck in infinite loops (e.g., calendars, endlessly paginated archives with slight URL variations).
        *   **Respect Site Policies:** Can be used to avoid crawling sensitive or disallowed sections (though `robots.txt` is the primary mechanism for this).
    *   **How `FilterChain` processes filters sequentially:**
        When you provide a `FilterChain` with multiple filters, a URL must pass *all* of them to be accepted. If `Filter1.apply(url)` returns `False`, the URL is rejected, and `Filter2`, `Filter3`, etc., are not even called for that URL. This "short-circuiting" behavior means you should order your filters strategically.
        `* Diagram: [URL -> Filter1 -> (if True) -> Filter2 -> (if True) -> Filter3 -> (if True) -> Accepted | (if False at any step) -> Rejected]`

*   **4.2. `DomainFilter`**
    *   **4.2.1. Purpose:** The most fundamental filter. It restricts the crawl to specific domains or subdomains (`allowed_domains`) and/or explicitly blocks certain domains (`blocked_domains`).
    *   **4.2.2. How it Works:** It extracts the netloc (e.g., `www.example.com`) from a URL.
        *   If `allowed_domains` is specified, the URL's domain (or a parent domain) must be in this list.
        *   If `blocked_domains` is specified, the URL's domain (or a parent domain) must *not* be in this list.
        *   If both are specified, it must satisfy the allow condition AND not satisfy the block condition.
        *   It handles subdomains correctly: if `example.com` is allowed, `blog.example.com` is also allowed. If `example.com` is blocked, `blog.example.com` is also blocked.
    *   **4.2.3. Configuration & Usage:**
        ```python
        from crawl4ai import DomainFilter, FilterChain

        # Allow only 'example.com' and its subdomains
        allow_example = DomainFilter(allowed_domains=["example.com"])

        # Block 'ads.example.com' and 'tracker.com'
        block_ads = DomainFilter(blocked_domains=["ads.example.com", "tracker.com"])

        # Combine them: only example.com, but not ads.example.com
        combined_filter = FilterChain(filters=[allow_example, block_ads])
        ```
        *   Wildcard usage is not directly supported in the `allowed_domains` list itself (e.g., `*.example.com`). You'd typically allow `example.com` which implicitly covers subdomains. For more complex pattern matching, use `URLPatternFilter`.
        *   `* Code Example: [Allowing 'blog.example.com' and 'docs.example.com', explicitly blocking 'ads.example.com', assuming 'example.com' is the main domain for other content.]`
            ```python
            # This setup means only blog.example.com and docs.example.com are allowed
            # and ads.example.com (if it were a subdomain of an allowed domain) would be blocked.
            # A more typical setup to crawl ONLY these two subdomains:
            specific_subdomains_filter = DomainFilter(allowed_domains=["blog.example.com", "docs.example.com"])

            # If you wanted to crawl example.com but exclude ads.example.com:
            crawl_main_exclude_ads = FilterChain(filters=[
                DomainFilter(allowed_domains=["example.com"]),
                DomainFilter(blocked_domains=["ads.example.com"])
            ])
            ```
    *   **4.2.4. Best Practices:**
        *   Almost always start your `FilterChain` with a `DomainFilter` specifying `allowed_domains` to keep your crawl focused.
        *   Be precise if you only want specific subdomains. Allowing a TLD (Top-Level Domain) like `.com` is generally not what you want unless you intend a very broad crawl.

*   **4.3. `ContentTypeFilter`**
    *   **4.3.1. Purpose:** To filter URLs based on their likely file extension, thereby inferring the content type. This is a quick way to avoid downloading large binary files, images, or unwanted document types.
    *   **4.3.2. How it Works:** By default (`check_extension=True`), it extracts the extension from the URL's path (e.g., `.html`, `.pdf`, `.jpg`). It then checks if this extension (or its corresponding MIME type via an internal map) is present in the `allowed_types`.
    *   **4.3.3. Configuration & Usage:**
        ```python
        from crawl4ai import ContentTypeFilter

        # Only allow HTML and PDF files
        html_pdf_filter = ContentTypeFilter(allowed_types=['.html', '.htm', '.pdf'])
        # OR by MIME type partial match (less common for this filter but possible):
        # html_pdf_filter_mime = ContentTypeFilter(allowed_types=['text/html', 'application/pdf'])
        ```
        *   The `allowed_types` can be a list of extensions (e.g., `'.jpg'`) or partial MIME types (e.g., `'image/'`, `'text/html'`).
        *   `check_extension=True` (default) is generally faster as it avoids needing actual content type from HTTP headers. Set to `False` if you must rely on `Content-Type` headers (note: this filter as shown in `filters.py` primarily works on extensions for pre-fetch filtering).
        *   `* Code Example: [Filtering to only allow HTML, HTM, and PHP files]`
            ```python
            webpage_filter = ContentTypeFilter(allowed_types=['.html', '.htm', '.php'])
            # This would allow URLs like:
            # https://example.com/page.html
            # https://example.com/article.php
            # But would block:
            # https://example.com/image.jpg
            # https://example.com/document.pdf
            ```
    *   **4.3.4. Best Practices:** Use this early in your filter chain to quickly discard URLs pointing to unwanted file types, saving bandwidth and processing for HEAD requests or full fetches that later filters might trigger.

*   **4.4. `URLPatternFilter`**
    *   **4.4.1. Purpose:** Provides fine-grained control over which URLs to process based on matching them against regular expressions or glob-style patterns.
    *   **4.4.2. How it Works:** It takes a list of `patterns`. For each URL, it checks if the URL string matches any of these patterns. The behavior depends on the `reverse` flag.
    *   **4.4.3. Configuration & Usage:**
        ```python
        from crawl4ai import URLPatternFilter

        # Allow only URLs under '/blog/' or '/products/'
        blog_products_filter = URLPatternFilter(
            patterns=[r".*/blog/.*", r".*/products/.*"] # Regex patterns
        )

        # Exclude URLs containing '/archive/' or '/temp/'
        exclude_archive_filter = URLPatternFilter(
            patterns=[r".*/archive/.*", r".*/temp/.*"],
            reverse=True # Exclude if matches
        )
        ```
        *   `patterns`: A list of strings. These are treated as regular expressions by default (Python's `re` module). The provided code shows `fnmatch.translate` is used, which converts glob patterns to regex, but also handles direct regex if it detects regex-specific characters like `^`, `$`, `\d`.
        *   `reverse (bool, default=False)`: If `False` (default), the URL passes if it matches *any* pattern. If `True`, the URL passes only if it matches *none* of the patterns (effectively a blocklist).
        *   `* Code Example: [Allowing only URLs matching '/articles/[year]/[month]/[slug]' and excluding any URL containing '?replytocom=']`
            ```python
            article_path_filter = URLPatternFilter(
                patterns=[r"/articles/\d{4}/\d{2}/[\w-]+/?$"] # Matches /articles/YYYY/MM/slug
            )
            no_reply_filter = URLPatternFilter(
                patterns=[r"\?replytocom="],
                reverse=True # Exclude if it contains replytocom
            )
            # In a FilterChain, these would apply sequentially
            # final_chain = FilterChain(filters=[article_path_filter, no_reply_filter])
            ```
    *   **4.4.4. Best Practices:**
        *   Use for complex URL structures that `DomainFilter` or `ContentTypeFilter` can't handle.
        *   Test your regular expressions thoroughly to ensure they match what you intend and don't have unintended side effects or performance issues. Online regex testers can be very helpful.
        *   Prefer simpler string methods or other filters if a regex is overkill, as regex can be slower.

*   **4.5. `ContentRelevanceFilter` (Requires HEAD requests)**
    *   **4.5.1. Purpose:** To pre-filter URLs based on the relevance of their metadata (title, meta description, keywords found in the `<head>` section) to a given query, using the BM25 ranking algorithm. This helps in prioritizing or including only pages that are likely to be about a specific topic *before* downloading the full content.
    *   **4.5.2. How it Works:**
        1.  Performs an HTTP HEAD request to fetch the headers of the URL.
        2.  If successful, it uses `HeadPeek` to extract text content from `<title>`, `<meta name="description">`, and `<meta name="keywords">` tags from the response (if the HEAD response contains enough of the head, which is not guaranteed and server-dependent. Often, servers don't send body content with HEAD).
        3.  Calculates a BM25 relevance score between the extracted text and the user-provided `query`.
        4.  The URL passes if the score is above the specified `threshold`.
        *   **Important Note on HEAD requests:** While HEAD requests are designed to be lightweight, not all servers implement them correctly or return meaningful content previews in the head. Some servers might return the full HTML head, others might return very little, and some might even block HEAD requests. The effectiveness of this filter depends heavily on server behavior.
    *   **4.5.3. Configuration & Usage:**
        ```python
        from crawl4ai import ContentRelevanceFilter

        relevance_filter = ContentRelevanceFilter(
            query="AI in healthcare applications",
            threshold=0.3,  # Adjust based on desired strictness
            # k1=1.2, b=0.75, avgdl=1000 are BM25 parameters, defaults are usually fine
        )
        ```
        *   `query (str)`: The search query to compare against.
        *   `threshold (float)`: The minimum BM25 score for a URL to pass.
        *   `k1`, `b`, `avgdl`: BM25 algorithm parameters. Defaults are generally reasonable.
        *   `* Code Example: [Filtering for pages potentially relevant to "sustainable energy solutions" with a score threshold of 0.25]`
            ```python
            # Assuming this filter is part of a FilterChain
            sustainable_energy_filter = ContentRelevanceFilter(
                query="sustainable energy solutions impact",
                threshold=0.25
            )
            # This would attempt to fetch HEAD for candidate URLs and check their head content.
            ```
    *   **4.5.4. When to Use:**
        *   When you need a preliminary content relevance check before committing to a full page download and processing.
        *   For highly targeted crawls where topic relevance is paramount.
        *   Be mindful of the performance implications: each HEAD request adds network latency. This filter is best used after broader, faster filters (like `DomainFilter`, `ContentTypeFilter`).
        *   Test with target sites to see if their HEAD responses are useful for this filter.

*   **4.6. `SEOFilter` (Requires HEAD requests)**
    *   **4.6.1. Purpose:** To filter URLs based on a quantitative assessment of their basic on-page SEO quality, derived from elements in the `<head>` section.
    *   **4.6.2. How it Works:**
        1.  Similar to `ContentRelevanceFilter`, it performs an HTTP HEAD request.
        2.  It then uses `HeadPeek` to extract information like title length, presence and length of meta description, canonical URL validity, robots meta tag status (e.g., `noindex`), schema.org markup presence, and general URL "quality" heuristics (length, parameters, underscores).
        3.  Each factor is scored, and a weighted total score is calculated.
        4.  The URL passes if the total score is above the specified `threshold`.
    *   **4.6.3. Configuration & Usage:**
        ```python
        from crawl4ai import SEOFilter

        seo_quality_filter = SEOFilter(
            threshold=0.65,  # Pages must meet at least 65% of SEO quality checks
            keywords=['data science', 'machine learning'], # Optional: boost score if these appear
            # weights: Optional dict to customize scoring of different SEO factors
        )
        ```
        *   `threshold (float)`: Minimum overall SEO score (0.0 to 1.0) for the URL to pass.
        *   `keywords (Optional[List[str]])`: If provided, presence of these keywords in title or meta description can boost the score.
        *   `weights (Optional[Dict[str, float]])`: Allows customization of how much each SEO factor (e.g., `"title_length"`, `"meta_description"`, `"canonical"`) contributes to the total score. See `SEOFilter.DEFAULT_WEIGHTS` for default factors and their weights.
        *   `* Code Example: [Filtering for pages with an SEO score > 0.7, particularly looking for pages optimized for "python programming tutorials"]`
            ```python
            python_tutorial_seo_filter = SEOFilter(
                threshold=0.7,
                keywords=["python programming tutorials", "learn python"]
            )
            # This filter would favor pages that are generally well-optimized for SEO
            # and also contain the specified keywords.
            ```
    *   **4.6.4. When to Use:**
        *   Performing SEO audits to quickly identify pages with potential on-page issues.
        *   Targeting well-optimized pages for content scraping or analysis.
        *   Like `ContentRelevanceFilter`, be aware of HEAD request overhead. Use after faster filters.

*   **4.7. Building Effective `FilterChain`s**
    *   **Order of filters matters:** This is crucial for performance.
        1.  **Fastest, broadest filters first:** Start with `DomainFilter` to immediately exclude irrelevant domains. Follow with `ContentTypeFilter` to discard unwanted file types by extension. `URLPatternFilter` with simple patterns can also be early.
        2.  **More expensive filters later:** Filters requiring network requests (like `ContentRelevanceFilter`, `SEOFilter`) or complex computations should come last, so they only operate on a reduced set of URLs.
    *   **Combining allow and deny logic:** Use multiple `DomainFilter` or `URLPatternFilter` instances (some with `reverse=True`) to create include/exclude rules. For example, allow `example.com` but block `example.com/private/`.
    *   `* Code Example: [A FilterChain demonstrating strategic ordering]`
        ```python
        from crawl4ai import (
            FilterChain, DomainFilter, ContentTypeFilter, URLPatternFilter,
            ContentRelevanceFilter
        )

        # Goal: Crawl blog posts about "AI ethics" on 'myblog.com',
        #       excluding PDFs and archive sections, ensuring basic relevance.

        # 1. Domain Filter: Only 'myblog.com'
        domain_filter = DomainFilter(allowed_domains=["myblog.com"])

        # 2. Content Type Filter: Only HTML
        content_type_filter = ContentTypeFilter(allowed_types=['.html', '.htm'])

        # 3. URL Pattern Filter: Exclude '/archive/'
        archive_exclude_filter = URLPatternFilter(patterns=[r"/archive/"], reverse=True)

        # 4. Content Relevance Filter: Must be somewhat about "AI ethics"
        relevance_filter = ContentRelevanceFilter(query="AI ethics", threshold=0.2)


        effective_chain = FilterChain(filters=[
            domain_filter,
            content_type_filter,
            archive_exclude_filter,
            relevance_filter  # This one makes HEAD requests, so it's last
        ])

        # This chain would be passed to a DeepCrawlStrategy
        # bfs_strategy = BFSDeepCrawlStrategy(max_depth=3, filter_chain=effective_chain)
        ```
        This example shows how to layer filters: first, quickly narrow down by domain and content type, then apply URL pattern rules, and finally, perform the more costly relevance check on the remaining candidates.

## 5. Prioritizing URLs: Scoring Mechanisms

URL scoring is the cornerstone of the `BestFirstCrawlingStrategy`. It allows you to define what "best" means for your crawl, guiding the crawler to explore the most promising URLs first.

*   **5.1. Why Score URLs?**
    *   **Guided Exploration:** Directs the `BestFirstCrawlingStrategy` to URLs that are most likely to contain the information you seek.
    *   **Resource Optimization:** If you have a `max_pages` limit or a time constraint, scoring helps ensure that the most valuable pages are processed before the limit is reached.
    *   **Relevance Ranking:** Allows you to implicitly rank discovered pages by their potential importance or relevance to your task.
    *   **Focus:** Helps concentrate crawling efforts on specific types of content (e.g., fresh news, product pages, high-authority articles).

*   **5.2. `CompositeScorer`: Combining Multiple Signals**
    *   **How it works:** `CompositeScorer` takes a list of individual `URLScorer` instances. For a given URL, it calls the `score()` method of each child scorer. The final score for the URL is typically a weighted sum of the scores from these individual scorers. Each child scorer's raw score is multiplied by its assigned `weight` (which defaults to 1.0 if not specified when adding the scorer to the `CompositeScorer`).
        ```python
        from crawl4ai import CompositeScorer, KeywordRelevanceScorer, PathDepthScorer

        keyword_scorer = KeywordRelevanceScorer(keywords=["news", "update"], weight=0.7)
        path_scorer = PathDepthScorer(optimal_depth=2, weight=0.3)

        # The CompositeScorer will calculate:
        # final_score = (keyword_scorer.score(url) * 0.7) + (path_scorer.score(url) * 0.3)
        # Note: The individual scorers' `weight` parameter is used by CompositeScorer.
        # It is more common to set weights when adding to CompositeScorer if that API exists,
        # or ensure individual scorers output in a way that their inherent weight makes sense.
        # Based on the code: CompositeScorer sums the results of `scorer.score(url)` directly,
        # which already includes the scorer's own weight.
        # So, the weights are applied within each scorer before summing.

        # Corrected understanding based on code:
        # Each scorer's score() method already incorporates its own weight.
        # CompositeScorer simply sums these pre-weighted scores.
        composite_scorer = CompositeScorer(scorers=[keyword_scorer, path_scorer])
        # final_score = keyword_scorer.score(url) + path_scorer.score(url)
        # where keyword_scorer.score(url) is raw_keyword_score * 0.7
        # and path_scorer.score(url) is raw_path_score * 0.3
        ```
    *   **Normalizing scores:** The individual scorers in Crawl4ai are generally designed to output scores that are somewhat normalized (often between 0 and 1 before their internal weight is applied). However, if you create custom scorers with vastly different output ranges, their contribution to the `CompositeScorer` might be skewed. It's good practice to design custom scorers to produce scores in a relatively consistent range (e.g., 0-1) before their `weight` is applied, or adjust their individual `weight`s accordingly to balance their influence in the `CompositeScorer`. The `CompositeScorer` itself simply sums the already weighted scores from its child scorers.
    *   `* Code Example: [Creating a CompositeScorer combining KeywordRelevanceScorer, FreshnessScorer, and DomainAuthorityScorer. Keyword relevance is most important, followed by freshness, then domain authority.]`
        ```python
        from crawl4ai import (
            CompositeScorer, KeywordRelevanceScorer, FreshnessScorer,
            DomainAuthorityScorer
        )
        from datetime import datetime

        # Scorer for keywords, highly weighted
        keyword_scorer = KeywordRelevanceScorer(
            keywords=["financial analysis", "market trends"],
            weight=0.5 # This weight is applied internally by KeywordRelevanceScorer
        )

        # Scorer for freshness, moderately weighted
        freshness_scorer = FreshnessScorer(
            current_year=datetime.now().year,
            weight=0.3
        )

        # Scorer for domain authority, less weighted
        domain_scorer = DomainAuthorityScorer(
            domain_weights={"bloomberg.com": 0.9, "reuters.com": 0.85, "wsj.com": 0.95},
            default_weight=0.5, # For other domains
            weight=0.2
        )

        # CompositeScorer sums the weighted scores from each child scorer
        final_url_scorer = CompositeScorer(
            scorers=[keyword_scorer, freshness_scorer, domain_scorer]
        )
        # Example URL score calculation:
        # url = "https://www.bloomberg.com/news/articles/2023-10-26/ai-impact-on-finance"
        # score_for_url = final_url_scorer.score(url)
        # This would be:
        # (raw_keyword_score_for_url * 0.5) + \
        # (raw_freshness_score_for_url * 0.3) + \
        # (raw_domain_score_for_url * 0.2)
        ```

*   **5.3. `KeywordRelevanceScorer`**
    *   **5.3.1. Purpose:** Scores URLs based on how many of the specified keywords appear within the URL string itself. This is a simple way to prioritize URLs that seem topically relevant by their path or query parameters.
    *   **5.3.2. How it Works:** It iterates through the provided list of `keywords`. For each keyword, it checks if it's present in the URL (case-insensitively by default). The score is typically proportional to the number of matched keywords, normalized by the total number of keywords, and then multiplied by the scorer's `weight`.
    *   **5.3.3. Configuration & Usage:**
        ```python
        from crawl4ai import KeywordRelevanceScorer

        # Prioritize URLs related to Python or Machine Learning
        tech_keyword_scorer = KeywordRelevanceScorer(
            keywords=["python", "machine learning", "pytorch"],
            weight=1.0,       # Overall weight for this scorer
            case_sensitive=False # Default
        )
        ```
        *   `keywords (List[str])`: A list of keywords to search for in the URL.
        *   `weight (float, default=1.0)`: A multiplier applied to the raw score.
        *   `case_sensitive (bool, default=False)`: Whether keyword matching should be case-sensitive.
        *   `* Code Example: [Scoring URLs for a job board, prioritizing those containing "remote", "engineer", or "developer"]`
            ```python
            job_url_scorer = KeywordRelevanceScorer(
                keywords=["remote", "engineer", "developer", "software"],
                weight=1.0
            )
            # Example scores:
            # score1 = job_url_scorer.score("https://jobs.example.com/remote-software-engineer-position") # high score
            # score2 = job_url_scorer.score("https://jobs.example.com/marketing-manager-sf")         # lower score
            ```

*   **5.4. `DomainAuthorityScorer` (Conceptual/External Data)**
    *   **5.4.1. Purpose:** To give preference to URLs from domains that are considered more authoritative or trustworthy. This requires you to provide the authority scores.
    *   **5.4.2. How it Works:** It uses a dictionary (`domain_weights`) that maps domain names (e.g., `"wikipedia.org"`) to numerical authority scores (e.g., `0.9`). When a URL is scored, its domain is extracted. If the domain is in `domain_weights`, its score is used; otherwise, `default_weight` is applied. This raw score is then multiplied by the scorer's overall `weight`.
    *   **5.4.3. Configuration & Usage:**
        ```python
        from crawl4ai import DomainAuthorityScorer

        authority_scorer = DomainAuthorityScorer(
            domain_weights={
                "wikipedia.org": 0.9,
                "scholar.google.com": 0.85,
                "archive.org": 0.7
            },
            default_weight=0.3, # Score for domains not in the list
            weight=1.0          # Overall weight for this scorer's contribution
        )
        ```
        *   `domain_weights (Dict[str, float])`: A dictionary mapping domain strings to their authority scores (typically 0-1).
        *   `default_weight (float, default=0.5)`: Score assigned to URLs from domains not explicitly listed in `domain_weights`.
        *   `weight (float, default=1.0)`: Multiplier for the final score from this scorer.
        *   `* Code Example: [In a news crawl, giving higher scores to URLs from 'bbc.com', 'nytimes.com', and 'reuters.com']`
            ```python
            news_authority_scorer = DomainAuthorityScorer(
                domain_weights={
                    "bbc.com": 0.95,
                    "nytimes.com": 0.9,
                    "reuters.com": 0.88
                },
                default_weight=0.4, # Other news sources
                weight=1.0
            )
            # score_bbc = news_authority_scorer.score("https://www.bbc.com/news/world-europe-12345")
            # score_local_blog = news_authority_scorer.score("https://my-local-blog.com/news-update")
            ```
    *   **5.4.4. Note:** This scorer's effectiveness depends entirely on the quality and relevance of the `domain_weights` you provide. There's no built-in mechanism in Crawl4ai to automatically determine domain authority; you must supply this data.

*   **5.5. `FreshnessScorer`**
    *   **5.5.1. Purpose:** To prioritize URLs that appear to contain more recent content, typically by looking for date patterns (especially years) in the URL string.
    *   **5.5.2. How it Works:** It uses regular expressions to find date patterns (like YYYY/MM/DD, YYYY-MM-DD, YYYY_MM_DD, or just YYYY) in the URL. It extracts the most recent valid year found. The score is then calculated based on the difference between this extracted year and the `current_year` provided during initialization. More recent years get higher scores. The pre-defined `_FRESHNESS_SCORES` list provides a quick lookup for common year differences.
    *   **5.5.3. Configuration & Usage:**
        ```python
        from crawl4ai import FreshnessScorer
        from datetime import datetime

        # Prioritize content from the current year or last few years
        current_year = datetime.now().year
        freshness_scorer = FreshnessScorer(
            current_year=current_year,
            weight=1.0
        )
        ```
        *   `current_year (int)`: The reference year for calculating freshness.
        *   `weight (float, default=1.0)`: Multiplier for the final score.
        *   Date patterns detected: The `_date_pattern` regex in `scorers.py` looks for common date formats.
        *   `* Code Example: [Prioritizing news articles or blog posts from 2023 onwards, assuming current year is 2024]`
            ```python
            # Assuming it's 2024
            recent_content_scorer = FreshnessScorer(current_year=2024, weight=1.0)
            # score_2023 = recent_content_scorer.score("https://example.com/blog/2023/10/my-article") # High score
            # score_2020 = recent_content_scorer.score("https://example.com/archive/2020/old-post") # Lower score
            # score_no_date = recent_content_scorer.score("https://example.com/about-us")      # Default score (0.5)
            ```

*   **5.6. `PathDepthScorer`**
    *   **5.6.1. Purpose:** Scores URLs based on their path depth, which is the number of segments in the URL path (e.g., `/folder1/folder2/page.html` has depth 3). This can be used to prefer shallower pages (often more important) or pages around a specific `optimal_depth`.
    *   **5.6.2. How it Works:** It parses the URL to count the number of segments in its path. The scoring logic (from `_LOOKUP_SCORES`) gives higher scores to depths closer to `optimal_depth`. Depths further away receive progressively lower scores.
    *   **5.6.3. Configuration & Usage:**
        ```python
        from crawl4ai import PathDepthScorer

        # Prefer pages with a path depth of 2 or 3
        path_scorer = PathDepthScorer(
            optimal_depth=2, # Or 3, depending on preference
            weight=1.0
        )
        ```
        *   `optimal_depth (int, default=3)`: The path depth considered most desirable.
        *   `weight (float, default=1.0)`: Multiplier for the final score.
        *   `* Code Example: [Slightly preferring top-level category pages (depth 1) or main articles (depth 2)]`
            ```python
            shallow_page_scorer = PathDepthScorer(optimal_depth=1, weight=1.0)
            # score_depth1 = shallow_page_scorer.score("https://example.com/products/")  # High score (closer to 1.0)
            # score_depth3 = shallow_page_scorer.score("https://example.com/products/category/item") # Lower score
            ```
        The scoring is based on the difference from `optimal_depth`, using `_SCORE_LOOKUP = [1.0, 0.5, 0.333..., 0.25]`. A difference of 0 gets 1.0, 1 gets 0.5, etc.

*   **5.7. `ContentTypeScorer`**
    *   **5.7.1. Purpose:** Assigns scores to URLs based on their inferred content type, primarily determined by the file extension. This allows prioritizing certain types of content (e.g., HTML pages over images or documents).
    *   **5.7.2. How it Works:** It extracts the file extension from the URL. If this extension is found as a key in the `type_weights` dictionary provided during initialization, the corresponding score is used. If the extension is not found, a default score of 0.0 is typically assigned (unless `type_weights` provides a wildcard or default). The raw score is then multiplied by the scorer's `weight`.
    *   **5.7.3. Configuration & Usage:**
        ```python
        from crawl4ai import ContentTypeScorer

        html_priority_scorer = ContentTypeScorer(
            type_weights={
                '.html': 1.0,
                '.htm': 1.0,
                '.pdf': 0.7,
                '.doc': 0.5,
                '.jpg': 0.2,
                '.png': 0.2
            },
            weight=1.0
        )
        ```
        *   `type_weights (Dict[str, float])`: A dictionary mapping file extensions (including the dot, e.g., `'.html'`) to scores.
        *   `weight (float, default=1.0)`: Multiplier for the final score.
        *   `* Code Example: [Prioritizing HTML and PDF documents, while down-weighting images and executables for a document-focused crawl]`
            ```python
            document_content_scorer = ContentTypeScorer(
                type_weights={
                    '.html': 1.0, '.htm': 1.0,
                    '.pdf': 0.9, '.doc': 0.8, '.docx': 0.8,
                    '.txt': 0.7,
                    '.jpg': 0.1, '.png': 0.1, '.gif': 0.1,
                    '.exe': 0.0, '.zip': 0.05
                },
                weight=1.0
            )
            # score_html = document_content_scorer.score("https://example.com/index.html") # High
            # score_zip = document_content_scorer.score("https://example.com/archive.zip") # Low
            ```

*   **5.8. `URLScorer` (Base Class)**
    *   If the built-in scorers or their combination via `CompositeScorer` don't meet your specific needs, you can create a highly custom scorer by inheriting from the `URLScorer` base class.
    *   **Key method to implement:** `_calculate_score(self, url: str) -> float`. This method should take a URL string and return a float representing its score. Remember that the `score(self, url:str)` public method will automatically multiply this by `self._weight`.
    *   Consider the range of scores your custom scorer produces to ensure it integrates well if used within a `CompositeScorer`.

## 6. Configuring and Running Deep Crawls

The `CrawlerRunConfig` object is central to configuring how any specific crawl, including deep crawls, behaves. You'll pass your chosen `DeepCrawlStrategy` (along with its configured filters and scorers) to it.

*   **6.1. The Role of `CrawlerRunConfig`**
    *   The `deep_crawl_strategy` parameter of `CrawlerRunConfig` is how you enable and configure deep crawling for a specific `arun()` or `arun_many()` call.
    *   You instantiate your chosen strategy (e.g., `BFSDeepCrawlStrategy`), configure it with any `FilterChain` and `URLScorer` instances, and then assign this strategy object to `CrawlerRunConfig.deep_crawl_strategy`.
    *   `* Code Example: [Illustrating how strategy is passed to CrawlerRunConfig]`
        ```python
        from crawl4ai import (
            AsyncWebCrawler, CrawlerRunConfig, BFSDeepCrawlStrategy,
            DomainFilter, FilterChain
        )
        import asyncio

        # 1. Define Filters and Scorers (if needed)
        my_filters = FilterChain(filters=[DomainFilter(allowed_domains=["example.com"])])

        # 2. Instantiate and Configure the Strategy
        my_bfs_strategy = BFSDeepCrawlStrategy(max_depth=2, filter_chain=my_filters)

        # 3. Create CrawlerRunConfig and assign the strategy
        my_run_config = CrawlerRunConfig(
            deep_crawl_strategy=my_bfs_strategy,
            # ... other run-specific settings like cache_mode, verbosity, etc.
            cache_mode=CacheMode.BYPASS,
            verbose=True
        )

        async def main():
            async with AsyncWebCrawler() as crawler:
                results = await crawler.arun(url="https://example.com", config=my_run_config)
                for result in results: # arun returns a CrawlResultContainer
                    if result.success:
                        print(f"Crawled: {result.url} - Depth: {result.metadata.get('depth')}")
        # asyncio.run(main())
        ```

*   **6.2. Essential Global Deep Crawl Parameters in `DeepCrawlStrategy` (and reflected in `CrawlerRunConfig`)**
    These parameters are generally set on the strategy object itself.
    *   **`max_depth`:**
        *   **Meaning:**
            *   **BFS:** The maximum number of levels to explore from the seed URL(s). Level 0 is the seed.
            *   **DFS:** The maximum number of links to follow down a single path before backtracking.
            *   **Best-First:** While primarily score-driven, `max_depth` still acts as an upper limit on how deep any single path can go, preventing infinite exploration even if scores are high.
        *   **Strategies for choosing `max_depth`:**
            *   Start small (e.g., 1 or 2) and observe the number of pages found.
            *   Increase incrementally based on your understanding of the target site's structure and your data needs.
            *   For very large sites, a high `max_depth` can lead to an enormous number of URLs.
        *   `* Diagram: [Show two small site graphs. One with max_depth=1, showing only direct links. Another with max_depth=2, showing links of links. Highlight the exponential growth potential.]`
    *   **`max_pages`:**
        *   **How it acts as a global stop condition:** Regardless of `max_depth` or strategy, the crawl will halt once `max_pages` have been successfully processed.
        *   **Interaction with `max_depth` and strategy:**
            *   A crawl might hit `max_pages` before reaching `max_depth` on all branches.
            *   For BFS, this means some deeper levels might not be touched.
            *   For DFS, this means some branches might not be fully explored.
            *   For Best-First, this means lower-scoring URLs might never be visited.
        *   **Use cases:**
            *   Budgeting crawl resources (time, bandwidth, API calls if any).
            *   Getting a quick sample or overview of a site.
            *   Preventing runaway crawls on unexpectedly large sites.
    *   **`include_external` (Context: Typically a parameter on the strategy, e.g., `BestFirstCrawlingStrategy` and other strategies in `crawl4ai`):**
        *   **What it does:** If `True`, the crawler will follow links to domains different from the seed URL's domain.
        *   **When to use it:**
            *   Discovering backlinks or references to your site from external sources (though this is usually done by starting crawls on those external sources).
            *   Exploring a small, trusted ecosystem of related websites.
        *   **Potential pitfalls:**
            *   **Crawl Scope Explosion:** The web is vast. Without *extremely* strict filters (`DomainFilter` for allowed external domains, `URLPatternFilter`), enabling `include_external` can lead to an unmanageably large and irrelevant crawl.
            *   **Resource Drain:** Crawling external sites consumes your resources for potentially off-topic content.
            *   **Best Practice:** Keep `include_external=False` (the default for most strategies) unless you have a very specific reason and robust filters in place. If you need to crawl multiple specific domains, it's often better to run separate, focused crawls for each or use `arun_many` with a list of seed URLs.

*   **6.3. Practical Examples of `CrawlerRunConfig` for Deep Crawls**
    *   `* Code Example: [BFS crawl limited to depth 3 within 'example.com', only HTML pages, verbose logging]`
        ```python
        bfs_strat_example = BFSDeepCrawlStrategy(
            max_depth=3,
            filter_chain=FilterChain(filters=[
                DomainFilter(allowed_domains=["example.com"]),
                ContentTypeFilter(allowed_types=['.html', '.htm'])
            ])
        )
        run_config_bfs = CrawlerRunConfig(
            deep_crawl_strategy=bfs_strat_example,
            verbose=True,
            cache_mode=CacheMode.BYPASS # Forcing fresh crawl for example
        )
        # Usage: await crawler.arun(url="https://example.com", config=run_config_bfs)
        ```
    *   `* Code Example: [DFS crawl following '/blog/' paths, max 50 pages, stream results]`
        ```python
        dfs_strat_example = DFSDeepCrawlStrategy(
            max_depth=10, # DFS can go deep
            max_pages=50,
            filter_chain=FilterChain(filters=[
                URLPatternFilter(patterns=[r"https://example.com/blog/.*"])
            ]),
            stream=True # Yield results as they are found
        )
        run_config_dfs = CrawlerRunConfig(
            deep_crawl_strategy=dfs_strat_example,
            verbose=True
        )
        # Usage:
        # async for result in await crawler.arun(url="https://example.com/blog/", config=run_config_dfs):
        #     # process result
        ```
    *   `* Code Example: [Best-First crawl for "AI ethics" articles, prioritizing recent, high-authority sources, excluding PDFs, max 100 pages]`
        ```python
        from crawl4ai import KeywordRelevanceScorer, FreshnessScorer, DomainAuthorityScorer, CompositeScorer, ContentTypeFilter
        from datetime import datetime

        # Scorers
        keyword_scorer_ai_ethics = KeywordRelevanceScorer(keywords=["AI ethics", "responsible AI"], weight=0.6)
        freshness_scorer_recent = FreshnessScorer(current_year=datetime.now().year, weight=0.25)
        authority_scorer_news = DomainAuthorityScorer(
            domain_weights={"techcrunch.com": 0.8, "wired.com": 0.85},
            default_weight=0.4,
            weight=0.15
        )
        composite_scorer_ai = CompositeScorer(scorers=[
            keyword_scorer_ai_ethics, freshness_scorer_recent, authority_scorer_news
        ])

        # Filters
        filter_chain_ai = FilterChain(filters=[
            DomainFilter(allowed_domains=["techcrunch.com", "wired.com", "another-news-site.com"]),
            ContentTypeFilter(allowed_types=['.html']) # Exclude PDFs by only allowing HTML
        ])

        best_first_strat_ai = BestFirstCrawlingStrategy(
            url_scorer=composite_scorer_ai,
            filter_chain=filter_chain_ai,
            max_pages=100,
            max_depth=5 # Still good to have a depth limit
        )
        run_config_best_first_ai = CrawlerRunConfig(
            deep_crawl_strategy=best_first_strat_ai,
            verbose=True
        )
        # Usage: await crawler.arun(url="https://techcrunch.com", config=run_config_best_first_ai)
        ```

*   **6.4. Integrating with `AsyncWebCrawler.arun()` and `AsyncWebCrawler.arun_many()`**
    *   **`arun(url="seed_url", config=run_config_with_deep_crawl)`:**
        When you call `arun` with a `CrawlerRunConfig` that includes a `deep_crawl_strategy`, the deep crawling process starts from the single `seed_url`. The `DeepCrawlDecorator` intercepts this and delegates to your strategy.
    *   **`arun_many(urls=["seed1", "seed2"], config=run_config_with_deep_crawl)`:**
        If you use `arun_many`, Crawl4ai will typically initiate *independent* deep crawls starting from each seed URL in the `urls` list. Each deep crawl will adhere to the `max_depth`, `max_pages`, filters, and scorers defined in the *same* `run_config_with_deep_crawl`.
        *   **Important Consideration:** The `max_pages` limit in this scenario usually applies *per seed URL's deep crawl task* if the dispatcher handles them as separate tasks. If you need a global `max_pages` across all seed URLs in an `arun_many` call, that would require a more custom dispatcher or a wrapper around `arun_many` to track the total pages. The default behavior is often per-task limits.
        *   If `stream=True` is set on the strategy (or within the `run_config`), results from the different deep crawls initiated by `arun_many` will be yielded as they become available.

## 7. Understanding the `DeepCrawlDecorator`

While you primarily interact with deep crawling through `CrawlerRunConfig` and strategy objects, it's helpful to have a conceptual understanding of the `DeepCrawlDecorator`.

*   **7.1. How Deep Crawling is Activated (Conceptual)**
    *   **Role of `DeepCrawlDecorator`:** The `DeepCrawlDecorator` is a Python decorator that wraps the `AsyncWebCrawler.arun()` method. When `arun()` is called, the decorator checks if the `config` argument (an instance of `CrawlerRunConfig`) has a `deep_crawl_strategy` defined.
    *   If a `deep_crawl_strategy` is present *and* deep crawling is not already active (see `deep_crawl_active` below), the decorator intercepts the call. Instead of the standard single-page crawl, it invokes the `arun()` method of your specified `deep_crawl_strategy` instance, passing along the crawler, seed URL, and configuration.
    *   If no `deep_crawl_strategy` is set, or if deep crawling is already active, the decorator allows the original `arun()` method (for single-page crawling) to proceed.
    *   **The `deep_crawl_active` `ContextVar`:** This is a context variable (from Python's `contextvars` module). The decorator sets `deep_crawl_active` to `True` before calling the strategy's `arun()` method and resets it to `False` afterwards.
        *   **Purpose:** Its primary function is to prevent accidental recursive deep crawls. If your `deep_crawl_strategy` internally calls `crawler.arun()` (which it typically does to fetch individual pages), this flag ensures that those internal calls perform standard single-page fetches and don't try to initiate another layer of deep crawling using the same strategy.

*   **7.2. What This Means for You as a User**
    *   **Transparency:** For most use cases, the `DeepCrawlDecorator` operates transparently. You don't need to instantiate or call it directly.
    *   **Centralized Configuration:** You enable and configure deep crawling by setting the `deep_crawl_strategy` attribute in your `CrawlerRunConfig` object. This is the main entry point.
    *   **Awareness for Advanced Scenarios:** Understanding its existence is useful if:
        *   You are debugging complex deep crawling behavior and want to trace the execution flow.
        *   You are developing very custom strategies that might need to interact with or be aware of this context.

## 8. Monitoring and Analyzing Deep Crawl Performance

Understanding what happened during your deep crawl is key to optimizing it.

*   **8.1. Using `TraversalStats`**
    *   **Accessing `TraversalStats`:**
        *   The `BFSDeepCrawlStrategy`, `DFSDeepCrawlStrategy`, and `BestFirstCrawlingStrategy` (and their base class `DeepCrawlStrategy`) maintain a `self.stats` attribute of type `TraversalStats`.
        *   After a crawl initiated by one of these strategies completes (or even during, if you have access to the strategy instance), you can inspect `strategy_instance.stats`.
        *   The strategies also log these stats upon completion (or at intervals if verbose logging is high).
    *   **Key Metrics and Their Meaning (from `TraversalStats` in `crawl4ai/deep_crawling/base_strategy.py` and related files):**
        *   `start_time`, `end_time`: The overall start and end Python `datetime` objects for the crawl.
        *   `urls_processed` (often part of `FilterStats` within a strategy or a general counter): Total number of unique URLs that were actually fetched and processed by the crawler.
        *   `urls_failed`: Count of URLs that resulted in an error during fetching or processing.
        *   `urls_skipped`: Count of URLs that were discovered but discarded by the `FilterChain` or other conditions (e.g., already visited, exceeded `max_depth`).
        *   `total_depth_reached`: The maximum depth level explored during the crawl.
        *   `current_depth` (Relevant for strategies like BFS/DFS): The current depth level being explored.
        *   Individual filters (`URLFilter` subclasses) also have their own `FilterStats` (`filter.stats`) which track `total_urls` processed by that filter, `passed_urls`, and `rejected_urls`. This is very useful for seeing which filter is having the most impact.
    *   **Interpreting Stats for Optimization:**
        *   **High `urls_skipped` (overall) or high `rejected_urls` (for a specific filter):** This indicates your filters are very active. Review them to ensure they aren't too restrictive or are correctly configured.
        *   **`total_depth_reached` < `max_depth` (when `max_pages` is high):** This could mean the crawl exhausted all discoverable links within the filter scope before reaching the maximum depth, or filters are preventing deeper exploration.
        *   **Crawl finishes too quickly and `urls_processed` is low:** Check seed URLs, initial filters, and `max_depth/max_pages` limits.
        *   **Crawl takes too long:**
            *   Are filters too loose?
            *   Is `max_depth` or `max_pages` too high for the site?
            *   Are expensive filters/scorers (requiring network or heavy computation) being used excessively?
    *   `* Code Example: [Accessing stats after a BFS crawl (assuming batch mode)]`
        ```python
        # ... (setup bfs_strategy and run_config as in previous examples) ...
        # async with AsyncWebCrawler(config=browser_config) as crawler:
        #     results_container = await crawler.arun(
        #         url="https://example.com",
        #         config=run_config_with_bfs_strategy # run_config_with_bfs_strategy.deep_crawl_strategy is bfs_strategy
        #     )
        #
        #     # Access stats from the strategy instance
        #     crawl_stats = run_config_with_bfs_strategy.deep_crawl_strategy.stats
        #     print(f"\n--- Crawl Statistics ---")
        #     print(f"Start Time: {crawl_stats.start_time}")
        #     # Note: urls_processed might be better tracked by summing successful results
        #     # or by inspecting filter stats on the FilterChain if available.
        #     # TraversalStats itself in the provided code doesn't explicitly have urls_processed.
        #     # Let's assume we count successful results:
        #     print(f"URLs Successfully Processed: {len([r for r in results_container if r.success])}")
        #     print(f"URLs Failed: {len([r for r in results_container if not r.success])}") # Approximation
        #     print(f"URLs Skipped by Filters (Example): {getattr(run_config_with_bfs_strategy.deep_crawl_strategy.filter_chain, 'stats', FilterStats()).rejected_urls if run_config_with_bfs_strategy.deep_crawl_strategy.filter_chain else 'N/A'}")
        #     print(f"Max Depth Reached: {crawl_stats.total_depth_reached}")
        #     if crawl_stats.end_time:
        #          print(f"End Time: {crawl_stats.end_time}")
        #          print(f"Duration: {crawl_stats.end_time - crawl_stats.start_time}")
        #     else:
        #          print("Crawl may still be in progress or ended prematurely.")

        # Note: The TraversalStats model has start_time, end_time, urls_processed, urls_failed, urls_skipped, total_depth_reached, current_depth
        # The strategy code increments self._pages_crawled for successful crawls towards max_pages
        # and self.stats.urls_skipped for links skipped by filters.
        ```

*   **8.2. Logging in Deep Crawls**
    *   Crawl4ai's `AsyncLogger` (if `verbose=True` in `CrawlerRunConfig` or `BrowserConfig`) provides valuable insights. During deep crawls, you'll see:
        *   URLs being added to the frontier/queue/stack.
        *   Actions taken by filters (e.g., "URL rejected by DomainFilter").
        *   Scores assigned to URLs if using `BestFirstCrawlingStrategy` with a `URLScorer`.
        *   Newly discovered links from each page.
        *   Errors encountered during fetching or processing.
    *   **Setting verbosity:**
        *   `verbose=True` (default) provides a good level of detail.
        *   For extremely detailed debugging, you might need to delve into the library's source or add custom logging within custom components.
        *   The logger uses tags (e.g., `[BFS]`, `[FILTER]`, `[SCORE]`) to help identify the source of log messages.

## 9. Best Practices for Effective Deep Crawling

Crafting an effective deep crawl involves more than just setting a strategy; it requires planning, careful configuration, and ethical considerations.

*   **9.1. Planning Your Crawl Strategy**
    *   **Define Clear Objectives:**
        *   What specific data are you trying to collect? (e.g., all product names and prices, all blog post titles and content, site structure).
        *   What is the precise scope of your crawl? (e.g., a single subdomain, specific path patterns, content related to certain keywords).
        *   Clearly defined objectives will guide your choice of strategy, filters, and scorers.
    *   **Analyze Target Website Structure:**
        *   **`robots.txt`:** Always check `robots.txt` first (e.g., `https://example.com/robots.txt`) to understand disallowed paths. Crawl4ai can do this automatically if `check_robots_txt=True` (in `CrawlerRunConfig`).
        *   **Sitemaps:** Look for XML sitemaps (`/sitemap.xml`). They often provide a good list of canonical URLs and can be a great source of seed URLs.
        *   **URL Patterns:** Observe common URL structures for different content types (e.g., `/blog/YYYY/MM/DD/slug`, `/product/category/item-id`). This helps in crafting `URLPatternFilter`s.
        *   **Navigation:** Understand how users (and thus crawlers) navigate the site. Are there clear menus, breadcrumbs, pagination?
    *   **Estimate Crawl Size:**
        *   Before launching a full deep crawl, try a very limited crawl (e.g., `max_depth=1` or `2`, `max_pages=50`) to get a sense of how many URLs are discovered per level.
        *   This helps in estimating resource consumption and setting realistic `max_depth` and `max_pages` for the full crawl.

*   **9.2. Configuring for Efficiency and Relevance**
    *   **Filter Aggressively, Then Loosen:**
        *   Start with a restrictive `DomainFilter` to stay within your target domain(s).
        *   Add `ContentTypeFilter` early to exclude unwanted file types.
        *   Use `URLPatternFilter` to include/exclude specific paths.
        *   Only if necessary, add more computationally expensive filters like `ContentRelevanceFilter` or `SEOFilter` later in the chain.
    *   **Control Scope with `max_depth` and `max_pages`:**
        *   These are your primary safety nets against runaway crawls.
        *   Set them based on your objectives and initial site analysis.
    *   **Choose the Right Strategy:**
        *   BFS for broad, systematic coverage of shallow sites.
        *   DFS for deep dives into specific paths (with careful depth control).
        *   Best-First for targeted crawling based on relevance, freshness, or other criteria (requires good scorer design).
    *   **Iterate on Scorer Design (for Best-First):**
        *   Start with simple scorers.
        *   Run test crawls, analyze the types of URLs being prioritized, and refine your scorer weights or logic.
        *   Use `CompositeScorer` to combine multiple weak signals into a stronger one.
    *   **Test Filter and Scorer Logic:**
        *   Before a large crawl, test your `FilterChain` and `URLScorer` logic with a small, representative set of sample URLs to ensure they behave as expected.

*   **9.3. Ethical and Respectful Crawling**
    *   **Respect `robots.txt`:** Set `check_robots_txt=True` in your `CrawlerRunConfig`. Crawl4ai's `RobotsParser` will then automatically check `robots.txt` for each domain.
    *   **Implement Politeness Delays:**
        *   Crawl4ai's default dispatcher strategies often include rate limiting and backoff mechanisms.
        *   You can configure `mean_delay` and `max_range` in `CrawlerRunConfig` if using `arun_many` to introduce delays between individual `arun` calls managed by the dispatcher.
        *   Avoid hitting any single server too frequently.
    *   **Identify and Handle Sensitive Data Responsibly:** If your crawl might encounter personally identifiable information (PII) or other sensitive data, ensure you have mechanisms to either avoid it (via filters) or handle it according to privacy regulations and ethical guidelines.
    *   **Avoid Overwhelming Servers:** Monitor your crawl's impact. If you notice server errors (5xx status codes) or very slow responses, reduce your crawl rate or concurrency.
    *   **User-Agent:** While Crawl4ai provides a default User-Agent, consider setting a custom one that identifies your bot and provides a way to contact you if site administrators have questions (e.g., `MyCoolBot/1.0 (+http://mybotinfo.example.com)`).

*   **9.4. Handling Common Challenges**
    *   **Crawler Traps:**
        *   **Solution:** Use a sensible `max_depth`. Employ `URLPatternFilter` to exclude patterns that lead to traps (e.g., infinitely deep calendar links, search facets creating unique URLs for every combination).
        *   **Example:** A filter like `URLPatternFilter(patterns=[r"/calendar/\d{4}/\d{2}/\d{2}/.*"], reverse=True)` could block deep calendar paths.
    *   **Session-Dependent Content:**
        *   **Challenge:** If deep pages require a login session established on an earlier page.
        *   **Solution:** Use Crawl4ai's session persistence features.
            1.  Perform an initial crawl/interaction to log in, then save the `storage_state` from the browser context.
            2.  For subsequent deep crawls, load this `storage_state` into `BrowserConfig` or `CrawlerRunConfig` to reuse the session.
            *   Refer to the [Session Management](./session-management.md) and [Advanced Features](./advanced-features.md#5-session-persistence--local-storage) guides for details.
    *   **AJAX-Loaded Content / JavaScript-Generated Links:**
        *   **Challenge:** If links are not present in the initial HTML but are loaded or generated by JavaScript.
        *   **Solution:** Ensure your base crawling strategy (e.g., `AsyncPlaywrightCrawlerStrategy`, which is default) is used, as it executes JavaScript. You might need to use `wait_for` in `CrawlerRunConfig` to allow time for JS to execute and render links before link discovery happens.
    *   **Large-Scale Crawls:**
        *   **Challenge:** In-memory `visited` sets and URL frontiers can become bottlenecks for very large crawls (millions of pages).
        *   **Solution:** For enterprise-scale crawls, consider:
            *   Distributed crawling architecture (breaking the crawl into smaller, manageable parts).
            *   Using a persistent, disk-based queue (e.g., Redis, RabbitMQ) for the URL frontier.
            *   Using a distributed database or bloom filter service for the `visited` set.
            *   Crawl4ai's current built-in strategies are primarily designed for single-instance operation with in-memory tracking.

## 10. Troubleshooting Common Deep Crawling Issues

Even with careful planning, deep crawls can sometimes behave unexpectedly. Here's how to diagnose common problems.

*   **10.1. Crawl Not Going Deep Enough / Stopping Prematurely**
    *   **Check `max_depth` and `max_pages`:** Are these limits set too low for your target?
        *   `print(f"Config: max_depth={my_strategy.max_depth}, max_pages={my_strategy.max_pages}")`
    *   **Inspect Filters:**
        *   Are your filters (`DomainFilter`, `URLPatternFilter`, `ContentTypeFilter`, etc.) too aggressive?
        *   **Debugging Tip:** Temporarily disable filters one by one or simplify them to see if they are the cause. Add logging within custom filters or enable verbose logging in Crawl4ai to see filter decisions.
        *   `* Code Example: [Temporarily disabling a filter chain for debugging]`
            ```python
            # original_filter_chain = my_strategy.filter_chain
            # my_strategy.filter_chain = None # Temporarily disable for a test run
            # ... run crawl ...
            # my_strategy.filter_chain = original_filter_chain # Restore
            ```
    *   **Examine Link Discovery:**
        *   Are links being correctly extracted from the initial pages? After an `arun` call on a seed URL, inspect `result.links`.
        *   If links are JavaScript-generated, ensure your `wait_for` or other JS execution settings are adequate.
    *   **`include_external` Behavior:** If you *intend* to crawl subdomains but they are treated as external (and `include_external=False`), they won't be followed. Ensure your `DomainFilter` correctly specifies all allowed (sub)domains if `include_external` is `False`.

*   **10.2. Crawling Too Many Irrelevant Pages**
    *   **Tighten Filters:** This is the most common solution.
        *   Make `DomainFilter` more specific.
        *   Refine `URLPatternFilter` to exclude unwanted paths.
        *   Add a `ContentTypeFilter` if you're getting many non-HTML pages.
    *   **Refine Scoring (for Best-First):** If using `BestFirstCrawlingStrategy`, improve your `URLScorer` to give lower scores to irrelevant URL patterns or domains.
    *   **Verify `include_external`:** Ensure it's `False` (default for most strategies) if you don't intend to leave your primary domain(s).

*   **10.3. Performance Bottlenecks**
    *   **Network-Intensive Filters:** `ContentRelevanceFilter` and `SEOFilter` make HEAD requests for each URL they evaluate. If your `FilterChain` applies these to many URLs, it will significantly slow down the link processing phase.
        *   **Solution:** Place these filters *after* faster filters that can eliminate many URLs without network calls.
    *   **Complex Regex:** Very complex or inefficient regular expressions in `URLPatternFilter` can be slow.
        *   **Solution:** Simplify regex where possible. Test regex performance independently.
    *   **Complex Scoring Logic:** If your custom `URLScorer` or `CompositeScorer` performs heavy computations or external API calls for every URL, it will become a bottleneck for `BestFirstCrawlingStrategy`.
        *   **Solution:** Optimize scorer logic. Cache external API results if possible.
    *   **BFS Memory Usage:** As mentioned, BFS on very wide sites can lead to high memory usage.
        *   **Solution:** Limit `max_depth`, or consider DFS/Best-First for parts of the crawl.
    *   **Logging Overhead:** Extremely verbose logging to the console or file can add overhead. Reduce verbosity for production runs once debugging is complete.

*   **10.4. Using Logs for Diagnosis**
    *   **Enable Verbose Logging:** Set `verbose=True` in `CrawlerRunConfig` and/or `BrowserConfig`.
    *   **Look for Key Log Messages:**
        *   Messages indicating a URL is being added to the queue/stack.
        *   Filter actions: "URL [url] rejected by [FilterName]" or "URL [url] passed [FilterName]".
        *   Scorer outputs: "URL [url] scored [score] by [ScorerName]" (if scorers log verbosely).
        *   Link discovery: "Discovered [N] links on [page_url]".
        *   Errors: Any exceptions or error messages during fetching, processing, or strategy execution.
    *   **Custom Logging:** Add `self.logger.debug(...)` or `self.logger.info(...)` statements within your custom filters or scorers to trace their specific behavior.

## 11. Advanced Deep Crawling: Customization and Integration

When built-in components aren't enough, Crawl4ai's modular design allows you to create custom strategies, filters, and scorers.

*   **11.1. Why and When to Create Custom Components**
    *   **Unique Filtering Logic:** You might need to filter URLs based on criteria not covered by existing filters, e.g., checking against a dynamic blocklist from an API, or complex domain-specific path rules.
    *   **Domain-Specific Scoring Heuristics:** Your definition of a "valuable" URL might involve proprietary business logic, data from your own databases, or very specific content cues that standard scorers don't address.
    *   **Novel Traversal Strategies:** While BFS, DFS, and Best-First cover many cases, you might envision a hybrid approach or a strategy tailored to a very unusual site structure.
    *   **Integration with External Systems:** Custom components can interact with external APIs or databases during the filtering or scoring process.

*   **11.2. Implementing a Custom `DeepCrawlStrategy`**
    *   **Key methods to override (from `DeepCrawlStrategy` base class):**
        *   `async def _arun_batch(self, start_url: str, crawler: AsyncWebCrawler, config: CrawlerRunConfig) -> List[CrawlResult]:` Implement the core non-streaming traversal logic here. You'll manage the frontier (queue/stack/priority queue), call `crawler.arun_many()` for batches of URLs, and use `self.link_discovery()` and `self.can_process_url()`.
        *   `async def _arun_stream(self, start_url: str, crawler: AsyncWebCrawler, config: CrawlerRunConfig) -> AsyncGenerator[CrawlResult, None]:` Implement the streaming version of the traversal logic. Yield `CrawlResult` objects as they become available.
        *   `async def can_process_url(self, url: str, depth: int) -> bool:` (Often relies on `self.filter_chain`) This method determines if a given URL should be processed based on depth and filters.
        *   `async def link_discovery(self, result: CrawlResult, source_url: str, current_depth: int, visited: Set[str], next_level: List[Tuple], depths: Dict[str, int]) -> None:` (Helper method) Extracts links from a `CrawlResult`, applies `can_process_url`, and adds valid URLs to `next_level` and updates `depths`.
    *   **Managing state:** Your custom strategy will need to manage:
        *   A **frontier** of URLs to visit (e.g., `asyncio.Queue` for BFS, `list` for DFS stack, `asyncio.PriorityQueue` for Best-First).
        *   A **visited set** to avoid re-processing URLs.
        *   **URL depths** to respect `max_depth`.
        *   Counters for `max_pages`.
    *   `* Code Example: [Skeleton for a custom strategy that crawls based on page title length (longer titles first - a simplistic example)]`
        ```python
        from crawl4ai import DeepCrawlStrategy, CrawlResult, AsyncWebCrawler, CrawlerRunConfig
        import asyncio
        from typing import List, AsyncGenerator, Set, Dict, Tuple

        class TitleLengthPriorityStrategy(DeepCrawlStrategy):
            def __init__(self, max_depth: int = 3, max_pages: int = 100, **kwargs):
                super().__init__(max_depth=max_depth, max_pages=max_pages, **kwargs)
                self.priority_queue = asyncio.PriorityQueue() # (score, url, depth, parent_url)
                # Note: Lower numbers = higher priority for asyncio.PriorityQueue

            async def _process_page_and_discover_links(
                self, url: str, depth: int, parent_url: str,
                crawler: AsyncWebCrawler, config: CrawlerRunConfig,
                visited: Set[str]
            ) -> AsyncGenerator[CrawlResult, None]:
                if url in visited or depth > self.max_depth or self._pages_crawled >= self.max_pages:
                    return

                visited.add(url)
                # Create a config for fetching a single page
                page_config = config.clone(deep_crawl_strategy=None) # Ensure no recursive deep crawl
                
                # arun returns a CrawlResultContainer, access the first (and only) result
                crawl_result_container = await crawler.arun(url=url, config=page_config)
                page_result = crawl_result_container.results[0] if crawl_result_container.results else None

                if page_result and page_result.success:
                    self._pages_crawled += 1
                    page_result.metadata = page_result.metadata or {}
                    page_result.metadata["depth"] = depth
                    page_result.metadata["parent_url"] = parent_url
                    yield page_result

                    # Discover links
                    new_links_to_score = []
                    # Simplified link discovery for example
                    if page_result.links:
                        for link_type in ["internal", "external"]: # Or just internal
                            for link_info in page_result.links.get(link_type, []):
                                next_url = link_info.get("href")
                                if next_url and await self.can_process_url(next_url, depth + 1):
                                     if next_url not in visited: # Check visited again before adding to score
                                        new_links_to_score.append(next_url)
                    
                    for new_url in set(new_links_to_score): # Process unique new links
                        # Fetch title (simplified, real scorer would be more robust)
                        # This is inefficient here, a real scorer would be separate
                        temp_page_config = config.clone(deep_crawl_strategy=None, only_text=True, word_count_threshold=0) # very light fetch
                        try:
                            temp_result_container = await crawler.arun(url=new_url, config=temp_page_config)
                            temp_result = temp_result_container.results[0] if temp_result_container.results else None
                            title_len = len(temp_result.metadata.get("title", "")) if temp_result and temp_result.metadata else 0
                            score = -title_len # Negative because lower number = higher priority
                            if new_url not in visited: # Final check before putting in queue
                                await self.priority_queue.put((score, new_url, depth + 1, url))
                        except Exception as e:
                            if self.logger: self.logger.warning(f"Scoring error for {new_url}: {e}")


            async def _arun_stream(self, start_url: str, crawler: AsyncWebCrawler, config: CrawlerRunConfig) -> AsyncGenerator[CrawlResult, None]:
                self.reset_stats() # Important!
                visited: Set[str] = set()
                await self.priority_queue.put((0, start_url, 0, None)) # Initial score 0 for seed

                while not self.priority_queue.empty() and self._pages_crawled < self.max_pages:
                    _score, current_url, current_depth, parent_url = await self.priority_queue.get()
                    
                    if current_url in visited:
                        continue

                    async for page_result in self._process_page_and_discover_links(
                        current_url, current_depth, parent_url, crawler, config, visited
                    ):
                        yield page_result
                self.log_stats()
            
            # _arun_batch would collect all yields from _arun_stream
            async def _arun_batch(self, start_url: str, crawler: AsyncWebCrawler, config: CrawlerRunConfig) -> List[CrawlResult]:
                results = []
                async for result in self._arun_stream(start_url, crawler, config):
                    results.append(result)
                return results

        # Usage:
        # title_strategy = TitleLengthPriorityStrategy(max_depth=3)
        # run_config_custom = CrawlerRunConfig(deep_crawl_strategy=title_strategy)
        ```

*   **11.3. Developing a Custom `URLFilter`**
    *   **Inheriting from `URLFilter`:** Your custom filter class must inherit from `crawl4ai.deep_crawling.filters.URLFilter`.
    *   **Implementing `apply(self, url: str) -> bool`:** This is the core method. It takes a URL string and must return `True` if the URL should pass the filter, or `False` if it should be rejected.
        *   You can also make `apply` an `async def` if it needs to perform asynchronous operations (e.g., an API call to check a dynamic blocklist).
    *   **Leveraging `FilterStats`:**
        *   Call `self._update_stats(passed=True/False)` at the end of your `apply` method to correctly update the filter's statistics (`total_urls`, `passed_urls`, `rejected_urls`).
    *   `* Code Example: [A filter that blocks URLs with more than 5 path segments or URLs containing a specific query parameter 'sessionid']`
        ```python
        from crawl4ai import URLFilter, FilterStats
        from urllib.parse import urlparse, parse_qs

        class PathAndQueryParamFilter(URLFilter):
            def __init__(self, max_segments=5, forbidden_param="sessionid", **kwargs):
                super().__init__(**kwargs) # Pass name if desired
                self.max_segments = max_segments
                self.forbidden_param = forbidden_param

            # This can be async if needed: async def apply(self, url: str) -> bool:
            def apply(self, url: str) -> bool:
                parsed_url = urlparse(url)
                path_segments = parsed_url.path.strip('/').split('/')
                
                # Check path depth
                if len(path_segments) > self.max_segments:
                    self._update_stats(passed=False)
                    return False

                # Check for forbidden query parameter
                query_params = parse_qs(parsed_url.query)
                if self.forbidden_param in query_params:
                    self._update_stats(passed=False)
                    return False
                
                self._update_stats(passed=True)
                return True

        # Usage:
        # custom_filter = PathAndQueryParamFilter(max_segments=4, forbidden_param="tracking_id")
        # filter_chain = FilterChain(filters=[custom_filter])
        ```

*   **11.4. Designing a Custom `URLScorer`**
    *   **Inheriting from `URLScorer`:** Your custom scorer class must inherit from `crawl4ai.deep_crawling.scorers.URLScorer`.
    *   **Implementing `_calculate_score(self, url: str) -> float`:** This method takes a URL string and must return a float representing its "raw" score (before the scorer's `weight` is applied).
        *   This method can also be `async def` if it needs to perform asynchronous operations (e.g., fetching external data to inform the score).
    *   **Score Range:** It's good practice to design your `_calculate_score` to return values in a somewhat consistent range (e.g., 0.0 to 1.0) to make it easier to combine with other scorers in a `CompositeScorer`. The final score returned by the public `score()` method will be `_calculate_score(url) * self.weight`.
    *   `* Code Example: [A scorer that assigns higher scores to URLs ending with '.html' or '.php' and containing the word 'article']`
        ```python
        from crawl4ai import URLScorer

        class ArticlePageScorer(URLScorer):
            def __init__(self, weight: float = 1.0, **kwargs):
                super().__init__(weight=weight, **kwargs) # Pass name if desired

            # Can be async: async def _calculate_score(self, url: str) -> float:
            def _calculate_score(self, url: str) -> float:
                score = 0.0
                if url.lower().endswith(('.html', '.htm', '.php')):
                    score += 0.5
                if 'article' in url.lower():
                    score += 0.5
                return min(score, 1.0) # Cap score at 1.0

        # Usage:
        # article_scorer = ArticlePageScorer(weight=0.8)
        # composite_scorer = CompositeScorer(scorers=[article_scorer, ...])
        ```

*   **11.5. Integrating Deep Crawling with Other Crawl4ai Features**
    *   **Combining with Extraction Strategies:**
        *   **How it works:** The `DeepCrawlStrategy` (BFS, DFS, Best-First) is responsible for discovering and fetching pages. Each `CrawlResult` object it yields (in stream mode) or returns (in batch mode) contains the HTML content. This `CrawlResult` is then passed to the extraction pipeline defined in your `CrawlerRunConfig` (e.g., `LLMExtractionStrategy`, `JsonCssExtractionStrategy`).
        *   **Data Flow:** `DeepCrawlStrategy` produces `CrawlResult` -> `AsyncWebCrawler`'s main loop -> `ExtractionStrategy` (if defined) consumes `CrawlResult.html` or `CrawlResult.markdown` -> Populates `CrawlResult.extracted_content`.
        *   **Ensuring Compatibility:** Built-in deep crawl strategies yield standard `CrawlResult` objects, which are directly usable by extraction strategies. If you build a very custom strategy, ensure it also yields or returns `CrawlResult` instances.
    *   **Authenticated Deep Crawls:**
        *   **Challenge:** Many websites require login to access deeper content. A deep crawl needs to maintain this session.
        *   **Solution:**
            1.  **Initial Login:** Perform a separate, initial `crawler.arun()` call to the login page. Use `js_code` to fill in login forms and submit.
            2.  **Save Session State:** After successful login, save the browser's `storage_state` (cookies, localStorage):
                ```python
                # In the login part of your script
                # login_page_result = await crawler.arun(...)
                # await page.context.storage_state(path="my_session_state.json")
                ```
                (Assuming `page` is accessible, or via a hook that gets the context). A more robust way is to use a dedicated `BrowserConfig` with `user_data_dir` for persistence across crawler instances, or use the `session_id` feature if you keep the same crawler instance.
            3.  **Deep Crawl with Session:** For the subsequent deep crawl, configure `BrowserConfig` to use this saved state or `CrawlerRunConfig` to reuse a session via `session_id`:
                ```python
                # Option A: Persistent context via user_data_dir (for multiple crawler instances)
                # browser_config_authed = BrowserConfig(user_data_dir="path/to/my_profile_with_login")
                # async with AsyncWebCrawler(config=browser_config_authed) as authed_crawler:
                #    await authed_crawler.arun(..., config=deep_crawl_run_config)

                # Option B: Reusing a session within the same crawler instance
                # deep_crawl_run_config.session_id = "my_authed_session"
                # (after initial login that established this session)
                # await crawler.arun(..., config=deep_crawl_run_config)
                ```
        *   **Considerations:**
            *   **Token Refresh/Session Expiry:** Long crawls might encounter session expiry. More advanced solutions might need hooks or custom logic to detect expired sessions and re-authenticate.
            *   **AJAX/SPA Logins:** Ensure login interactions are fully completed (e.g., using `wait_for` for redirection or dashboard elements) before saving state or proceeding.
        *   `* Scenario Walkthrough: [Conceptual steps for deep crawling a members-only forum]`
            1.  Create `CrawlerRunConfig` for login: `js_code` to fill login form, `wait_for` a dashboard element.
            2.  `crawler.arun()` to login page with this config. Save `session_id` (e.g., "forum_session").
            3.  Create `CrawlerRunConfig` for deep crawl:
                *   `deep_crawl_strategy` (e.g., BFS to find all threads).
                *   `session_id="forum_session"` to reuse the logged-in state.
                *   Filters to stay within forum sections.
            4.  `crawler.arun()` with the forum's starting URL and deep crawl config.

## 12. Conclusion and Further Exploration

Crawl4ai's `deep_crawling` component offers a powerful and flexible toolkit for exploring websites beyond a single page. By understanding and combining strategies, filters, and scorers, you can tailor your crawls to a wide variety of tasks, from comprehensive site indexing to highly targeted data extraction.

*   **Recap:**
    *   Choose the right **strategy** (BFS, DFS, Best-First) based on your exploration goals.
    *   Use **filters** (`DomainFilter`, `ContentTypeFilter`, `URLPatternFilter`, etc.) to precisely define the scope of your crawl and improve efficiency.
    *   Leverage **scorers** (especially with `BestFirstCrawlingStrategy`) to prioritize URLs and focus on the most relevant content.
    *   Configure everything through `CrawlerRunConfig` and its `deep_crawl_strategy` parameter.
    *   Monitor your crawls using `TraversalStats` and logs to optimize performance.
*   **Encouragement:** The best way to master deep crawling is to experiment!
    *   Start with simple configurations and gradually add complexity.
    *   Test different filter combinations and scorer weightings.
    *   Observe how your changes affect the crawl path and results.
*   **Pointers to Other Relevant Documentation:**
    *   **Basic Crawling:** [Simple Crawling Guide](../core/simple-crawling.md)
    *   **Configuration:** [Browser, Crawler & LLM Configuration](../core/browser-crawler-config.md)
    *   **Specific Filters/Scorers API:** (Refer to API documentation if available, or source code comments)
    *   **Extraction Strategies:** [No-LLM Extraction Strategies](../extraction/no-llm-strategies.md), [LLM-based Extraction](../extraction/llm-extraction.md)
    *   **Session Management & Authentication:** [Session Management](./session-management.md), [Hooks & Auth](./hooks-auth.md)
    *   **Advanced Page Interaction:** [Page Interaction](./page-interaction.md)

Happy deep crawling with Crawl4ai!
```

---


## Deployment - Memory
Source: crawl4ai_deployment_memory_content.llm.md

```markdown
# Detailed Outline for crawl4ai - deployment Component

**Target Document Type:** memory
**Target Output Filename Suggestion:** `llm_memory_deployment.md`
**Library Version Context:** 0.6.0 (as per Dockerfile ARG `C4AI_VER` from provided `Dockerfile` content)
**Outline Generation Date:** 2025-05-24
---

## 1. Introduction to Deployment
    * 1.1. Purpose: This document provides a factual reference for installing the `crawl4ai` library and deploying its server component using Docker. It covers basic and advanced library installation, various Docker deployment methods, server configuration, and an overview of the API for interaction.
    * 1.2. Scope:
        * Installation of the `crawl4ai` Python library.
        * Setup and diagnostic commands for the library.
        * Deployment of the `crawl4ai` server using Docker, including pre-built images, Docker Compose, and manual builds.
        * Explanation of Dockerfile parameters and server configuration via `config.yml`.
        * Details of API interaction, including the Playground UI, Python SDK, and direct REST API calls.
        * Overview of additional server API endpoints and Model Context Protocol (MCP) support.
        * High-level understanding of the server's internal logic relevant to users.
        * The library's version numbering scheme.

## 2. Library Installation

    * 2.1. **Basic Library Installation**
        * 2.1.1. Standard Installation
            * Command: `pip install crawl4ai`
            * Purpose: Installs the core `crawl4ai` library and its essential dependencies for performing web crawling and scraping tasks. This provides the fundamental `AsyncWebCrawler` and related configuration objects.
        * 2.1.2. Post-Installation Setup
            * Command: `crawl4ai-setup`
            * Purpose:
                * Initializes the user's home directory structure for Crawl4ai (e.g., `~/.crawl4ai/cache`).
                * Installs or updates necessary Playwright browsers (Chromium is installed by default) required for browser-based crawling. The `crawl4ai-setup` script internally calls `playwright install --with-deps chromium`.
                * Performs OS-level checks for common missing libraries that Playwright might depend on, providing guidance if issues are found.
                * Creates a default `global.yml` configuration file if one doesn't exist.
        * 2.1.3. Diagnostic Check
            * Command: `crawl4ai-doctor`
            * Purpose:
                * Verifies Python version compatibility.
                * Confirms Playwright installation and browser integrity by attempting a simple crawl of `https://crawl4ai.com`.
                * Inspects essential environment variables and potential library conflicts that might affect Crawl4ai's operation.
                * Provides diagnostic messages indicating success or failure of these checks, with suggestions for resolving common issues.
        * 2.1.4. Verification Process
            * Purpose: To confirm that the basic installation and setup were successful and Crawl4ai can perform a simple crawl.
            * Script Example (as inferred from `crawl4ai-doctor` logic and typical usage):
                ```python
                import asyncio
                from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

                async def main():
                    browser_config = BrowserConfig(
                        headless=True,
                        browser_type="chromium",
                        ignore_https_errors=True,
                        light_mode=True,
                        viewport_width=1280,
                        viewport_height=720,
                    )
                    run_config = CrawlerRunConfig(
                        cache_mode=CacheMode.BYPASS,
                        screenshot=True,
                    )
                    async with AsyncWebCrawler(config=browser_config) as crawler:
                        print("Testing crawling capabilities...")
                        result = await crawler.arun(url="https://crawl4ai.com", config=run_config)
                        if result and result.markdown:
                            print(" Crawling test passed!")
                            return True
                        else:
                            print(" Test failed: Failed to get content")
                            return False

                if __name__ == "__main__":
                    asyncio.run(main())
                ```
            * Expected Outcome: The script should print " Crawling test passed!" and successfully output Markdown content from the crawled page.

    * 2.2. **Advanced Library Installation (Optional Features)**
        * 2.2.1. Installation of Optional Extras
            * Purpose: To install additional dependencies required for specific advanced features of Crawl4ai, such as those involving machine learning models.
            * Options (as defined in `pyproject.toml`):
                * `pip install crawl4ai[pdf]`:
                    * Purpose: Installs `PyPDF2` for PDF processing capabilities.
                * `pip install crawl4ai[torch]`:
                    * Purpose: Installs `torch`, `nltk`, and `scikit-learn`. Enables features relying on PyTorch models, such as some advanced text clustering or semantic analysis within extraction strategies.
                * `pip install crawl4ai[transformer]`:
                    * Purpose: Installs `transformers` and `tokenizers`. Enables the use of Hugging Face Transformers models for tasks like summarization, question answering, or other advanced NLP features within Crawl4ai.
                * `pip install crawl4ai[cosine]`:
                    * Purpose: Installs `torch`, `transformers`, and `nltk`. Specifically for features utilizing cosine similarity with embeddings (implies model usage).
                * `pip install crawl4ai[sync]`:
                    * Purpose: Installs `selenium` for synchronous crawling capabilities (less common, as Crawl4ai primarily focuses on async).
                * `pip install crawl4ai[all]`:
                    * Purpose: Installs all optional dependencies listed above (`PyPDF2`, `torch`, `nltk`, `scikit-learn`, `transformers`, `tokenizers`, `selenium`), providing the complete suite of Crawl4ai capabilities.
        * 2.2.2. Model Pre-fetching
            * Command: `crawl4ai-download-models` (maps to `crawl4ai.model_loader:main`)
            * Purpose: Downloads and caches machine learning models (e.g., specific sentence transformers or classification models from Hugging Face) that are used by certain optional features, particularly those installed via `crawl4ai[transformer]` or `crawl4ai[cosine]`. This avoids runtime downloads and ensures models are available offline.

## 3. Docker Deployment (Server Mode)

    * 3.1. **Prerequisites**
        * 3.1.1. Docker: A working Docker installation. (Link: `https://docs.docker.com/get-docker/`)
        * 3.1.2. Git: Required for cloning the `crawl4ai` repository if building locally or using Docker Compose from the repository. (Link: `https://git-scm.com/book/en/v2/Getting-Started-Installing-Git`)
        * 3.1.3. RAM Requirements:
            * Minimum: 2GB for the basic server without intensive LLM tasks. The `Dockerfile` HEALTCHECK indicates a warning if less than 2GB RAM is available.
            * Recommended for LLM support: 4GB+ (as specified in `docker-compose.yml` limits).
            * Shared Memory (`/dev/shm`): Recommended size is 1GB (`--shm-size=1g`) for optimal Chromium browser performance, as specified in `docker-compose.yml` and run commands.
    * 3.2. **Installation Options**
        * 3.2.1. **Using Pre-built Images from Docker Hub**
            * 3.2.1.1. Image Source: `unclecode/crawl4ai:<tag>`
                * Explanation of `<tag>`:
                    * `latest`: Points to the most recent stable release of Crawl4ai.
                    * Specific version tags (e.g., `0.6.0`, `0.5.1`): Correspond to specific library releases.
                    * Pre-release tags (e.g., `0.6.0-rc1`, `0.7.0-devN`): Development or release candidate versions for testing.
            * 3.2.1.2. Pulling the Image
                * Command: `docker pull unclecode/crawl4ai:<tag>` (e.g., `docker pull unclecode/crawl4ai:latest`)
            * 3.2.1.3. Environment Setup (`.llm.env`)
                * File Name: `.llm.env` (to be created by the user in the directory where `docker run` or `docker-compose` commands are executed).
                * Purpose: To securely provide API keys for various LLM providers used by Crawl4ai for features like LLM-based extraction or Q&A.
                * Example Content (based on `docker-compose.yml`):
                    ```env
                    OPENAI_API_KEY=your_openai_api_key
                    DEEPSEEK_API_KEY=your_deepseek_api_key
                    ANTHROPIC_API_KEY=your_anthropic_api_key
                    GROQ_API_KEY=your_groq_api_key
                    TOGETHER_API_KEY=your_together_api_key
                    MISTRAL_API_KEY=your_mistral_api_key
                    GEMINI_API_TOKEN=your_gemini_api_token
                    ```
                * Creation: Users should create this file and populate it with their API keys. An example (`.llm.env.example`) might be provided in the repository.
            * 3.2.1.4. Running the Container
                * Basic Run (without LLM support):
                    * Command: `docker run -d -p 11235:11235 --shm-size=1g --name crawl4ai-server unclecode/crawl4ai:<tag>`
                    * Port Mapping: `-p 11235:11235` maps port 11235 on the host to port 11235 in the container (default server port).
                    * Shared Memory: `--shm-size=1g` allocates 1GB of shared memory for the browser.
                * Run with LLM Support (mounting `.llm.env`):
                    * Command: `docker run -d -p 11235:11235 --env-file .llm.env --shm-size=1g --name crawl4ai-server unclecode/crawl4ai:<tag>`
            * 3.2.1.5. Stopping the Container
                * Command: `docker stop crawl4ai-server`
                * Command (to remove): `docker rm crawl4ai-server`
            * 3.2.1.6. Docker Hub Versioning:
                * Docker image tags on Docker Hub (e.g., `unclecode/crawl4ai:0.6.0`) directly correspond to `crawl4ai` library releases. The `latest` tag usually points to the most recent stable release. Pre-release tags include suffixes like `-devN`, `-aN`, `-bN`, or `-rcN`.

        * 3.2.2. **Using Docker Compose (`docker-compose.yml`)**
            * 3.2.2.1. Cloning the Repository
                * Command: `git clone https://github.com/unclecode/crawl4ai.git`
                * Command: `cd crawl4ai`
            * 3.2.2.2. Environment Setup (`.llm.env`)
                * File Name: `.llm.env` (should be created in the root of the cloned `crawl4ai` repository).
                * Purpose: Same as above, to provide LLM API keys.
            * 3.2.2.3. Running Pre-built Images
                * Command: `docker-compose up -d`
                * Behavior: Uses the image specified in `docker-compose.yml` (e.g., `${IMAGE:-unclecode/crawl4ai}:${TAG:-latest}`).
                * Overriding image tag: `TAG=0.6.0 docker-compose up -d` or `IMAGE=mycustom/crawl4ai TAG=mytag docker-compose up -d`.
            * 3.2.2.4. Building Locally with Docker Compose
                * Command: `docker-compose up -d --build`
                * Build Arguments (passed from environment variables to `docker-compose.yml` which then passes to `Dockerfile`):
                    * `INSTALL_TYPE`: (e.g., `default`, `torch`, `all`)
                        * Purpose: To include optional Python dependencies during the Docker image build process.
                        * Example: `INSTALL_TYPE=all docker-compose up -d --build`
                    * `ENABLE_GPU`: (e.g., `true`, `false`)
                        * Purpose: To include GPU support (e.g., CUDA toolkits) in the Docker image if the build hardware and target runtime support it.
                        * Example: `ENABLE_GPU=true docker-compose up -d --build`
            * 3.2.2.5. Stopping Docker Compose Services
                * Command: `docker-compose down`

        * 3.2.3. **Manual Local Build & Run**
            * 3.2.3.1. Cloning the Repository: (As above)
            * 3.2.3.2. Environment Setup (`.llm.env`): (As above)
            * 3.2.3.3. Building with `docker buildx`
                * Command Example:
                    ```bash
                    docker buildx build --platform linux/amd64,linux/arm64 \
                      --build-arg C4AI_VER=0.6.0 \
                      --build-arg INSTALL_TYPE=all \
                      --build-arg ENABLE_GPU=false \
                      --build-arg USE_LOCAL=true \
                      -t my-crawl4ai-image:custom .
                    ```
                * Purpose of `docker buildx`: A Docker CLI plugin that extends the `docker build` command with full support for BuildKit builder capabilities, including multi-architecture builds.
                * Explanation of `--platform`: Specifies the target platform(s) for the build (e.g., `linux/amd64`, `linux/arm64`).
                * Explanation of `--build-arg`: Passes build-time variables defined in the `Dockerfile` (see section 3.3).
            * 3.2.3.4. Running the Custom-Built Container
                * Basic Run: `docker run -d -p 11235:11235 --shm-size=1g --name my-crawl4ai-server my-crawl4ai-image:custom`
                * Run with LLM Support: `docker run -d -p 11235:11235 --env-file .llm.env --shm-size=1g --name my-crawl4ai-server my-crawl4ai-image:custom`
            * 3.2.3.5. Stopping the Container: (As above)

    * 3.3. **Dockerfile Parameters (`ARG` values)**
        * 3.3.1. `C4AI_VER`: (Default: `0.6.0`)
            * Role: Specifies the version of the `crawl4ai` library. Used for labeling the image and potentially for version-specific logic.
        * 3.3.2. `APP_HOME`: (Default: `/app`)
            * Role: Defines the working directory inside the Docker container where the application code and related files are stored and executed.
        * 3.3.3. `GITHUB_REPO`: (Default: `https://github.com/unclecode/crawl4ai.git`)
            * Role: The URL of the GitHub repository to clone if `USE_LOCAL` is set to `false`.
        * 3.3.4. `GITHUB_BRANCH`: (Default: `main`)
            * Role: The specific branch of the GitHub repository to clone if `USE_LOCAL` is `false`.
        * 3.3.5. `USE_LOCAL`: (Default: `true`)
            * Role: A boolean flag. If `true`, the `Dockerfile` installs `crawl4ai` from the local source code copied into `/tmp/project/` during the build context. If `false`, it clones the repository specified by `GITHUB_REPO` and `GITHUB_BRANCH`.
        * 3.3.6. `PYTHON_VERSION`: (Default: `3.12`)
            * Role: Specifies the Python version for the base image (e.g., `python:3.12-slim-bookworm`).
        * 3.3.7. `INSTALL_TYPE`: (Default: `default`)
            * Role: Controls which optional dependencies of `crawl4ai` are installed. Possible values: `default` (core), `pdf`, `torch`, `transformer`, `cosine`, `sync`, `all`.
        * 3.3.8. `ENABLE_GPU`: (Default: `false`)
            * Role: A boolean flag. If `true` and `TARGETARCH` is `amd64`, the `Dockerfile` attempts to install the NVIDIA CUDA toolkit for GPU acceleration.
        * 3.3.9. `TARGETARCH`:
            * Role: An automatic build argument provided by Docker, indicating the target architecture of the build (e.g., `amd64`, `arm64`). Used for conditional logic in the `Dockerfile`, such as installing platform-specific optimized libraries or CUDA for `amd64`.

    * 3.4. **Server Configuration (`config.yml`)**
        * 3.4.1. Location: The server loads its configuration from `/app/config.yml` inside the container by default. This path is relative to `APP_HOME`.
        * 3.4.2. Structure Overview (based on `deploy/docker/config.yml`):
            * `app`: General application settings.
                * `title (str)`: API title (e.g., "Crawl4AI API").
                * `version (str)`: API version (e.g., "1.0.0").
                * `host (str)`: Host address for the server to bind to (e.g., "0.0.0.0").
                * `port (int)`: Port for the server to listen on (e.g., 11234, though Docker usually maps to 11235).
                * `reload (bool)`: Enable/disable auto-reload for development (default: `false`).
                * `workers (int)`: Number of worker processes (default: 1).
                * `timeout_keep_alive (int)`: Keep-alive timeout in seconds (default: 300).
            * `llm`: Default LLM configuration.
                * `provider (str)`: Default LLM provider string (e.g., "openai/gpt-4o-mini").
                * `api_key_env (str)`: Environment variable name to read the API key from (e.g., "OPENAI_API_KEY").
                * `api_key (Optional[str])`: Directly pass API key (overrides `api_key_env`).
            * `redis`: Redis connection details.
                * `host (str)`: Redis host (e.g., "localhost").
                * `port (int)`: Redis port (e.g., 6379).
                * `db (int)`: Redis database number (e.g., 0).
                * `password (str)`: Redis password (default: "").
                * `ssl (bool)`: Enable SSL for Redis connection (default: `false`).
                * `ssl_cert_reqs (Optional[str])`: SSL certificate requirements (e.g., "none", "optional", "required").
                * `ssl_ca_certs (Optional[str])`: Path to CA certificate file.
                * `ssl_certfile (Optional[str])`: Path to SSL certificate file.
                * `ssl_keyfile (Optional[str])`: Path to SSL key file.
            * `rate_limiting`: Configuration for API rate limits.
                * `enabled (bool)`: Enable/disable rate limiting (default: `true`).
                * `default_limit (str)`: Default rate limit (e.g., "1000/minute").
                * `trusted_proxies (List[str])`: List of trusted proxy IP addresses.
                * `storage_uri (str)`: Storage URI for rate limit counters (e.g., "memory://", "redis://localhost:6379").
            * `security`: Security-related settings.
                * `enabled (bool)`: Master switch for security features (default: `false`).
                * `jwt_enabled (bool)`: Enable/disable JWT authentication (default: `false`).
                * `https_redirect (bool)`: Enable/disable HTTPS redirection (default: `false`).
                * `trusted_hosts (List[str])`: List of allowed host headers (e.g., `["*"]` or specific domains).
                * `headers (Dict[str, str])`: Default security headers to add to responses (e.g., `X-Content-Type-Options`, `Content-Security-Policy`).
            * `crawler`: Default crawler behavior.
                * `base_config (Dict[str, Any])`: Base parameters for `CrawlerRunConfig`.
                    * `simulate_user (bool)`: (default: `true`).
                * `memory_threshold_percent (float)`: Memory usage threshold for adaptive dispatcher (default: `95.0`).
                * `rate_limiter (Dict[str, Any])`: Configuration for the internal rate limiter for crawling.
                    * `enabled (bool)`: (default: `true`).
                    * `base_delay (List[float, float])`: Min/max delay range (e.g., `[1.0, 2.0]`).
                * `timeouts (Dict[str, float])`: Timeouts for different crawler operations.
                    * `stream_init (float)`: Timeout for stream initialization (default: `30.0`).
                    * `batch_process (float)`: Timeout for batch processing (default: `300.0`).
                * `pool (Dict[str, Any])`: Browser pool settings.
                    * `max_pages (int)`: Max concurrent browser pages (default: `40`).
                    * `idle_ttl_sec (int)`: Time-to-live for idle crawlers in seconds (default: `1800`).
                * `browser (Dict[str, Any])`: Default `BrowserConfig` parameters.
                    * `kwargs (Dict[str, Any])`: Keyword arguments for `BrowserConfig`.
                        * `headless (bool)`: (default: `true`).
                        * `text_mode (bool)`: (default: `true`).
                    * `extra_args (List[str])`: List of additional browser launch arguments (e.g., `"--no-sandbox"`).
            * `logging`: Logging configuration.
                * `level (str)`: Logging level (e.g., "INFO", "DEBUG").
                * `format (str)`: Log message format string.
            * `observability`: Observability settings.
                * `prometheus (Dict[str, Any])`: Prometheus metrics configuration.
                    * `enabled (bool)`: (default: `true`).
                    * `endpoint (str)`: Metrics endpoint path (e.g., "/metrics").
                * `health_check (Dict[str, str])`: Health check endpoint configuration.
                    * `endpoint (str)`: Health check endpoint path (e.g., "/health").
        * 3.4.3. JWT Authentication
            * Enabling: Set `security.enabled: true` and `security.jwt_enabled: true` in `config.yml`.
            * Secret Key: Configured via `security.jwt_secret_key`. This value can be overridden by the environment variable `JWT_SECRET_KEY`.
            * Algorithm: Configured via `security.jwt_algorithm` (default: `HS256`).
            * Token Expiry: Configured via `security.jwt_expire_minutes` (default: `30`).
            * Usage:
                * 1. Client obtains a token by sending a POST request to the `/token` endpoint with an email in the request body (e.g., `{"email": "user@example.com"}`). The email domain might be validated if configured.
                * 2. Client includes the received token in the `Authorization` header of subsequent requests to protected API endpoints: `Authorization: Bearer <your_jwt_token>`.
        * 3.4.4. Customizing `config.yml`
            * 3.4.4.1. Modifying Before Build:
                * Method: Edit the `deploy/docker/config.yml` file within the cloned `crawl4ai` repository before building the Docker image. This new configuration will be baked into the image.
            * 3.4.4.2. Runtime Mount:
                * Method: Mount a custom `config.yml` file from the host machine to `/app/config.yml` (or the path specified by `APP_HOME`) inside the running Docker container.
                * Example Command: `docker run -d -p 11235:11235 -v /path/on/host/my-config.yml:/app/config.yml --name crawl4ai-server unclecode/crawl4ai:latest`
        * 3.4.5. Key Configuration Recommendations
            * Security:
                * Enable JWT (`security.jwt_enabled: true`) if the server is exposed to untrusted networks.
                * Use a strong, unique `jwt_secret_key`.
                * Configure `security.trusted_hosts` to a specific list of allowed hostnames instead of `["*"]` for production.
                * If using a reverse proxy for SSL termination, ensure `https_redirect` is appropriately configured or disabled if the proxy handles it.
            * Resource Management:
                * Adjust `crawler.pool.max_pages` based on server resources to prevent overwhelming the system.
                * Tune `crawler.pool.idle_ttl_sec` to balance resource usage and responsiveness for pooled browser instances.
            * Monitoring:
                * Keep `observability.prometheus.enabled: true` for production monitoring via the `/metrics` endpoint.
                * Ensure the `/health` endpoint is accessible to health checking systems.
            * Performance:
                * Review and customize `crawler.browser.extra_args` for headless browser optimization (e.g., disabling GPU, sandbox if appropriate for your environment).
                * Set reasonable `crawler.timeouts` to prevent long-stalled crawls.

    * 3.5. **API Usage (Interacting with the Dockerized Server)**
        * 3.5.1. **Playground Interface**
            * Access URL: `http://localhost:11235/playground` (assuming default port mapping).
            * Purpose: An interactive web UI (Swagger UI/OpenAPI) allowing users to explore API endpoints, view schemas, construct requests, and test API calls directly from their browser.
        * 3.5.2. **Python SDK (`Crawl4aiDockerClient`)**
            * Class Name: `Crawl4aiDockerClient`
            * Location: (Typically imported as `from crawl4ai.docker_client import Crawl4aiDockerClient`) - Actual import might vary based on final library structure; refer to `docs/examples/docker_example.py` or `docs/examples/docker_python_sdk.py`.
            * Initialization:
                * Signature: `Crawl4aiDockerClient(base_url: str = "http://localhost:11235", api_token: Optional[str] = None, timeout: int = 300)`
                * Parameters:
                    * `base_url (str)`: The base URL of the Crawl4ai server. Default: `"http://localhost:11235"`.
                    * `api_token (Optional[str])`: JWT token for authentication if enabled on the server. Default: `None`.
                    * `timeout (int)`: Default timeout in seconds for HTTP requests to the server. Default: `300`.
            * Authentication (JWT):
                * Method: Pass the `api_token` during client initialization. The token can be obtained from the server's `/token` endpoint or other authentication mechanisms.
            * `crawl()` Method:
                * Signature (Conceptual, based on typical SDK patterns and server capabilities): `async def crawl(self, urls: Union[str, List[str]], browser_config: Optional[Dict] = None, crawler_config: Optional[Dict] = None, stream: bool = False) -> Union[List[Dict], AsyncGenerator[Dict, None]]`
                    *Note: SDK might take `BrowserConfig` and `CrawlerRunConfig` objects directly, which it then serializes.*
                * Key Parameters:
                    * `urls (Union[str, List[str]])`: A single URL string or a list of URL strings to crawl.
                    * `browser_config (Optional[Dict])`: A dictionary representing the `BrowserConfig` object, or a `BrowserConfig` instance itself.
                    * `crawler_config (Optional[Dict])`: A dictionary representing the `CrawlerRunConfig` object, or a `CrawlerRunConfig` instance itself.
                    * `stream (bool)`: If `True`, the method returns an async generator yielding individual `CrawlResult` dictionaries as they are processed by the server. If `False` (default), it returns a list containing all `CrawlResult` dictionaries after all URLs are processed.
                * Return Type: `List[Dict]` (for `stream=False`) or `AsyncGenerator[Dict, None]` (for `stream=True`), where each `Dict` represents a `CrawlResult`.
                * Streaming Behavior:
                    * `stream=True`: Allows processing of results incrementally, suitable for long crawl jobs or real-time data feeds.
                    * `stream=False`: Collects all results before returning, simpler for smaller batches.
            * `get_schema()` Method:
                * Signature: `async def get_schema(self) -> dict`
                * Return Type: `dict`.
                * Purpose: Fetches the JSON schemas for `BrowserConfig` and `CrawlerRunConfig` from the server's `/schema` endpoint. This helps in constructing valid configuration payloads.
        * 3.5.3. **JSON Request Schema for Configurations**
            * Structure: `{"type": "ClassName", "params": {...}}`
            * Purpose: This structure is used by the server (and expected by the Python SDK internally) to deserialize JSON payloads back into Pydantic configuration objects like `BrowserConfig`, `CrawlerRunConfig`, and their nested strategy objects (e.g., `LLMExtractionStrategy`, `PruningContentFilter`). The `type` field specifies the Python class name, and `params` holds the keyword arguments for its constructor.
            * Example (`BrowserConfig`):
                ```json
                {
                    "type": "BrowserConfig",
                    "params": {
                        "headless": true,
                        "browser_type": "chromium",
                        "viewport_width": 1920,
                        "viewport_height": 1080
                    }
                }
                ```
            * Example (`CrawlerRunConfig` with a nested `LLMExtractionStrategy`):
                ```json
                {
                    "type": "CrawlerRunConfig",
                    "params": {
                        "cache_mode": {"type": "CacheMode", "params": "BYPASS"},
                        "screenshot": false,
                        "extraction_strategy": {
                            "type": "LLMExtractionStrategy",
                            "params": {
                                "llm_config": {
                                    "type": "LLMConfig",
                                    "params": {"provider": "openai/gpt-4o-mini"}
                                },
                                "instruction": "Extract the main title and summary."
                            }
                        }
                    }
                }
                ```
        * 3.5.4. **REST API Examples**
            * `/crawl` Endpoint:
                * URL: `http://localhost:11235/crawl`
                * HTTP Method: `POST`
                * Payload Structure (`CrawlRequest` model from `deploy/docker/schemas.py`):
                    ```json
                    {
                        "urls": ["https://example.com"],
                        "browser_config": { // JSON representation of BrowserConfig
                            "type": "BrowserConfig",
                            "params": {"headless": true}
                        },
                        "crawler_config": { // JSON representation of CrawlerRunConfig
                            "type": "CrawlerRunConfig",
                            "params": {"screenshot": true}
                        }
                    }
                    ```
                * Response Structure: A JSON object, typically `{"success": true, "results": [CrawlResult, ...], "server_processing_time_s": float, ...}`.
            * `/crawl/stream` Endpoint:
                * URL: `http://localhost:11235/crawl/stream`
                * HTTP Method: `POST`
                * Payload Structure: Same as `/crawl` (`CrawlRequest` model).
                * Response Structure: Newline Delimited JSON (NDJSON, `application/x-ndjson`). Each line is a JSON string representing a `CrawlResult` object.
                    * Headers: Includes `Content-Type: application/x-ndjson` and `X-Stream-Status: active` while streaming, and a final JSON object `{"status": "completed"}`.

    * 3.6. **Additional API Endpoints (from `server.py`)**
        * 3.6.1. `/html`
            * Endpoint URL: `/html`
            * HTTP Method: `POST`
            * Purpose: Crawls the given URL, preprocesses its raw HTML content specifically for schema extraction purposes (e.g., by sanitizing and simplifying the structure), and returns the processed HTML.
            * Request Body (`HTMLRequest` from `deploy/docker/schemas.py`):
                * `url (str)`: The URL to fetch and process.
            * Response Structure (JSON):
                * `html (str)`: The preprocessed HTML string.
                * `url (str)`: The original URL requested.
                * `success (bool)`: Indicates if the operation was successful.
        * 3.6.2. `/screenshot`
            * Endpoint URL: `/screenshot`
            * HTTP Method: `POST`
            * Purpose: Captures a full-page PNG screenshot of the specified URL. Allows an optional delay before capture and an option to save the file server-side.
            * Request Body (`ScreenshotRequest` from `deploy/docker/schemas.py`):
                * `url (str)`: The URL to take a screenshot of.
                * `screenshot_wait_for (Optional[float])`: Seconds to wait before taking the screenshot. Default: `2.0`.
                * `output_path (Optional[str])`: If provided, the screenshot is saved to this path on the server, and the path is returned. Otherwise, the base64 encoded image is returned. Default: `None`.
            * Response Structure (JSON):
                * `success (bool)`: Indicates if the screenshot was successfully taken.
                * `screenshot (Optional[str])`: Base64 encoded PNG image data, if `output_path` was not provided.
                * `path (Optional[str])`: The absolute server-side path to the saved screenshot, if `output_path` was provided.
        * 3.6.3. `/pdf`
            * Endpoint URL: `/pdf`
            * HTTP Method: `POST`
            * Purpose: Generates a PDF document of the rendered content of the specified URL.
            * Request Body (`PDFRequest` from `deploy/docker/schemas.py`):
                * `url (str)`: The URL to convert to PDF.
                * `output_path (Optional[str])`: If provided, the PDF is saved to this path on the server, and the path is returned. Otherwise, the base64 encoded PDF data is returned. Default: `None`.
            * Response Structure (JSON):
                * `success (bool)`: Indicates if the PDF generation was successful.
                * `pdf (Optional[str])`: Base64 encoded PDF data, if `output_path` was not provided.
                * `path (Optional[str])`: The absolute server-side path to the saved PDF, if `output_path` was provided.
        * 3.6.4. `/execute_js`
            * Endpoint URL: `/execute_js`
            * HTTP Method: `POST`
            * Purpose: Executes a list of JavaScript snippets on the specified URL in the browser context and returns the full `CrawlResult` object, including any modifications or data retrieved by the scripts.
            * Request Body (`JSEndpointRequest` from `deploy/docker/schemas.py`):
                * `url (str)`: The URL on which to execute the JavaScript.
                * `scripts (List[str])`: A list of JavaScript code snippets to execute sequentially. Each script should be an expression that returns a value.
            * Response Structure (JSON): A `CrawlResult` object (serialized to a dictionary) containing the state of the page after JS execution, including `js_execution_result`.
        * 3.6.5. `/ask` (Endpoint defined as `/ask` in `server.py`)
            * Endpoint URL: `/ask`
            * HTTP Method: `GET`
            * Purpose: Retrieves context about the Crawl4ai library itself, either code snippets or documentation sections, filtered by a query. This is designed for AI assistants or RAG systems needing information about Crawl4ai.
            * Parameters (Query):
                * `context_type (str, default="all", enum=["code", "doc", "all"])`: Specifies whether to return "code", "doc", or "all" (both).
                * `query (Optional[str])`: A search query string used to filter relevant chunks using BM25 ranking. If `None`, returns all context of the specified type(s).
                * `score_ratio (float, default=0.5, ge=0.0, le=1.0)`: The minimum score (as a fraction of the maximum possible score for the query) for a chunk to be included in the results.
                * `max_results (int, default=20, ge=1)`: The maximum number of result chunks to return.
            * Response Structure (JSON):
                * If `query` is provided:
                    * `code_results (Optional[List[Dict[str, Union[str, float]]]])`: A list of dictionaries, where each dictionary contains `{"text": "code_chunk...", "score": bm25_score}`. Present if `context_type` is "code" or "all".
                    * `doc_results (Optional[List[Dict[str, Union[str, float]]]])`: A list of dictionaries, where each dictionary contains `{"text": "doc_chunk...", "score": bm25_score}`. Present if `context_type` is "doc" or "all".
                * If `query` is not provided:
                    * `code_context (Optional[str])`: The full concatenated code context as a single string. Present if `context_type` is "code" or "all".
                    * `doc_context (Optional[str])`: The full concatenated documentation context as a single string. Present if `context_type` is "doc" or "all".

    * 3.7. **MCP (Model Context Protocol) Support**
        * 3.7.1. Explanation of MCP:
            * Purpose: The Model Context Protocol (MCP) is a standardized way for AI models (like Anthropic's Claude with Code Interpreter capabilities) to discover and interact with external tools and data sources. Crawl4ai's MCP server exposes its functionalities as tools that an MCP-compatible AI can use.
        * 3.7.2. Connection Endpoints (defined in `mcp_bridge.py` and attached to FastAPI app):
            * `/mcp/sse`: Server-Sent Events (SSE) endpoint for MCP communication.
            * `/mcp/ws`: WebSocket endpoint for MCP communication.
            * `/mcp/messages`: Endpoint for clients to POST messages in the SSE transport.
        * 3.7.3. Usage with Claude Code Example:
            * Command: `claude mcp add -t sse c4ai-sse http://localhost:11235/mcp/sse`
            * Purpose: This command (specific to the Claude Code CLI) registers the Crawl4ai MCP server as a tool provider named `c4ai-sse` using the SSE transport. The AI can then discover and invoke tools from this source.
        * 3.7.4. List of Available MCP Tools (defined by `@mcp_tool` decorators in `server.py`):
            * `md`: Fetches Markdown for a URL.
                * Parameters (derived from `get_markdown` function signature): `url (str)`, `filter_type (FilterType)`, `query (Optional[str])`, `cache (Optional[str])`.
            * `html`: Generates preprocessed HTML for a URL.
                * Parameters (derived from `generate_html` function signature): `url (str)`.
            * `screenshot`: Generates a screenshot of a URL.
                * Parameters (derived from `generate_screenshot` function signature): `url (str)`, `screenshot_wait_for (Optional[float])`, `output_path (Optional[str])`.
            * `pdf`: Generates a PDF of a URL.
                * Parameters (derived from `generate_pdf` function signature): `url (str)`, `output_path (Optional[str])`.
            * `execute_js`: Executes JavaScript on a URL.
                * Parameters (derived from `execute_js` function signature): `url (str)`, `scripts (List[str])`.
            * `crawl`: Performs a full crawl operation.
                * Parameters (derived from `crawl` function signature): `urls (List[str])`, `browser_config (Optional[Dict])`, `crawler_config (Optional[Dict])`.
            * `ask`: Retrieves library context.
                * Parameters (derived from `get_context` function signature): `context_type (str)`, `query (Optional[str])`, `score_ratio (float)`, `max_results (int)`.
        * 3.7.5. Testing MCP Connections:
            * Method: Use an MCP client tool (e.g., `claude mcp call c4ai-sse.md url=https://example.com`) to invoke a tool and verify the response.
        * 3.7.6. Accessing MCP Schemas:
            * Endpoint URL: `/mcp/schema`
            * Purpose: Returns a JSON response detailing all registered MCP tools, including their names, descriptions, and input schemas, enabling clients to understand how to use them.

    * 3.8. **Metrics & Monitoring Endpoints**
        * 3.8.1. `/health`
            * Purpose: Provides a basic health check for the server, indicating if it's running and responsive.
            * Response Structure (JSON from `server.py`): `{"status": "ok", "timestamp": float, "version": str}` (where version is `__version__` from `server.py`).
            * Configuration: Path configurable via `observability.health_check.endpoint` in `config.yml`.
        * 3.8.2. `/metrics`
            * Purpose: Exposes application metrics in a format compatible with Prometheus for monitoring and alerting.
            * Response Format: Prometheus text format.
            * Configuration: Enabled via `observability.prometheus.enabled: true` and endpoint path via `observability.prometheus.endpoint` in `config.yml`.

    * 3.9. **Underlying Server Logic (`server.py` - High-Level Understanding)**
        * 3.9.1. FastAPI Application:
            * Framework: The server is built using the FastAPI Python web framework for creating APIs.
        * 3.9.2. `crawler_pool` (`CrawlerPool` from `deploy.docker.crawler_pool`):
            * Role: Manages a pool of `AsyncWebCrawler` instances to reuse browser resources efficiently.
            * `get_crawler(BrowserConfig)`: Fetches an existing idle crawler compatible with the `BrowserConfig` or creates a new one if none are available or compatible.
            * `close_all()`: Iterates through all pooled crawlers and closes them.
            * `janitor()`: An `asyncio.Task` that runs periodically to close and remove crawler instances that have been idle for longer than `crawler.pool.idle_ttl_sec` (configured in `config.yml`).
        * 3.9.3. Global Page Semaphore (`GLOBAL_SEM`):
            * Type: `asyncio.Semaphore`.
            * Purpose: A global semaphore that limits the total number of concurrently open browser pages across all `AsyncWebCrawler` instances managed by the server. This acts as a hard cap to prevent excessive resource consumption.
            * Configuration: The maximum number of concurrent pages is set by `crawler.pool.max_pages` in `config.yml` (default: `30` in `server.py`, but `40` in `config.yml`). The `AsyncWebCrawler.arun` method acquires this semaphore.
        * 3.9.4. Job Router (`init_job_router` from `deploy.docker.job`):
            * Role: Manages asynchronous, long-running tasks, particularly for the `/crawl` (non-streaming batch) endpoint.
            * Mechanism: Uses Redis (configured in `config.yml`) as a backend for task queuing (storing task metadata like status, creation time, URL, result, error) and status tracking.
            * User Interaction: When a job is submitted to an endpoint using this router (e.g., `/crawl/job`), a `task_id` is returned. The client then polls an endpoint like `/task/{task_id}` to get the status and eventual result or error.
        * 3.9.5. Rate Limiting Middleware:
            * Implementation: Uses the `slowapi` library, integrated with FastAPI.
            * Purpose: To protect the server from abuse by limiting the number of requests an IP address can make within a specified time window.
            * Configuration: Settings like `enabled`, `default_limit`, `storage_uri` (e.g., `memory://` or `redis://...`) are managed in the `rate_limiting` section of `config.yml`.
        * 3.9.6. Security Middleware:
            * Implementations: `HTTPSRedirectMiddleware` and `TrustedHostMiddleware` from FastAPI, plus custom logic for adding security headers.
            * Purpose:
                * `HTTPSRedirectMiddleware`: Redirects HTTP requests to HTTPS if `security.https_redirect` is true.
                * `TrustedHostMiddleware`: Ensures requests are only served if their `Host` header matches an entry in `security.trusted_hosts`.
                * Custom header logic: Adds HTTP security headers like `X-Content-Type-Options`, `X-Frame-Options`, `Content-Security-Policy`, `Strict-Transport-Security` to all responses if `security.enabled` is true. These are defined in `security.headers` in `config.yml`.
        * 3.9.7. API Request Mapping:
            * Request Models: Pydantic models defined in `deploy/docker/schemas.py` (e.g., `CrawlRequest`, `MarkdownRequest`, `HTMLRequest`, `ScreenshotRequest`, `PDFRequest`, `JSEndpointRequest`, `TokenRequest`, `RawCode`) define the expected JSON structure for incoming API request bodies.
            * Endpoint Logic: Functions decorated with `@app.post(...)`, `@app.get(...)`, etc., in `server.py` handle incoming HTTP requests. These functions use FastAPI's dependency injection to parse and validate request bodies against the Pydantic models.
            * `AsyncWebCrawler` Interaction:
                * The parameters from the parsed request models (e.g., `CrawlRequest.urls`, `CrawlRequest.browser_config`, `CrawlRequest.crawler_config`) are used.
                * `BrowserConfig` and `CrawlerRunConfig` objects are created by calling their respective `.load()` class methods with the dictionary payloads received in the request (e.g., `BrowserConfig.load(crawl_request.browser_config)`).
                * These configuration objects are then passed to an `AsyncWebCrawler` instance obtained from the `crawler_pool`, typically to its `arun()` (for single URL or when JS execution context is critical) or `arun_many()` (for batch processing of multiple URLs) methods.
            * Result Serialization: The `CrawlResult` objects (or lists/generators of them) returned by the `AsyncWebCrawler` are usually serialized to JSON using their `.model_dump()` method before being included in the HTTP response. For streaming endpoints, each `CrawlResult` is serialized and sent as a separate NDJSON line.

## 4. Version Numbering Scheme

    * 4.1. **Standard Versioning (`MAJOR.MINOR.PATCH`)**
        * `MAJOR`: Incremented when incompatible API changes are made.
        * `MINOR`: Incremented when functionality is added in a backward-compatible manner.
        * `PATCH`: Incremented for backward-compatible bug fixes.
    * 4.2. **Pre-release Suffixes**
        * `devN`: (e.g., `0.6.0.dev1`) Development release. These are typically unstable and used for internal testing or early feedback on new, unrefined features.
        * `aN`: (e.g., `0.6.0a1`) Alpha release. Indicates an early preview of a new version, potentially unstable, and APIs might still change.
        * `bN`: (e.g., `0.6.0b1`) Beta release. Generally feature-complete for the targeted minor or major version but may still contain bugs. APIs are mostly stable at this point.
        * `rcN`: (e.g., `0.6.0rc1`) Release Candidate. A version that is potentially the final release, undergoing final testing to catch critical bugs before official release.
```

---


## Deployment - Reasoning
Source: crawl4ai_deployment_reasoning_content.llm.md

Okay, I'm ready to generate the "Reasoning & Problem-Solving Framework" document for the `deployment` component of `crawl4ai`, based on the outline you provided and the code/documentation context from our previous interaction.

```markdown
# Detailed Outline for crawl4ai - deployment Component

**Target Document Type:** reasoning
**Target Output Filename Suggestion:** `llm_reasoning_deployment.md`
**Library Version Context:** 0.6.0
**Outline Generation Date:** 2025-05-24
---

## 1. Introduction to Crawl4ai Deployment

This guide focuses on deploying and configuring `crawl4ai`, whether you're using it as a simple Python library or as a robust Dockerized server. Understanding the different deployment strategies will help you choose the best approach for your specific needs, from quick local scripts to scalable, API-driven crawling services.

*   1.1. Why Different Deployment Strategies Matter
    *   1.1.1. Explaining the trade-offs: Library vs. Server (Docker) mode.
        *   **Library Mode:**
            *   **Pros:** Simplest to get started with for Python developers, direct integration into existing Python projects, easier debugging of Python-specific logic.
            *   **Cons:** Requires Python environment setup on every machine, can be harder to manage dependencies for larger teams or across different OS, resource management (browsers, memory) is directly tied to the script's host.
            *   **Why choose it?** Ideal for individual developers, small scripts, quick prototyping, or when `crawl4ai` is a component within a larger Python application.
        *   **Server (Docker) Mode:**
            *   **Pros:** Consistent environment (Docker handles dependencies), easy to scale, API-first (accessible from any language), better resource isolation and management, simplified deployment to cloud or on-premise servers.
            *   **Cons:** Requires Docker knowledge, slightly more setup initially, debugging might involve looking at container logs in addition to application logs.
            *   **Why choose it?** Best for team collaboration, production deployments, providing crawling as a service, language-agnostic access, or when you need robust, isolated browser instances.
    *   1.1.2. When to choose simple library installation.
        *   Choose simple library installation when:
            *   You are primarily working in a Python environment.
            *   You need to quickly integrate crawling into an existing Python script or application.
            *   Your deployment target is a machine where you can easily manage Python environments and Playwright browser installations.
            *   You are prototyping or working on a small-scale project.
    *   1.1.3. When a Dockerized server deployment is beneficial (scalability, isolation, API access).
        *   Opt for a Dockerized server when:
            *   You need a consistent, reproducible crawling environment across different machines or team members.
            *   You plan to offer crawling capabilities as an API to other services or applications (potentially written in different languages).
            *   You require better resource isolation for browser instances to prevent them from impacting other processes on the host machine.
            *   You anticipate needing to scale your crawling operations up or down based on demand.
            *   You are deploying to a cloud environment or a server where Docker is the preferred deployment method.

*   1.2. Overview of Installation Paths
    *   1.2.1. Quick guide to choosing your installation path based on needs.
        *   **For local Python development/scripting:** Start with "Core Library Installation." Add "Advanced Library Installation" if you need features like local ML model inference.
        *   **For a standalone, API-accessible server:** Jump to "Docker Deployment." You can choose between pre-built images (easiest), Docker Compose (good for managing related services like Redis), or manual builds (for full control).
    *   1.2.2. What this guide will cover for each path.
        *   This guide will provide step-by-step instructions, explanations of "why" certain steps are necessary, best practices, and troubleshooting tips for both library installation and the various Docker deployment options.

## 2. Core Library Installation & Usage

This section details how to get the `crawl4ai` library up and running directly in your Python environment.

*   2.1. Understanding the Basic Installation
    *   2.1.1. **How-to:** Installing the core `crawl4ai` library.
        *   **Command:**
            ```bash
            pip install crawl4ai
            ```
        *   **What core functionalities this provides:**
            *   The `AsyncWebCrawler` class and its associated configuration objects (`BrowserConfig`, `CrawlerRunConfig`).
            *   Core scraping capabilities (HTML, Markdown, links, media).
            *   Basic content processing and filtering.
            *   Support for Playwright-driven browser automation.
            *   The `crawl4ai-setup` and `crawl4ai-doctor` CLI tools.
    *   2.1.2. The Importance of Post-Installation Setup (`crawl4ai-setup`)
        *   **Why `crawl4ai-setup` is crucial:** `crawl4ai` relies on Playwright for browser automation. Playwright, in turn, needs browser executables (like Chromium, Firefox, WebKit) to be downloaded and installed in a location it can find. `crawl4ai-setup` automates this process.
        *   **What it does:**
            *   Invokes Playwright's browser installation mechanism (e.g., `playwright install --with-deps chromium`).
            *   Performs OS-specific checks to ensure necessary libraries or dependencies for running headless browsers are present (especially important on Linux).
            *   Sets up the local Crawl4ai home directory structure (e.g., `~/.crawl4ai/cache`).
        *   **Troubleshooting common `crawl4ai-setup` issues:**
            *   **Permission errors:** Ensure you have write permissions to the Playwright browser installation directory (often in your user's home directory or a system-wide location if installing as root).
            *   **Network issues:** Browser downloads can be large; ensure a stable internet connection. Proxies might interfere if not configured correctly for Playwright.
            *   **Missing OS dependencies (Linux):** The script attempts to guide you, but you might need to manually install packages like `libnss3`, `libatk1.0-0`, etc.
        *   *Code Example: Running `crawl4ai-setup` and interpreting its output.*
            ```bash
            crawl4ai-setup
            ```
            **Expected Output (Success):**
            ```
            [INIT] Running post-installation setup...
            [SETUP] Playwright browser installation complete.
            [COMPLETE] Post-installation setup completed!
            ```
            **Potential Issue Output:**
            ```
            [ERROR] Failed to install Playwright browsers. Please run 'playwright install --with-deps' manually.
            ```
    *   2.1.3. Diagnosing Your Environment with `crawl4ai-doctor`
        *   **When and why to use `crawl4ai-doctor`:** Run this command if you encounter issues after installation, or if crawls are failing unexpectedly. It performs a series of checks to verify that your Python environment, Playwright installation, and browser executables are correctly set up and accessible.
        *   **Interpreting `crawl4ai-doctor` output for common problems:**
            *   It will check Python version compatibility.
            *   It verifies if Playwright is installed and if browsers can be launched.
            *   It might suggest solutions for common issues it detects.
        *   *Code Example: Running `crawl4ai-doctor` and typical successful/problematic outputs.*
            ```bash
            crawl4ai-doctor
            ```
            **Expected Output (Success):**
            ```
            [INIT] Running Crawl4ai health check...
            [INFO] Python version: 3.X.X
            [INFO] Playwright version: X.Y.Z
            [TEST] Testing crawling capabilities...
            [COMPLETE]  Crawling test passed!
            Crawl4ai doctor check completed. All systems operational.
            ```
            **Potential Issue Output:**
            ```
            [ERROR]  Test failed: Could not launch browser. Ensure Playwright browsers are installed (run 'crawl4ai-setup' or 'playwright install --with-deps chromium').
            ```
    *   2.1.4. Verifying Your Basic Installation: Your First Simple Crawl
        *   **Step-by-step guide:**
            1.  Create a new Python file (e.g., `test_crawl.py`).
            2.  Import necessary classes: `AsyncWebCrawler`, `BrowserConfig`, `CrawlerRunConfig`.
            3.  Write an `async` function.
            4.  Inside the function, create a `BrowserConfig` instance (defaults are usually fine for a first test).
            5.  Create an `AsyncWebCrawler` instance, passing the `BrowserConfig`. Use an `async with` statement for proper resource management.
            6.  Create a `CrawlerRunConfig` instance (again, defaults are fine).
            7.  Call `crawler.arun(url="https://example.com", config=run_config)`.
            8.  Print a part of the result, e.g., `result.markdown[:300]`.
            9.  Use `asyncio.run()` to execute your `async` function.
        *   **Expected output:** You should see the first 300 characters of the Markdown content extracted from `example.com`.
        *   **How to confirm success:** If the script runs without errors and prints Markdown content, your basic installation is working.
        *   *Code Example: A minimal Python script to crawl `example.com`.*
            ```python
            import asyncio
            from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

            async def main():
                browser_cfg = BrowserConfig(headless=True) # Keep headless for non-UI environments
                run_cfg = CrawlerRunConfig(cache_mode=CacheMode.BYPASS) # Bypass cache for a fresh fetch

                async with AsyncWebCrawler(config=browser_cfg) as crawler:
                    print("Attempting to crawl https://example.com...")
                    result = await crawler.arun(url="https://example.com", config=run_cfg)
                    if result.success:
                        print("Crawl successful!")
                        print("Markdown (first 300 chars):")
                        if result.markdown:
                            print(result.markdown.raw_markdown[:300])
                        else:
                            print("No markdown content generated.")
                    else:
                        print(f"Crawl failed: {result.error_message}")

            if __name__ == "__main__":
                asyncio.run(main())
            ```

*   2.2. Advanced Library Installation: Extending Functionality
    *   2.2.1. When to Consider Optional Features
        *   **Identifying use cases:**
            *   **Local Machine Learning/NLP tasks:** If you plan to use features like `CosineSimilarityFilter`, advanced `LLMContentFilter` modes that might leverage local sentence transformers, or other AI-driven text processing directly within your Python script without relying on an external LLM API for everything.
            *   **PyTorch-dependent features:** Some advanced filters or future AI integrations might specifically require PyTorch.
            *   **Hugging Face Transformers:** If you intend to use models directly from the Hugging Face Hub for tasks like summarization, classification, or custom embedding generation within your Crawl4ai workflow.
        *   **Understanding the additional capabilities:** These extras typically bring in libraries like `torch`, `transformers`, `scikit-learn`, and `nltk`, enabling more sophisticated local data processing and AI model inference.
    *   2.2.2. **How-to:** Installing Optional Extras
        *   **Explaining `crawl4ai[torch]`:**
            *   Installs `torch` and related dependencies.
            *   **Why:** Necessary for features that perform local neural network inference, such as certain embedding models or advanced NLP tasks that are PyTorch-based.
        *   **Explaining `crawl4ai[transformer]`:**
            *   Installs `transformers` (from Hugging Face) and `tokenizers`.
            *   **Why:** Enables the use of a wide range of pre-trained transformer models for tasks like text generation, summarization, and classification directly within Crawl4ai, often in conjunction with `torch`.
        *   **Explaining `crawl4ai[all]`:**
            *   Installs all optional dependencies, including `torch`, `transformers`, `nltk`, `scikit-learn`, `PyPDF2`, etc.
            *   **When to use:** If you anticipate needing a broad range of features and don't mind a larger installation footprint. Convenient for development environments.
            *   **Potential downsides:** Significantly larger installation size and more dependencies to manage, which might increase the chance of conflicts in complex environments.
        *   *Code Example: `pip install crawl4ai[torch]` and `pip install crawl4ai[all]`.*
            ```bash
            # For PyTorch related features
            pip install crawl4ai[torch]

            # For Hugging Face Transformers related features
            pip install crawl4ai[transformer]

            # To install all optional features
            pip install crawl4ai[all]
            ```
    *   2.2.3. Pre-fetching Models with `crawl4ai-download-models`
        *   **Why pre-fetch models:**
            *   **Offline use:** Allows features dependent on these models (e.g., certain embedding generators, classifiers) to run without an internet connection after initial download.
            *   **Faster startup:** Avoids download time on the first run of a script that uses these models.
            *   **Controlled environment:** Ensures you have the specific model versions Crawl4ai expects.
        *   **How to use the command:**
            ```bash
            crawl4ai-download-models
            ```
        *   **Where models are stored:** Typically in a cache directory managed by the underlying libraries (e.g., Hugging Face's cache, usually in `~/.cache/huggingface`). `crawl4ai-download-models` simply triggers the download process via these libraries.

## 3. Docker Deployment: Running Crawl4ai as a Server

Deploying Crawl4ai with Docker provides a consistent, isolated, and scalable environment, making it ideal for production or when offering crawling as an API service.

*   3.1. Why Deploy Crawl4ai with Docker?
    *   3.1.1. **Benefits:**
        *   **Isolation:** Browser instances and dependencies are contained within the Docker image, preventing conflicts with your host system or other applications.
        *   **Reproducibility:** Ensures that Crawl4ai runs the same way across different environments (development, staging, production).
        *   **Scalability:** Docker containers can be easily scaled up or down using orchestration tools like Kubernetes or Docker Swarm.
        *   **API-first Access:** Exposes Crawl4ai's functionality via a REST API, allowing applications written in any language to utilize its crawling capabilities.
    *   3.1.2. Common use cases for a Dockerized Crawl4ai server.
        *   Providing a centralized crawling service for multiple applications or teams.
        *   Integrating Crawl4ai into non-Python microservices architectures.
        *   Deploying to cloud platforms that favor containerized applications.
        *   Ensuring consistent browser behavior and dependency management for critical crawling tasks.

*   3.2. Prerequisites for Docker Deployment
    *   3.2.1. **Docker:** Ensure Docker Desktop (for Windows/Mac) or Docker Engine (for Linux) is installed and the Docker daemon is running.
        *   *Decision:* If you're new to Docker, visit the official Docker website for installation instructions specific to your OS.
    *   3.2.2. **Git:** Required if you plan to build the Docker image locally from the source code or use Docker Compose with a local repository clone.
        *   *Decision:* If you only intend to use pre-built images from Docker Hub, Git might not be strictly necessary on the deployment machine, but it's good practice for managing configurations.
    *   3.2.3. **RAM Requirements:** Web browsers, especially multiple concurrent instances, can be memory-intensive.
        *   **Guidance:**
            *   Minimum: At least 2GB RAM for the Docker container itself, plus additional RAM per concurrent browser page (e.g., 250-500MB per page, can vary).
            *   A common starting point for a server expected to handle a few concurrent crawls might be 4GB-8GB total allocated to Docker.
            *   Monitor your container's memory usage (`docker stats <container_id>`) and adjust resources as needed. Insufficient RAM can lead to browser crashes or slow performance.
            *   Remember to configure `--shm-size` (shared memory size) for your Docker run command (e.g., `--shm-size=1g`), as Chromium-based browsers heavily use it. The `docker-compose.yml` already includes a `/dev/shm` mount.

*   3.3. Docker Installation Options: A Decision Guide
    *   3.3.1. Option 1: Using Pre-built Images from Docker Hub
        *   **When to use:** This is the **easiest and quickest** way to get started if you don't need custom modifications to the Crawl4ai server image. It's ideal for standard use cases and trying out the server.
        *   **How-to:**
            *   **Pulling the image:**
                ```bash
                docker pull unclecode/crawl4ai:latest # For the latest stable release
                # Or, for a specific version (recommended for production):
                docker pull unclecode/crawl4ai:0.6.0
                ```
            *   **Understanding Docker Hub Tags:**
                *   `latest`: Points to the most recent stable release. Use with caution in production as it can change unexpectedly.
                *   Specific versions (e.g., `0.6.0`): Recommended for production to ensure reproducibility and avoid breaking changes.
                *   `0.6.0-rc1`: Release candidates, nearly stable.
                *   `dev`: Development builds from the `main` branch, potentially unstable.
                *   **Decision:** For production, always pin to a specific version tag. Use `latest` for quick tests or when you always want the newest features and are prepared for potential changes.
            *   **Setting up the environment:** Create a `.llm.env` file in your current directory to store API keys for LLM providers if you plan to use LLM-based extraction or filtering features.
                *   *Example `.llm.env` content:*
                    ```env
                    OPENAI_API_KEY=sk-yourOpenAiApiKeyxxxxxxxxxxxx
                    ANTHROPIC_API_KEY=sk-ant-yourAnthropicApiKeyxxxxxxxx
                    GEMINI_API_TOKEN=yourGoogleAIGeminiApiKeyxxxxxxxx
                    # Add other LLM provider keys as needed
                    ```
            *   **Running the container (Basic, no LLM support initially):**
                ```bash
                docker run -d -p 11235:11235 --name crawl4ai-server --shm-size=1g unclecode/crawl4ai:0.6.0
                ```
                *   `-d`: Run in detached mode (background).
                *   `-p 11235:11235`: Map port 11235 on your host to port 11235 in the container.
                *   `--name crawl4ai-server`: Assign a name to the container for easier management.
                *   `--shm-size=1g`: Allocate 1GB of shared memory, crucial for browser stability.
            *   **Running with LLM Support (mounting `.llm.env`):**
                ```bash
                docker run -d -p 11235:11235 --name crawl4ai-server --shm-size=1g --env-file .llm.env unclecode/crawl4ai:0.6.0
                ```
            *   **Stopping and removing the container:**
                ```bash
                docker stop crawl4ai-server
                docker rm crawl4ai-server
                ```
        *   **Best practices:**
            *   Always use specific version tags in production.
            *   Manage API keys securely using `.env` files or Docker secrets, not by hardcoding them into run commands or Dockerfiles.
    *   3.3.2. Option 2: Using Docker Compose
        *   **When to use:**
            *   When you want an easier way to manage the container's configuration and lifecycle.
            *   If you plan to run related services (e.g., a dedicated Redis instance for rate limiting or job queues) alongside Crawl4ai.
            *   If you need to make minor local customizations to the build process (like choosing `INSTALL_TYPE`) without managing complex `docker build` commands.
        *   **How-to:**
            1.  **Cloning the `crawl4ai` repository:**
                ```bash
                git clone https://github.com/unclecode/crawl4ai.git
                cd crawl4ai
                ```
            2.  **Setting up `.llm.env`:** Create this file in the root of the cloned repository if you need LLM support (see example above).
            3.  **Running with Pre-built Images (default in `docker-compose.yml`):**
                ```bash
                # This will use the image specified in docker-compose.yml (e.g., unclecode/crawl4ai:latest or a specific version)
                docker-compose up -d
                ```
                *   The `docker-compose.yml` file is pre-configured to pull official images and set up necessary volumes (like `/dev/shm`).
            4.  **Building Images Locally with Docker Compose:**
                *   **When this is preferred:** If you need to build the image with specific optional features (`INSTALL_TYPE`) or enable GPU support, and you prefer the `docker-compose` workflow.
                *   **How:** You'll modify the `docker-compose.yml` to use the `build` context or pass build arguments via the command line.
                    ```bash
                    # Example: Build with all features
                    docker-compose build --build-arg INSTALL_TYPE=all
                    docker-compose up -d

                    # Example: Build with GPU support (ensure Dockerfile supports this and host has NVIDIA drivers/toolkit)
                    # Potentially requires modifying docker-compose.yml to pass GPU runtime flags
                    docker-compose build --build-arg ENABLE_GPU=true
                    docker-compose up -d
                    ```
                    *Note: The provided `docker-compose.yml` already has a `build` section, so `docker-compose build` will use it. You can uncomment/modify `args` in the `build` section of `docker-compose.yml` as well.*
            5.  **Stopping services:**
                ```bash
                docker-compose down
                ```
        *   **Advantages:** Simplifies managing container configurations, volumes, and networks, especially if you add more services later.
    *   3.3.3. Option 3: Manual Local Build & Run
        *   **When to use:**
            *   When you need to make significant customizations to the `Dockerfile` itself.
            *   For development and testing of changes to the Crawl4ai server codebase.
            *   If you need to build for a specific architecture not readily available as a pre-built image variant (though `buildx` helps with this).
        *   **How-to:**
            1.  **Cloning the repository:**
                ```bash
                git clone https://github.com/unclecode/crawl4ai.git
                cd crawl4ai
                ```
            2.  **Setting up `.llm.env`:** Create this file in the root directory.
            3.  **Building with `docker buildx` (recommended for multi-arch):**
                *   **Understanding multi-arch builds:** `docker buildx` allows you to build images for multiple architectures (e.g., `linux/amd64` for typical Intel/AMD servers, `linux/arm64` for ARM-based servers like AWS Graviton or Raspberry Pi).
                *   **Passing build arguments:**
                    ```bash
                    # Example: Build for amd64 and arm64, with all features, and tag it
                    docker buildx build \
                      --platform linux/amd64,linux/arm64 \
                      --build-arg INSTALL_TYPE=all \
                      --build-arg ENABLE_GPU=false \
                      -t my-custom-crawl4ai:latest \
                      --push .  # Use --load to load into local Docker images instead of pushing
                    ```
                    *   Replace `--push` with `--load` if you want to use the image locally immediately.
            4.  **Running the locally built container:**
                ```bash
                docker run -d -p 11235:11235 --name my-crawl4ai-server --shm-size=1g --env-file .llm.env my-custom-crawl4ai:latest
                ```
            5.  **Stopping and removing the container:**
                ```bash
                docker stop my-crawl4ai-server
                docker rm my-crawl4ai-server
                ```
        *   **Considerations:** This method gives you the most control but requires a deeper understanding of Docker image building. Build times can be longer, especially with `INSTALL_TYPE=all`.

*   3.4. Understanding Dockerfile Build Parameters (`ARG` values)
    *   These arguments allow you to customize the Docker image during the build process (`docker build` or `docker-compose build`).
    *   `C4AI_VER`:
        *   **Role:** Specifies the version of Crawl4ai to install if not using local source. It's used in the Dockerfile if `USE_LOCAL=false`.
        *   **Why change:** You might want to build an image based on a specific older version or a development tag.
    *   `APP_HOME`:
        *   **Role:** Defines the working directory inside the container (e.g., `/app`).
        *   **Why change:** Rarely needed unless you have specific path requirements for integrations.
    *   `GITHUB_REPO`, `GITHUB_BRANCH`:
        *   **Role:** Used when `USE_LOCAL=false` to clone Crawl4ai from a specific GitHub repository and branch.
        *   **Why change:** To build from your own fork, a feature branch, or a specific commit for testing.
    *   `USE_LOCAL`:
        *   **Role:** A boolean (`true` or `false`). If `true`, the Docker build uses the local source code from the directory where the `Dockerfile` resides (copied via `COPY . /tmp/project/`). If `false`, it clones from `GITHUB_REPO` and `GITHUB_BRANCH`.
        *   **Why change:** Set to `true` when developing and wanting to build an image with your local changes. Set to `false` for CI/CD or building from a canonical Git source.
    *   `PYTHON_VERSION`:
        *   **Role:** Specifies the base Python slim image version (e.g., `3.12`).
        *   **Why change:** If you need to ensure compatibility with a specific Python version for your dependencies or environment.
    *   `INSTALL_TYPE`:
        *   **Role:** Controls which optional dependencies of `crawl4ai` are installed. Options include `default` (core), `all` (all extras), `torch`, `transformer`.
        *   **Impact:**
            *   `default`: Smallest image, fewest features.
            *   `all`: Largest image, all features (including ML/NLP capabilities).
            *   `torch`/`transformer`: Intermediate size, specific ML/NLP capabilities.
        *   **Why change:** To tailor the image size and included features to your specific needs, avoiding unnecessary bloat.
    *   `ENABLE_GPU`:
        *   **Role:** A boolean (`true` or `false`). If `true`, the Dockerfile attempts to install GPU-related dependencies (e.g., CUDA toolkit if `TARGETARCH` is compatible).
        *   **Why change:** Set to `true` if you have a compatible GPU on your Docker host and want to accelerate ML tasks (like local LLM inference or embeddings) inside the container. Requires appropriate Docker runtime configuration (e.g., `--gpus all`).
    *   `TARGETARCH`:
        *   **Role:** Automatically set by Docker Buildx based on the `--platform` flag. It informs the Dockerfile about the target architecture (e.g., `amd64`, `arm64`) so it can install architecture-specific dependencies (like OpenMP for AMD64 or OpenBLAS for ARM64, or CUDA for NVIDIA GPUs on compatible architectures).
        *   **Why be aware:** Essential for understanding multi-arch builds and ensuring correct dependencies are installed for the target platform.
    *   *Guidance: Best practices for setting these arguments:*
        *   For development with local changes: `USE_LOCAL=true`.
        *   For minimal production image: `INSTALL_TYPE=default` (if no advanced features needed).
        *   For ML-heavy tasks on GPU hardware: `ENABLE_GPU=true`, `INSTALL_TYPE=all` (or `torch`/`transformer`).
        *   Always specify `C4AI_VER` or `GITHUB_BRANCH` explicitly for reproducible builds if not using `USE_LOCAL=true`.

*   3.5. Server Configuration (`config.yml`)
    The `config.yml` file (located at `/app/config.yml` inside the container, and `deploy/docker/config.yml` in the source) controls various aspects of the Crawl4ai server's behavior.
    *   3.5.1. Overview of `config.yml` Structure
        *   **`app` section:**
            *   **Purpose:** Configures the FastAPI/Uvicorn server.
            *   `host`, `port`: Network interface and port the server listens on.
            *   `workers`: Number of Uvicorn worker processes (for handling concurrent requests).
            *   **Reasoning:** Adjust `workers` based on your server's CPU cores and expected load. `0.0.0.0` for `host` makes it accessible externally.
        *   **`llm` section:**
            *   **Purpose:** Default settings for LLM integrations.
            *   `provider`: Default LLM provider/model (e.g., `openai/gpt-4o-mini`).
            *   `api_key_env`: The environment variable name from which to read the API key for the default provider (e.g., `OPENAI_API_KEY`).
            *   `api_key`: (Optional, discouraged) Directly embed an API key. It's better to use `api_key_env`.
            *   **Reasoning:** Centralizes default LLM settings. API keys should almost always be managed via environment variables for security.
        *   **`redis` section:**
            *   **Purpose:** Configuration for connecting to a Redis instance.
            *   Used for distributed rate limiting (if `rate_limiting.storage_uri` points to Redis) and potentially for the job queue in future versions.
            *   **Reasoning:** Essential for robust rate limiting in a scaled environment. If not using distributed features, default `memory://` for rate limiting is simpler.
        *   **`rate_limiting` section:**
            *   **Purpose:** Controls API rate limiting to prevent abuse.
            *   `enabled`: `true` or `false`.
            *   `default_limit`: E.g., "1000/minute".
            *   `storage_uri`: `memory://` (default, per-instance) or `redis://...` (for distributed).
            *   **Reasoning:** Always enable in production. Adjust limits based on expected traffic and capacity.
        *   **`security` section:**
            *   **Purpose:** Security-related settings.
            *   `enabled`: Master switch for security features below.
            *   `jwt_enabled`: Enable/disable JWT token authentication for API endpoints.
            *   `https_redirect`: If `true`, redirects HTTP to HTTPS (requires a reverse proxy like Nginx to handle SSL termination).
            *   `trusted_hosts`: List of allowed host headers. `["*"]` allows all, but be more specific in production.
            *   `headers`: Default security headers (X-Content-Type-Options, X-Frame-Options, CSP, HSTS).
            *   **Reasoning:** Crucial for production. `jwt_enabled` protects your API. `trusted_hosts` prevents host header attacks. Default headers provide good baseline security.
        *   **`crawler` section:**
            *   **Purpose:** Default behaviors for the crawler instances managed by the server.
            *   `base_config`: Default `CrawlerRunConfig` parameters if not specified in the API request.
            *   `memory_threshold_percent`: For `MemoryAdaptiveDispatcher`, at what system memory percentage to start throttling.
            *   `rate_limiter`: Default settings for the `RateLimiter` used by dispatchers.
            *   `pool`:
                *   `max_pages`: Corresponds to `GLOBAL_SEM` in `server.py`. Max concurrent browser pages server-wide.
                *   `idle_ttl_sec`: How long an idle browser instance remains in the pool before being cleaned up by the `janitor`.
            *   `browser`: Default `BrowserConfig` parameters.
                *   `kwargs`: Passed to Playwright's browser launch.
                *   `extra_args`: Additional browser command-line flags.
            *   **Reasoning:** Fine-tune these based on server resources and crawling needs. `max_pages` is critical for stability. `idle_ttl_sec` balances responsiveness with resource conservation.
        *   **`logging` section:**
            *   **Purpose:** Controls server-side logging.
            *   `level`: `INFO`, `DEBUG`, `WARNING`, `ERROR`.
            *   `format`: Log message format.
            *   **Reasoning:** Set to `DEBUG` for detailed troubleshooting, `INFO` for general production logs.
        *   **`observability` section:**
            *   **Purpose:** Endpoints for monitoring.
            *   `prometheus.endpoint`: Path for Prometheus metrics (e.g., `/metrics`).
            *   `health_check.endpoint`: Path for health checks (e.g., `/health`).
            *   **Reasoning:** Essential for production monitoring and integration with alerting systems.
    *   3.5.2. Securing Your Server: JWT Authentication
        *   **Why enable JWT authentication:** To protect your Crawl4ai server API from unauthorized access, especially if it's exposed to the internet or a shared network.
        *   **How to enable:** In `config.yml`, under the `security` section, set `jwt_enabled: true`.
        *   **Impact on API requests:** Most API endpoints (those decorated with `Depends(token_dep)`) will require an `Authorization: Bearer <your_jwt_token>` header.
        *   **Generating tokens via the `/token` endpoint:**
            *   The `/token` endpoint itself is *not* protected by JWT.
            *   You send a POST request with an email (currently, any email in a valid format works, but domain verification can be configured for more robust auth if needed for other systems; for Crawl4ai's purpose, the token is the primary gate).
            *   The server responds with an access token.
            *   *Example: Requesting a token with `curl`.*
                ```bash
                curl -X POST "http://localhost:11235/token" \
                     -H "Content-Type: application/json" \
                     -d '{"email": "user@example.com"}'
                ```
                **Expected Response:**
                ```json
                {
                  "email": "user@example.com",
                  "access_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
                  "token_type": "bearer"
                }
                ```
            *   *Example: Requesting a token with Python `requests`.*
                ```python
                import requests
                import json

                response = requests.post(
                    "http://localhost:11235/token",
                    json={"email": "user@example.com"}
                )
                if response.status_code == 200:
                    token_data = response.json()
                    print(f"Access Token: {token_data['access_token']}")
                else:
                    print(f"Error getting token: {response.text}")
                ```
    *   3.5.3. Customizing `config.yml`
        *   **Method 1: Modifying `config.yml` before building a local Docker image.**
            *   **How:** If you're building your own Docker image (Option 3.3.3 or Docker Compose with a local build context), you can directly edit the `deploy/docker/config.yml` file in your cloned repository before running `docker build` or `docker-compose build`.
            *   **Why:** Best if you want the custom configuration to be part of the image itself, ensuring consistency if you distribute or version the image.
        *   **Method 2: Mounting a custom `config.yml` at runtime.**
            *   **How:** Create your custom `config.yml` file on your Docker host machine. Then, when running the container, use a volume mount to replace the default `config.yml` inside the container.
            *   *Code Example:*
                ```bash
                # Assuming your custom config is at ./my-custom-config.yml on the host
                docker run -d -p 11235:11235 \
                  --name crawl4ai-server \
                  --shm-size=1g \
                  --env-file .llm.env \
                  -v "$(pwd)/my-custom-config.yml:/app/config.yml" \
                  unclecode/crawl4ai:0.6.0
                ```
            *   **Why:** Useful for quick configuration changes without rebuilding the image, or for managing configurations separately from the image, especially if you use pre-built images.
    *   3.5.4. Key Configuration Recommendations
        *   **Security:**
            *   **Always** enable `security.jwt_enabled: true` in production or shared environments.
            *   If using a reverse proxy for SSL, set `security.https_redirect: true`.
            *   Configure `security.trusted_hosts` to your server's domain(s) instead of `["*"]` in production.
            *   Review default security headers in `security.headers` and customize if needed for your security policies (e.g., a stricter Content Security Policy).
        *   **Resource Management:**
            *   Adjust `crawler.pool.max_pages` based on your server's RAM and CPU. Too high can lead to instability; too low can underutilize resources.
            *   Set `app.workers` (Uvicorn workers) typically to `(2 * CPU_CORES) + 1` as a starting point, but benchmark for your specific workload.
            *   Tune `crawler.pool.idle_ttl_sec` to balance between keeping browser instances warm (lower TTL) and conserving resources (higher TTL).
        *   **Monitoring:**
            *   Ensure `observability.prometheus.enabled: true` if you use Prometheus.
            *   Integrate the `observability.health_check.endpoint` into your load balancer or container orchestrator health checks.
        *   **Performance:**
            *   For `rate_limiting`, use a Redis backend (`storage_uri: redis://...`) if you have multiple server instances behind a load balancer to share rate limit state. For a single instance, `memory://` is fine.
            *   Adjust `rate_limiting.default_limit` to a reasonable value that protects your server and downstream services without unduly restricting legitimate users.

*   3.6. Interacting with the Dockerized Crawl4ai Server
    *   3.6.1. The Playground Interface (`/playground`)
        *   **How-to:** Open your web browser and navigate to `http://localhost:11235/playground` (or your server's address and port).
        *   **Purpose:**
            *   Provides an interactive UI (Swagger/OpenAPI) to explore all available API endpoints.
            *   Allows you to test API calls directly from your browser.
            *   Shows request and response schemas, making it easy to understand payload structures.
            *   Helps in generating example request payloads for your own client applications.
        *   **Key features to explore:**
            *   Expand each endpoint to see its parameters, request body schema, and possible responses.
            *   Use the "Try it out" button to send test requests.
            *   Examine the "Schemas" section at the bottom to understand the structure of objects like `CrawlRequest`, `BrowserConfig`, `CrawlerRunConfig`, and `CrawlResult`.
    *   3.6.2. Using the Python SDK (`Crawl4aiDockerClient`)
        *   **How-to:**
            ```python
            from crawl4ai.docker_client import Crawl4aiDockerClient
            import asyncio

            client = Crawl4aiDockerClient(base_url="http://localhost:11235")

            async def run_crawl():
                # ... (define browser_config_dict and crawler_config_dict)
                # See "Constructing JSON Configuration Payloads" below for examples
                browser_config_dict = {"type": "BrowserConfig", "params": {"headless": True}}
                crawler_config_dict = {"type": "CrawlerRunConfig", "params": {"screenshot": True}}

                results = await client.crawl(
                    urls=["https://example.com"],
                    browser_config=browser_config_dict,
                    crawler_config=crawler_config_dict
                )
                for result in results:
                    if result.success:
                        print(f"Crawled {result.url}, screenshot available: {bool(result.screenshot)}")
                    else:
                        print(f"Failed {result.url}: {result.error_message}")

            # asyncio.run(run_crawl())
            ```
        *   **Authentication with the SDK when JWT is enabled:**
            *   If your server has `security.jwt_enabled: true`, you'll need to authenticate the client.
            *   *Code Example:*
                ```python
                # client = Crawl4aiDockerClient(base_url="http://localhost:11235")
                # await client.authenticate_with_email(email="user@example.com")
                # Now client will automatically include the token in subsequent requests.
                # Or, if you already have a token:
                # client.set_token("your_jwt_token_here")
                ```
                *Note: The `authenticate_with_email` method is a conceptual example. The actual SDK might require you to fetch the token separately and then use `client.set_token()`.*
        *   **Making `crawl()` requests:**
            *   **Non-streaming (default):**
                *   **When to use:** For a small number of URLs or when you need all results before proceeding.
                *   **How results are returned:** The `client.crawl()` call will block until all URLs are processed, then return a list of `CrawlResult` objects.
            *   **Streaming (`stream=True`):**
                *   **Benefits:** For long-running crawls involving many URLs or when processing time per URL is high. It allows you to process results incrementally as they become available, improving responsiveness and potentially reducing memory footprint if you process and discard results immediately.
                *   **How to process:** The `client.crawl(..., stream=True)` will return an async generator. You iterate over it using `async for`.
            *   *Code Example: Python snippet demonstrating both.*
                ```python
                from crawl4ai.docker_client import Crawl4aiDockerClient
                from crawl4ai import BrowserConfig, CrawlerRunConfig, CacheMode
                import asyncio

                client = Crawl4aiDockerClient(base_url="http://localhost:11235")
                # Assume client is authenticated if JWT is enabled server-side

                browser_cfg_dict = BrowserConfig(headless=True).dump() # Use .dump() to get the serializable dict
                crawler_cfg_dict_base = CrawlerRunConfig(cache_mode=CacheMode.BYPASS).dump()

                urls_to_crawl = ["https://example.com", "https://crawl4ai.com"]

                async def non_streaming_example():
                    print("\n--- Non-Streaming Example ---")
                    results_list = await client.crawl(
                        urls=urls_to_crawl,
                        browser_config=browser_cfg_dict,
                        crawler_config=crawler_cfg_dict_base
                    )
                    for result_data in results_list: # result_data is a dict here
                        print(f"Non-Streamed: {result_data.get('url')} - Success: {result_data.get('success')}")

                async def streaming_example():
                    print("\n--- Streaming Example ---")
                    crawler_cfg_dict_stream = CrawlerRunConfig(cache_mode=CacheMode.BYPASS, stream=True).dump()
                    async for result_data in client.crawl( # result_data is a dict here
                        urls=urls_to_crawl,
                        browser_config=browser_cfg_dict,
                        crawler_config=crawler_cfg_dict_stream
                    ):
                        print(f"Streamed: {result_data.get('url')} - Success: {result_data.get('success')}")
                        # Process each result as it arrives
                        if result_data.get('status') == 'completed': # Check for final stream completion marker
                            print("Stream ended.")
                            break


                async def main_sdk():
                    # If JWT is enabled:
                    # success = await client.authenticate_with_email(email="user@example.com")
                    # if not success:
                    #     print("SDK Authentication failed.")
                    #     return

                    await non_streaming_example()
                    await streaming_example()

                # asyncio.run(main_sdk())
                ```
        *   **Fetching API Schema with `get_schema()`:**
            *   **How this helps:** The schema describes the structure of `BrowserConfig` and `CrawlerRunConfig`, including all available parameters, their types, and default values. This is useful for programmatically understanding configuration options or validating your payloads.
            *   *Code Example:*
                ```python
                # async def show_schema():
                #     schema = await client.get_schema()
                #     print("BrowserConfig Schema:", json.dumps(schema['browser'], indent=2))
                #     print("CrawlerRunConfig Schema:", json.dumps(schema['crawler'], indent=2))
                # asyncio.run(show_schema())
                ```
    *   3.6.3. Constructing JSON Configuration Payloads
        *   **Understanding the `{"type": "ClassName", "params": {...}}` pattern:**
            *   Crawl4ai uses this pattern for serializing and deserializing configuration objects that can have different underlying implementations (strategies).
            *   `"type"`: The Python class name (e.g., "BrowserConfig", "CrawlerRunConfig", "LLMExtractionStrategy").
            *   `"params"`: A dictionary of the parameters that would be passed to the class's `__init__` method.
            *   **Why this pattern?** It allows the server to dynamically instantiate the correct Python configuration objects from the JSON payload sent by the client.
        *   **How-to:** Translate Python class initializations to JSON:
            *   If you have `BrowserConfig(headless=False, browser_type="firefox")` in Python.
            *   The JSON equivalent is:
                ```json
                {
                    "type": "BrowserConfig",
                    "params": {
                        "headless": false,
                        "browser_type": "firefox"
                    }
                }
                ```
            *   Python `BrowserConfig().dump()` or `CrawlerRunConfig().dump()` methods automatically generate this correct JSON-serializable dictionary structure.
        *   **Common pitfalls:**
            *   Forgetting the `"type"` field.
            *   Incorrectly nesting `"params"`.
            *   Using Python booleans (`True`) instead of JSON booleans (`true`). The `dump()` method handles this.
        *   *Example: JSON payload for a complex `CrawlRequest` for the `/crawl` endpoint.*
            ```json
            {
                "urls": ["https://example.com/news", "https://blog.example.com"],
                "browser_config": {
                    "type": "BrowserConfig",
                    "params": {
                        "headless": true,
                        "user_agent": "MyCustomCrawler/1.0"
                    }
                },
                "crawler_config": {
                    "type": "CrawlerRunConfig",
                    "params": {
                        "screenshot": true,
                        "pdf": false,
                        "word_count_threshold": 50,
                        "cache_mode": "bypass"
                    }
                }
            }
            ```
    *   3.6.4. Direct REST API Usage
        *   **When to prefer direct HTTP requests:**
            *   When integrating Crawl4ai into applications written in languages other than Python.
            *   For simple, one-off requests where setting up the SDK might be overkill.
            *   When you need fine-grained control over HTTP headers or request timing not exposed by the SDK.
        *   **How-to:** Making POST requests to `/crawl` (non-streaming).
            *   *Example: `curl` snippet for `/crawl`.*
                ```bash
                # Ensure you have your JWT token if security is enabled
                # export C4AI_TOKEN="your_jwt_token_here"
                curl -X POST "http://localhost:11235/crawl" \
                     -H "Content-Type: application/json" \
                     -H "Authorization: Bearer $C4AI_TOKEN" \
                     -d '{
                           "urls": ["https://example.com"],
                           "browser_config": {"type": "BrowserConfig", "params": {"headless": true}},
                           "crawler_config": {"type": "CrawlerRunConfig", "params": {"screenshot": false}}
                         }'
                ```
            *   *Python `requests` snippet for `/crawl`.*
                ```python
                # import requests
                # import json
                #
                # headers = {"Content-Type": "application/json"}
                # # if jwt_token: headers["Authorization"] = f"Bearer {jwt_token}"
                #
                # payload = {
                #     "urls": ["https://example.com"],
                #     "browser_config": {"type": "BrowserConfig", "params": {"headless": True}},
                #     "crawler_config": {"type": "CrawlerRunConfig", "params": {"screenshot": False}}
                # }
                # response = requests.post("http://localhost:11235/crawl", headers=headers, json=payload)
                # if response.status_code == 200:
                #     print(json.dumps(response.json(), indent=2))
                # else:
                #     print(f"Error: {response.status_code} - {response.text}")
                ```
        *   **How-to:** Making POST requests to `/crawl/stream` (streaming).
            *   **Understanding NDJSON:** The server will stream back results as Newline Delimited JSON. Each line is a complete JSON object representing a `CrawlResult` for one URL, or a status update.
            *   *Example: `curl` for `/crawl/stream` (NDJSON output will print to terminal).*
                ```bash
                # curl -N -X POST "http://localhost:11235/crawl/stream" \
                #      -H "Content-Type: application/json" \
                #      -H "Authorization: Bearer $C4AI_TOKEN" \
                #      -d '{
                #            "urls": ["https://example.com", "https://crawl4ai.com"],
                #            "browser_config": {"type": "BrowserConfig", "params": {"headless": true}},
                #            "crawler_config": {"type": "CrawlerRunConfig", "params": {"stream": true}}
                #          }'
                ```
            *   *Python `requests` snippet for `/crawl/stream` and processing NDJSON.*
                ```python
                # import requests
                # import json
                #
                # headers = {"Content-Type": "application/json"}
                # # if jwt_token: headers["Authorization"] = f"Bearer {jwt_token}"
                #
                # payload = {
                #     "urls": ["https://example.com", "https://crawl4ai.com"],
                #     "browser_config": {"type": "BrowserConfig", "params": {"headless": True}},
                #     "crawler_config": {"type": "CrawlerRunConfig", "params": {"stream": True}} # stream implicitly handled by endpoint
                # }
                # with requests.post("http://localhost:11235/crawl/stream", headers=headers, json=payload, stream=True) as r:
                #     if r.status_code == 200:
                #         for line in r.iter_lines():
                #             if line:
                #                 result_data = json.loads(line.decode('utf-8'))
                #                 print(f"Streamed API: {result_data.get('url')} - Success: {result_data.get('success')}")
                #                 if result_data.get('status') == 'completed':
                #                     print("Stream ended via API.")
                #                     break
                #     else:
                #         print(f"Error: {r.status_code} - {r.text}")
                ```

*   3.7. Exploring Additional API Endpoints
    These endpoints provide targeted functionalities beyond general crawling.
    *   3.7.1. `/html`: Generating Preprocessed HTML
        *   **Purpose:** Use this when you need the HTML of a page after JavaScript execution and basic sanitization (e.g., removing scripts, styles), but *before* Crawl4ai's full Markdown conversion or complex filtering. It's ideal for feeding into custom HTML parsers or schema extraction tools that expect mostly clean, rendered HTML.
        *   **Request structure (`HTMLRequest`):**
            *   `url (str)`: The URL to fetch.
        *   **Response structure:**
            *   `html (str)`: The preprocessed HTML content.
            *   `url (str)`: The original URL requested.
            *   `success (bool)`: Indicates if the operation was successful.
        *   *Example: Use case: You have an external tool that extracts microdata from HTML. Use `/html` to get the rendered HTML for this tool.*
            ```bash
            # curl -X POST "http://localhost:11235/html" \
            #      -H "Content-Type: application/json" \
            #      -H "Authorization: Bearer $C4AI_TOKEN" \
            #      -d '{"url": "https://example.com/dynamic-page"}'
            ```
    *   3.7.2. `/screenshot`: Capturing Web Pages
        *   **Purpose:** To obtain a visual snapshot (PNG) of a web page as it's rendered by the browser. Useful for archiving, visual verification, or when textual content alone isn't sufficient.
        *   **Key parameters (`ScreenshotRequest`):**
            *   `url (str)`: The URL to capture.
            *   `screenshot_wait_for (Optional[float])`: Seconds to wait after page load *before* taking the screenshot.
                *   **How to use:** Essential for pages with animations, delayed content loading via JS, or elements that appear after a short interval. Set this to a few seconds (e.g., `2.0` or `5.0`) to allow such content to render.
            *   `output_path (Optional[str])`:
                *   If provided (e.g., `"./screenshots/page.png"`), the server saves the screenshot to this path *relative to the server's filesystem*. The response will contain the absolute path.
                *   If `null` or omitted, the screenshot is returned as a base64-encoded string in the JSON response.
                *   **Decision:** Use `output_path` if the server has persistent storage and you want files saved directly. Omit it if the client needs to receive and handle the image data.
        *   **Response structure:**
            *   `success (bool)`
            *   `path (str)`: (If `output_path` was provided) Absolute path to the saved screenshot on the server.
            *   `screenshot (str)`: (If `output_path` was *not* provided) Base64 encoded PNG data.
        *   *Example: Capturing a screenshot of a dynamic page after a 2-second delay and receiving it as base64.*
            ```bash
            # curl -X POST "http://localhost:11235/screenshot" \
            #      -H "Content-Type: application/json" \
            #      -H "Authorization: Bearer $C4AI_TOKEN" \
            #      -d '{"url": "https://example.com/animated-chart", "screenshot_wait_for": 2.0}'
            ```
    *   3.7.3. `/pdf`: Generating PDFs
        *   **Purpose:** To create a PDF document from a rendered web page. Useful for printable versions, archiving, or offline reading.
        *   **Key parameters (`PDFRequest`):**
            *   `url (str)`: The URL to convert to PDF.
            *   `output_path (Optional[str])`: Similar to `/screenshot`, if provided, saves the PDF to this server-side path. Otherwise, returns base64 PDF data.
        *   **Response structure:**
            *   `success (bool)`
            *   `path (str)`: (If `output_path` was provided) Absolute path to the saved PDF on the server.
            *   `pdf (str)`: (If `output_path` was *not* provided) Base64 encoded PDF data.
        *   *Example: Generating a PDF for documentation and saving it on the server.*
            ```bash
            # curl -X POST "http://localhost:11235/pdf" \
            #      -H "Content-Type: application/json" \
            #      -H "Authorization: Bearer $C4AI_TOKEN" \
            #      -d '{"url": "https://crawl4ai.com/docs", "output_path": "/app_data/pdfs/crawl4ai_docs.pdf"}'
            ```
    *   3.7.4. `/execute_js`: Running Custom JavaScript
        *   **Purpose:** This is a powerful endpoint for advanced page interactions. Use it when you need to:
            *   Click buttons, fill forms, or trigger other UI events programmatically.
            *   Extract data that is only available after certain JS execution (e.g., from dynamically generated DOM elements).
            *   Modify the page content or state before further processing or screenshotting.
        *   **Key parameters (`JSEndpointRequest`):**
            *   `url (str)`: The URL on which to execute the scripts.
            *   `scripts (List[str])`: A list of JavaScript code snippets to execute in order.
            *   **Best practices for JS snippets:**
                *   Each script in the list should be an **expression that returns a value**, or an IIFE (Immediately Invoked Function Expression).
                *   If a script is asynchronous (e.g., involves `fetch` or `setTimeout`), it **must** return a `Promise`. Crawl4ai will `await` this promise.
                *   Keep snippets focused. For complex logic, consider breaking it into multiple steps in the `scripts` list.
                *   Be mindful of the page's context. Your script runs within the browser's environment for that page.
                *   *Example of a good snippet:* `async () => { await new Promise(r => setTimeout(r, 1000)); return document.title; }`
                *   *Example of a snippet that might not work as expected if not returning a promise for async ops:* `setTimeout(() => { console.log('done'); }, 1000);` (Crawl4ai might not wait for this).
        *   **Response structure:** The full `CrawlResult` object (as a JSON serializable dictionary). The results of your JS executions will be in the `js_execution_result` field of the `CrawlResult`. This field will be a dictionary where keys are `script_0`, `script_1`, etc., and values are the return values of your corresponding scripts.
            *   **How to access:** `response_json['results'][0]['js_execution_result']['script_0']`
        *   *Example: Clicking a "Load More" button and then extracting the count of new items.*
            ```python
            # Python client example
            # js_scripts = [
            #     "document.querySelector('#load-more-btn').click();",
            #     "async () => { await new Promise(r => setTimeout(r, 2000)); return document.querySelectorAll('.item').length; }"
            # ]
            # payload = {"url": "https://example.com/infinite-scroll", "scripts": js_scripts}
            # # ... make request to /execute_js ...
            # # new_item_count = response_data['results'][0]['js_execution_result']['script_1']
            ```
    *   3.7.5. `/ask` (or `/library-context`): Retrieving Library Context for AI
        *   **Purpose:** This endpoint is designed to provide contextual information about the Crawl4ai library itself. It's intended to be used by AI assistants (like code generation copilots or RAG systems) to help them understand Crawl4ai's API, features, and documentation, enabling them to generate more accurate code snippets or provide better assistance.
        *   **Key parameters:**
            *   `context_type (str)`: `"code"`, `"doc"`, or `"all"`.
                *   **When to use:**
                    *   `"code"`: For fetching relevant code snippets (functions, classes).
                    *   `"doc"`: For fetching relevant documentation sections.
                    *   `"all"`: For fetching both.
            *   `query (Optional[str])`: A natural language query or keywords. The endpoint uses BM25 (a text retrieval algorithm) to find relevant chunks of code or documentation based on this query.
                *   **How to formulate:** Be specific. E.g., "how to set proxy in BrowserConfig", "CrawlerRunConfig screenshot options".
            *   `score_ratio (float, default: 0.5)`: A value between 0.0 and 1.0. It filters results based on their BM25 score relative to the maximum possible score for the query. A higher `score_ratio` means more stringent filtering (fewer, more relevant results).
                *   **Understanding its impact:** Start with the default. If you get too few results, lower it. If too many irrelevant results, increase it.
            *   `max_results (int, default: 20)`: The maximum number of code chunks or documentation sections to return.
        *   **Response structure:** A JSON object containing:
            *   `code_results (List[Dict])`: If `context_type` includes "code". Each dict has `"text"` (the code chunk) and `"score"`.
            *   `doc_results (List[Dict])`: If `context_type` includes "doc". Each dict has `"text"` (the documentation chunk) and `"score"`.
        *   *Example: Using `/ask` to get information about `BrowserConfig` for an AI assistant.*
            ```bash
            # curl -X GET "http://localhost:11235/ask?context_type=all&query=BrowserConfig%20proxy%20settings&max_results=3" \
            #      -H "Authorization: Bearer $C4AI_TOKEN"
            ```
            This would return code snippets and documentation sections related to proxy settings in `BrowserConfig`.

*   3.8. MCP (Model Context Protocol) Integration
    *   3.8.1. What is MCP and Why Use It?
        *   **Explanation:** MCP (Model Context Protocol) is a standardized way for AI models and development tools (like IDE extensions) to interact with external services and fetch context. Crawl4ai's MCP support allows AI tools that understand MCP (e.g., Anthropic's Claude Code extension for VS Code) to directly use Crawl4ai's functionalities.
        *   **Benefits:**
            *   **Seamless Tool Integration:** AI tools can discover and use Crawl4ai's capabilities without custom API integrations for each tool.
            *   **Contextual Awareness:** The AI model gets structured information about what a tool can do, its parameters, and how to interpret its output.
            *   **Enhanced AI Assistance:** Enables AI to, for example, suggest Crawl4ai code, execute crawls, or get information from web pages directly within the development environment.
    *   3.8.2. Connection Endpoints: `/mcp/sse` and `/mcp/ws`
        *   **SSE (Server-Sent Events - `/mcp/sse`):** A unidirectional stream from server to client. Simpler for many MCP use cases where the tool primarily sends a request and awaits a response or stream of updates.
        *   **WebSockets (`/mcp/ws`):** A bidirectional, persistent connection. More suitable for highly interactive tools or when continuous two-way communication is needed.
        *   **When to choose:** For most current MCP integrations (like with Claude Code), SSE is often sufficient and simpler to implement on the client-tool side.
    *   3.8.3. **How-to:** Integrating with Claude Code
        *   The `claude mcp add` command registers an MCP-compliant service with your Claude Code extension.
        *   *Example:*
            ```bash
            # Assuming your Crawl4ai server is running locally
            claude mcp add -t sse c4ai-mcp-service http://localhost:11235/mcp/sse
            ```
            *Replace `c4ai-mcp-service` with a name of your choice for this tool in Claude.*
        *   **Illustrative workflow:**
            1.  Add the Crawl4ai MCP service to Claude Code.
            2.  In your code editor, you might ask Claude: "@c4ai-mcp-service Get the Markdown for example.com".
            3.  Claude, understanding MCP, would interact with the `/mcp/sse` endpoint, invoke the appropriate Crawl4ai tool (likely the `md` or `crawl` tool), and return the result to you in the editor.
    *   3.8.4. Available MCP Tools and Their Use Cases
        *   The tools exposed via MCP largely mirror the additional API endpoints:
            *   `md`: Get Markdown content for a URL. **Use case:** Quickly summarize a page for an LLM.
            *   `html`: Get preprocessed HTML. **Use case:** Provide cleaner HTML to an AI for parsing.
            *   `screenshot`: Get a screenshot. **Use case:** Visual context for an AI, or for documentation.
            *   `pdf`: Get a PDF. **Use case:** Archival or providing document context.
            *   `execute_js`: Run JavaScript on a page. **Use case:** Interact with dynamic elements before an AI processes the page.
            *   `crawl`: Perform a full crawl operation. **Use case:** Comprehensive data gathering directed by an AI.
            *   `ask`: Query library context. **Use case:** AI asks Crawl4ai about its own capabilities to generate better code.
    *   3.8.5. Testing MCP Connections and Tool Usage
        *   **Simple methods:**
            *   Use `curl` with the `-N` (no-buffering) flag for SSE to see the event stream:
                ```bash
                # Example: Test list_tools via MCP/SSE
                # You'd typically send a JSON-RPC request in the first message after connection.
                # This is a simplified conceptual test.
                # curl -N -H "Content-Type: application/json" http://localhost:11235/mcp/sse
                # (Then send a JSON-RPC request for list_tools on the established connection if the tool supports it interactively)
                ```
            *   Use a WebSocket client (like `wscat` or a browser's developer console) to connect to `/mcp/ws` and send JSON-RPC messages.
            *   The best way to test is often through an MCP-compliant client tool like the Claude Code extension.
    *   3.8.6. Accessing MCP Schemas (`/mcp/schema`)
        *   **How this helps:** This endpoint returns a JSON schema describing all available MCP tools, their methods, parameters, and return types. This is how MCP client tools (like Claude Code) discover what Crawl4ai can do via MCP. It's crucial for the self-describing nature of MCP.

*   3.9. Monitoring Your Crawl4ai Server
    *   3.9.1. Health Checks with `/health`
        *   **Purpose:** A simple endpoint to verify that the Crawl4ai server is running and responsive. Commonly used by load balancers, container orchestrators (like Kubernetes), or uptime monitoring services.
        *   **Interpreting the response:**
            *   A `200 OK` response with JSON like `{"status": "ok", "timestamp": ..., "version": "..."}` indicates the server is healthy.
            *   Any other status code or an inability to connect suggests a problem.
    *   3.9.2. Prometheus Metrics with `/metrics`
        *   **How to integrate:** If `observability.prometheus.enabled: true` in `config.yml` (default is true), this endpoint exposes metrics in Prometheus format. Configure your Prometheus server to scrape this endpoint.
        *   **Overview of important metrics (inferred from `prometheus_fastapi_instrumentator` usage):**
            *   Request counts, latencies, and error rates for API endpoints.
            *   Python process information (CPU, memory - if default instrumentator collectors are active).
            *   Potentially custom metrics related to crawl queue length, active browser instances, etc. (though these might need explicit addition in `server.py`).
        *   **Why use:** Essential for understanding server load, performance bottlenecks, error trends, and for setting up alerts.

*   3.10. Understanding the Server's Inner Workings (High-Level for Users)
    Understanding these components can help you configure the server optimally and troubleshoot issues.
    *   3.10.1. FastAPI Application: The Core of the Server
        *   **Role:** FastAPI is a modern, fast web framework for building APIs with Python. It handles incoming HTTP requests, routing, request validation, and response serialization for all Crawl4ai API endpoints.
        *   **Why it's used:** Its performance, ease of use, and automatic data validation/serialization make it well-suited for building robust APIs like Crawl4ai's.
    *   3.10.2. Managing Browser Instances with `crawler_pool`
        *   **Role:** The `crawler_pool` (likely an instance of `BrowserManager` or a similar custom pool) is responsible for managing a collection of `AsyncWebCrawler` instances.
        *   `get_crawler`: When an API request needs a browser, this function provides an available (and potentially pre-warmed) `AsyncWebCrawler` instance from the pool. If all instances are busy, it might create a new one up to a limit, or wait.
        *   `close_all` and `janitor`: These are crucial for resource management.
            *   `close_all` is typically called on server shutdown to gracefully close all browser instances.
            *   The `janitor` task (referenced in `lifespan`) periodically checks for idle browser instances in the pool and closes them if they've exceeded their `idle_ttl_sec` (configured in `config.yml`).
        *   **Impact:** Proper pool management prevents resource leaks (e.g., too many zombie browser processes) and optimizes browser startup times by reusing instances.
    *   3.10.3. Capping Concurrent Pages with `GLOBAL_SEM`
        *   **Role:** `GLOBAL_SEM` (an `asyncio.Semaphore`) acts as a server-wide gatekeeper, limiting the total number of browser pages that can be concurrently active across all `AsyncWebCrawler` instances.
        *   **Why this is important:** Each browser page consumes significant memory and CPU. Without a cap, a high volume of requests could easily overwhelm the server, leading to crashes or extreme slowdowns.
        *   **How `crawler.pool.max_pages` in `config.yml` relates:** This configuration value directly sets the limit for `GLOBAL_SEM`.
        *   **Decision:** Adjust `max_pages` carefully based on your server's RAM. If you see `asyncio.TimeoutError` or tasks getting stuck waiting for the semaphore, you might have too many concurrent requests for your `max_pages` setting, or individual crawls are taking too long.
    *   3.10.4. Asynchronous Task Management (Job Router - `api.py` based)
        *   **Role:** For operations that can be time-consuming (like a crawl involving many URLs, or an LLM extraction that requires multiple API calls), Crawl4ai often offloads these to background tasks. This is especially true for non-streaming `/crawl` or `/llm/{url_or_task_id}` endpoints.
        *   The "job router" (conceptually, parts of `api.py` and `job.py`) handles:
            1.  Receiving the initial request.
            2.  Assigning a unique `task_id`.
            3.  Storing initial task metadata (URL, status: PENDING/PROCESSING) often in Redis.
            4.  Adding the actual work (e.g., `process_llm_extraction` or `handle_crawl_job`) to a FastAPI `BackgroundTasks` queue or a more robust Celery/RQ queue (if integrated).
            5.  Returning the `task_id` to the client immediately.
            6.  The client then polls a status endpoint (e.g., `/task/{task_id}`) to check progress.
            7.  Once the background task completes, it updates the task's status and result in Redis.
        *   **Role of Redis:**
            *   Stores task state (status, result, error).
            *   Can act as a message broker for task queues in more advanced setups.
        *   **User Interaction:** You submit a job, get a task ID, and then poll for completion. This prevents HTTP timeouts for long-running operations.
    *   3.10.5. Rate Limiting and Security Middleware
        *   **How `config.yml` settings are applied:** FastAPI allows "middleware" to process requests before they hit your main endpoint logic and before responses are sent.
            *   **Rate Limiting:** The `slowapi` library is used. Middleware intercepts each request, checks the client's IP (or token identity) against configured limits (e.g., "1000/minute" from `config.yml`) stored in memory or Redis. If limits are exceeded, it returns a `429 Too Many Requests` error.
            *   **Security:** Middleware like `HTTPSRedirectMiddleware` and `TrustedHostMiddleware` enforce security policies (redirecting HTTP to HTTPS, validating Host headers). Security headers are added to outgoing responses.
        *   **Protections offered:**
            *   Rate limiting: Prevents abuse and server overload.
            *   HTTPS redirect: Enforces secure connections.
            *   Trusted hosts: Mitigates host header injection attacks.
            *   Security headers: Protect against common web vulnerabilities like XSS, clickjacking.
    *   3.10.6. Mapping API Requests to `AsyncWebCrawler`
        *   1.  An HTTP request hits a FastAPI endpoint (e.g., `POST /crawl`).
        *   2.  FastAPI, using Pydantic, validates and parses the JSON request body into a `CrawlRequest` Pydantic model. This model contains `urls`, `browser_config` (as a dict), and `crawler_config` (as a dict).
        *   3.  The endpoint logic uses `BrowserConfig.load(browser_config_dict)` and `CrawlerRunConfig.load(crawler_config_dict)` to convert these dictionaries back into their respective Python configuration objects.
        *   4.  It then calls `await crawler_pool.get_crawler(browser_config_object)` to obtain an appropriate `AsyncWebCrawler` instance. The pool might reuse an existing compatible instance or create a new one.
        *   5.  Finally, `await crawler_instance.arun(url=..., config=crawler_run_config_object)` or `await crawler_instance.arun_many(...)` is called to perform the actual crawl.
        *   **Key takeaway:** The `{"type": ..., "params": ...}` JSON structure is crucial for the server to correctly deserialize configurations passed from clients into the Python objects `AsyncWebCrawler` expects. The `.dump()` methods on config objects are the Pythonic way to generate these serializable dicts.

## 4. Understanding Crawl4ai Versioning

Crawl4ai follows Semantic Versioning (SemVer) to help you manage updates and understand the implications of new releases.

*   4.1. Semantic Versioning (`MAJOR.MINOR.PATCH`)
    *   **`MAJOR` (e.g., `0.x.x` -> `1.x.x`):** Incremented for **incompatible API changes** (breaking changes). You will likely need to update your code when upgrading to a new major version.
        *   *Why it matters:* Pay close attention when a MAJOR version changes. Read release notes carefully.
    *   **`MINOR` (e.g., `0.5.x` -> `0.6.x`):** Incremented for **new functionality added in a backward-compatible manner**. Your existing code should continue to work.
        *   *Why it matters:* You can usually upgrade minor versions safely to get new features and improvements.
    *   **`PATCH` (e.g., `0.6.0` -> `0.6.1`):** Incremented for **backward-compatible bug fixes**.
        *   *Why it matters:* It's generally safe and recommended to apply patch updates.
    *   **Why this is important for users:** SemVer provides predictability. You can configure your dependency management (e.g., in `requirements.txt` or `pyproject.toml`) to allow automatic patch and minor updates (e.g., `crawl4ai~=0.6.0`) but require manual intervention for major updates.

*   4.2. Pre-release Suffixes
    Crawl4ai uses standard suffixes for pre-release versions, allowing users to test upcoming features.
    *   `dev` (e.g., `0.7.0.dev1`): **Development versions.** These are typically built automatically from the main development branch. They are the most cutting-edge but can be unstable and are not recommended for production.
    *   `a` (alpha, e.g., `0.7.0a1`): **Alpha releases.** Early previews of new major or minor versions. Features might be incomplete or buggy. Use for testing and providing early feedback.
    *   `b` (beta, e.g., `0.7.0b1`): **Beta releases.** Feature-set is largely complete, but the release is still undergoing testing and refinement. More stable than alpha but may still contain bugs.
    *   `rc` (release candidate, e.g., `0.7.0rc1`): **Release candidates.** Believed to be stable and ready for final release, pending final testing. Good for testing in staging environments.
    *   **Guidance on when to use pre-release versions:**
        *   Use `dev`, `a`, or `b` if you want to experiment with upcoming features or contribute to testing, but be prepared for instability.
        *   Use `rc` if you want to test the very latest potentially stable version before its official release.
        *   For production, always stick to stable releases (no suffix).
        *   To install pre-releases: `pip install crawl4ai --pre`.

## 5. Troubleshooting Common Deployment Issues

Here are some common issues you might encounter and how to approach them:

*   5.1. Library Installation Problems
    *   **Playwright browser download failures:**
        *   **Symptom:** `crawl4ai-setup` or `playwright install` fails with network errors or messages about not being able to download browsers.
        *   **Reasoning:** Often due to network connectivity issues, firewalls, or proxies blocking the download. Playwright needs to download browser binaries which can be large.
        *   **Solution:**
            *   Ensure stable internet connection.
            *   If behind a proxy, configure Playwright's proxy environment variables (`HTTP_PROXY`, `HTTPS_PROXY`).
            *   Try running `playwright install --with-deps chromium` (or your browser of choice) manually to see more detailed error messages.
            *   Check Playwright's documentation for troubleshooting browser downloads.
    *   **Dependency conflicts:**
        *   **Symptom:** `pip install crawl4ai` fails with messages about conflicting package versions.
        *   **Reasoning:** Your existing Python environment might have packages with versions incompatible with Crawl4ai's dependencies.
        *   **Solution:**
            *   **Best Practice:** Use a virtual environment (e.g., `venv`, `conda`) for your Crawl4ai projects to isolate dependencies.
            *   Examine the error messages to identify the conflicting packages and try to resolve them, perhaps by upgrading/downgrading other packages or installing Crawl4ai in a fresh environment.

*   5.2. Docker Deployment Issues
    *   **Port conflicts:**
        *   **Symptom:** `docker run` or `docker-compose up` fails with an error like "port is already allocated."
        *   **Reasoning:** The default port for Crawl4ai (11235) is already in use by another application on your host machine.
        *   **Solution:**
            *   Stop the other application using the port.
            *   Map Crawl4ai to a different host port: `docker run -p <new_host_port>:11235 ...` (e.g., `-p 11236:11235`). Remember to update your client to use the new host port.
    *   **Incorrect environment variable setup for LLM API keys:**
        *   **Symptom:** LLM-dependent features (like `LLMExtractionStrategy`) fail, often with authentication errors from the LLM provider.
        *   **Reasoning:** The Docker container doesn't have access to the necessary API keys.
        *   **Solution:** Ensure you are correctly passing the `.llm.env` file when running the container (`--env-file .llm.env`) or that environment variables are set through Docker Compose or your orchestration platform. Double-check the variable names in your `.llm.env` file match what `config.yml` expects (e.g., `OPENAI_API_KEY`).
    *   **Memory allocation issues (`--shm-size`):**
        *   **Symptom:** Browsers inside Docker crash, pages fail to load with cryptic errors, or the container itself becomes unresponsive, especially under load.
        *   **Reasoning:** Chromium-based browsers use `/dev/shm` (shared memory) extensively. The Docker default for `/dev/shm` (often 64MB) is usually too small for multiple or complex browser tabs.
        *   **Solution:** Always run your Crawl4ai Docker container with an increased shared memory size. Start with `--shm-size=1g`. If issues persist, try `2g`. The `docker-compose.yml` provided in the Crawl4ai repository typically includes a volume mount for `/dev/shm` which effectively does the same.
    *   **Problems building local Docker images:**
        *   **Symptom:** `docker build` or `docker-compose build` fails.
        *   **Reasoning:** Could be network issues during dependency downloads, incorrect build arguments, problems with the Dockerfile syntax (if modified), or insufficient disk space.
        *   **Solution:**
            *   Check your internet connection.
            *   Carefully review the build arguments you're passing (`INSTALL_TYPE`, `ENABLE_GPU`, etc.).
            *   Examine the Docker build output for specific error messages.
            *   Ensure you have enough disk space.

*   5.3. Server Configuration (`config.yml`) Errors
    *   **YAML syntax errors:**
        *   **Symptom:** Server fails to start, with errors related to parsing `config.yml`.
        *   **Reasoning:** Incorrect indentation, missing colons, or other YAML syntax issues.
        *   **Solution:** Use a YAML linter or validator to check your `config.yml` file. Pay close attention to indentation (spaces, not tabs).
    *   **Misconfigured JWT settings:**
        *   **Symptom:** If `jwt_enabled: true`, clients might get `401 Unauthorized` or `403 Forbidden` errors even with what seems like a correct token.
        *   **Reasoning:** Issues with secret key consistency (if applicable, though Crawl4ai uses a fixed default or one configurable via env var), token expiration, or incorrect algorithm settings (though Crawl4ai handles this internally).
        *   **Solution:** Ensure clients are sending the token correctly in the `Authorization: Bearer <token>` header. Regenerate tokens if they might have expired. For complex JWT issues, you might need to debug the token generation/validation logic if you've heavily customized the server.

*   5.4. API Interaction Problems
    *   **Authentication failures:**
        *   **Symptom:** Client receives `401` or `403` errors.
        *   **Reasoning:** JWT is enabled on the server, but the client is not sending a valid token, or the token has expired.
        *   **Solution:** Ensure your client correctly obtains a token from `/token` and includes it in the `Authorization` header for subsequent requests.
    *   **Incorrectly structured request payloads:**
        *   **Symptom:** Client receives `422 Unprocessable Entity` errors.
        *   **Reasoning:** The JSON payload sent to endpoints like `/crawl` does not match the expected Pydantic schema (e.g., missing required fields, incorrect data types, wrong `{"type": ..., "params": ...}` structure for configs).
        *   **Solution:** Refer to the `/playground` (Swagger UI) for the correct request schemas. Use the `dump()` method of `BrowserConfig` and `CrawlerRunConfig` if constructing payloads in Python to ensure correct serialization.
    *   **Understanding error responses from the API:**
        *   The API usually returns JSON error responses with a `detail` field explaining the issue. Pay attention to this field.
        *   HTTP status codes also provide clues (400 for bad request, 401/403 for auth, 404 for not found, 422 for validation, 500 for server errors).

*   5.5. When to Check Server Logs
    *   **How to access Docker container logs:**
        ```bash
        docker logs crawl4ai-server # Replace crawl4ai-server with your container name/ID
        docker logs -f crawl4ai-server # To follow logs in real-time
        ```
        If using Docker Compose:
        ```bash
        docker-compose logs crawl4ai # Assuming 'crawl4ai' is the service name in docker-compose.yml
        ```
    *   **What to look for:**
        *   Python tracebacks indicating exceptions within the server code.
        *   Log messages from `crawl4ai` itself (often prefixed with tags like `[CRAWLER]`, `[ERROR]`, `[CONFIG]`).
        *   Uvicorn/FastAPI startup messages and request logs.
        *   Any messages related to resource limits (memory, file descriptors).
        *   Playwright browser errors if they are not caught and handled by the application.

## 6. Best Practices for Deployment

*   6.1. **Choosing the Right Deployment Method:**
    *   **Library:** For quick scripts, Python-centric projects, or when direct integration is paramount.
    *   **Docker (Pre-built):** For ease of use, standard deployments, and quick server setup.
    *   **Docker Compose:** For managing Crawl4ai with other services (like Redis) or for simplified local builds with custom arguments.
    *   **Docker (Manual Build):** For full customization, development, or specific CI/CD needs.
*   6.2. **Security Considerations for Server Deployment:**
    *   **Always enable JWT (`security.jwt_enabled: true`)** if the server is accessible beyond your local machine.
    *   Use strong, unique secrets for JWT if you customize it (though Crawl4ai has a default mechanism).
    *   Configure `security.trusted_hosts` to specific domains in production.
    *   Use a reverse proxy (like Nginx or Traefik) to handle SSL/TLS termination and potentially add another layer of security (WAF, IP blocking).
    *   Keep API keys and sensitive configurations out of version control; use `.llm.env` or environment variables.
*   6.3. **Monitoring and Scaling Your Dockerized Server:**
    *   Utilize the `/health` endpoint for liveness/readiness probes in orchestrators.
    *   Integrate `/metrics` with Prometheus and Grafana for performance monitoring and alerting.
    *   Scale horizontally (more container instances) behind a load balancer for high availability and increased throughput.
    *   Adjust `crawler.pool.max_pages` and container resources (CPU, RAM, `--shm-size`) based on observed load and performance.
*   6.4. **Managing Dependencies and Upgrades:**
    *   For library usage, use virtual environments.
    *   For Docker, pin to specific image versions (e.g., `unclecode/crawl4ai:0.6.0`) in production to avoid unexpected updates.
    *   Read release notes carefully before upgrading `MAJOR` or `MINOR` versions.
*   6.5. **Leveraging Configuration for Optimal Performance and Cost-Effectiveness:**
    *   Use appropriate `CacheMode` settings in `CrawlerRunConfig` to avoid re-crawling unchanged content.
    *   Fine-tune `word_count_threshold` and content filters to process only relevant data, especially before sending to costly LLMs.
    *   If using LLM extraction, design efficient prompts and schemas. Consider if a simpler CSS/XPath extraction can achieve the same for some fields.
    *   Adjust `crawler.pool.idle_ttl_sec` to balance resource usage and browser startup latency.

## 7. Next Steps & Further Learning

With a solid understanding of deployment, you're ready to explore more advanced capabilities:

*   7.1. **Exploring Advanced Crawler Configuration (`CrawlerRunConfig`):** Dive into parameters like `js_code`, `wait_for`, various filters (`word_count_threshold`, `exclude_paths`), and media handling options.
*   7.2. **Diving Deeper into Extraction Strategies:** Learn about `LLMExtractionStrategy`, `JsomCssExtractionStrategy`, and how to build custom schemas for precise data extraction.
*   7.3. **Advanced Page Interaction Techniques:** Master the use of `js_code` for complex interactions, form submissions, and handling dynamic content that simple waits can't manage.
*   7.4. **Contributing to Crawl4ai:** If you're interested in improving Crawl4ai, check out the [contribution guidelines](https://github.com/unclecode/crawl4ai/blob/main/CONTRIBUTORS.md) and open issues/PRs.

This deployment guide should provide a strong foundation. Remember that the best configuration often comes from understanding your specific use case, experimenting, and monitoring performance. Happy Crawling!
```

---


## Data Extraction - Memory
Source: crawl4ai_extraction_memory_content.llm.md

Okay, I will now generate the "Foundational Memory" document for the `extraction` component of `crawl4ai`, based on the outline you provided and the information I've processed from the codebase and existing documentation.

```markdown
# Detailed Outline for crawl4ai - extraction Component

**Target Document Type:** memory
**Target Output Filename Suggestion:** `llm_memory_extraction.md`
**Library Version Context:** 0.6.3
**Outline Generation Date:** 2024-05-24
---

## 1. Overview of Data Extraction in Crawl4ai

*   1.1. Purpose of the Extraction Component: The extraction component in Crawl4ai is responsible for parsing structured data from web content (HTML, text, Markdown) or PDF documents. It allows users to define how data should be identified and extracted, using various strategies ranging from rule-based (CSS, XPath, Regex) to LLM-powered approaches. Its goal is to transform raw crawled content into usable, structured information.
*   1.2. Core Concepts:
    *   1.2.1. `ExtractionStrategy`: This is an abstract base class (interface) that defines the contract for all specific extraction methods. Each strategy implements how data is extracted from the provided content.
    *   1.2.2. `ChunkingStrategy`: This is an abstract base class (interface) for strategies that preprocess content by splitting it into smaller, manageable chunks. This is particularly relevant for LLM-based extraction strategies that have token limits for their input.
    *   1.2.3. Schemas: Schemas define the structure of the data to be extracted. For non-LLM strategies like `JsonCssExtractionStrategy` or `JsonXPathExtractionStrategy`, schemas are typically dictionary-based, specifying selectors and field types. For `LLMExtractionStrategy`, schemas can be Pydantic models or JSON schema dictionaries that guide the LLM in structuring its output.
    *   1.2.4. `CrawlerRunConfig`: The `CrawlerRunConfig` object allows users to specify which `extraction_strategy` and `chunking_strategy` (if applicable) should be used for a particular crawl operation via its `arun()` method.

## 2. `ExtractionStrategy` Interface

*   2.1. Purpose: The `ExtractionStrategy` class, found in `crawl4ai.extraction_strategy`, serves as an abstract base class (ABC) defining the standard interface for all data extraction strategies within the Crawl4ai library. Implementations of this class provide specific methods for extracting structured data from content.
*   2.2. Key Abstract Methods:
    *   `extract(self, url: str, content: str, *q, **kwargs) -> List[Dict[str, Any]]`:
        *   Description: Abstract method intended to extract meaningful blocks or chunks from the given content. Subclasses must implement this.
        *   Parameters:
            *   `url (str)`: The URL of the webpage.
            *   `content (str)`: The HTML, Markdown, or text content of the webpage.
            *   `*q`: Variable positional arguments.
            *   `**kwargs`: Variable keyword arguments.
        *   Returns: `List[Dict[str, Any]]` - A list of extracted blocks or chunks, typically as dictionaries.
    *   `run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]`:
        *   Description: Abstract method to process sections of text, often in parallel by default implementations in subclasses. Subclasses must implement this.
        *   Parameters:
            *   `url (str)`: The URL of the webpage.
            *   `sections (List[str])`: List of sections (strings) to process.
            *   `*q`: Variable positional arguments.
            *   `**kwargs`: Variable keyword arguments.
        *   Returns: `List[Dict[str, Any]]` - A list of processed JSON blocks.
*   2.3. Input Format Property:
    *   `input_format (str)`: [Read-only] - An attribute indicating the expected input format for the content to be processed by the strategy (e.g., "markdown", "html", "fit_html", "text"). Default is "markdown".

## 3. Non-LLM Based Extraction Strategies

*   ### 3.1. Class `NoExtractionStrategy`
    *   3.1.1. Purpose: A baseline `ExtractionStrategy` that performs no actual data extraction. It returns the input content as is, typically useful for scenarios where only raw or cleaned HTML/Markdown is needed without further structuring.
    *   3.1.2. Inheritance: `ExtractionStrategy`
    *   3.1.3. Initialization (`__init__`):
        *   3.1.3.1. Signature: `NoExtractionStrategy(**kwargs)`
        *   3.1.3.2. Parameters:
            *   `**kwargs`: Passed to the base `ExtractionStrategy` initializer.
    *   3.1.4. Key Public Methods:
        *   `extract(self, url: str, html: str, *q, **kwargs) -> List[Dict[str, Any]]`:
            *   Description: Returns the provided `html` content wrapped in a list containing a single dictionary: `[{"index": 0, "content": html}]`.
        *   `run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]`:
            *   Description: Returns a list where each input section is wrapped in a dictionary: `[{"index": i, "tags": [], "content": section} for i, section in enumerate(sections)]`.

*   ### 3.2. Class `JsonCssExtractionStrategy`
    *   3.2.1. Purpose: Extracts structured data from HTML content using a JSON schema that defines CSS selectors to locate and extract data for specified fields. It uses BeautifulSoup4 for parsing and selection.
    *   3.2.2. Inheritance: `JsonElementExtractionStrategy` (which inherits from `ExtractionStrategy`)
    *   3.2.3. Initialization (`__init__`):
        *   3.2.3.1. Signature: `JsonCssExtractionStrategy(schema: Dict[str, Any], **kwargs)`
        *   3.2.3.2. Parameters:
            *   `schema (Dict[str, Any])`: The JSON schema defining extraction rules.
            *   `**kwargs`: Passed to the base class initializer. Includes `input_format` (default: "html").
    *   3.2.4. Schema Definition for `JsonCssExtractionStrategy`:
        *   3.2.4.1. `name (str)`: A descriptive name for the schema (e.g., "ProductDetails").
        *   3.2.4.2. `baseSelector (str)`: The primary CSS selector that identifies each root element representing an item to be extracted (e.g., "div.product-item").
        *   3.2.4.3. `fields (List[Dict[str, Any]])`: A list of dictionaries, each defining a field to be extracted from within each `baseSelector` element.
            *   Each field dictionary:
                *   `name (str)`: The key for this field in the output JSON object.
                *   `selector (str)`: The CSS selector for this field, relative to its parent element (either the `baseSelector` or a parent "nested" field).
                *   `type (str)`: Specifies how to extract the data. Common values:
                    *   `"text"`: Extracts the text content of the selected element.
                    *   `"attribute"`: Extracts the value of a specified HTML attribute.
                    *   `"html"`: Extracts the raw inner HTML of the selected element.
                    *   `"list"`: Extracts a list of items. The `fields` sub-key then defines the structure of each item in the list (if objects) or the `selector` directly targets list elements for primitive values.
                    *   `"nested"`: Extracts a nested JSON object. The `fields` sub-key defines the structure of this nested object.
                *   `attribute (str, Optional)`: Required if `type` is "attribute". Specifies the name of the HTML attribute to extract (e.g., "href", "src").
                *   `fields (List[Dict[str, Any]], Optional)`: Required if `type` is "list" (for a list of objects) or "nested". Defines the structure of the nested object or list items.
                *   `transform (str, Optional)`: A string indicating a transformation to apply to the extracted value (e.g., "lowercase", "uppercase", "strip").
                *   `default (Any, Optional)`: A default value to use if the selector does not find an element or the attribute is missing.
    *   3.2.5. Key Public Methods:
        *   `extract(self, url: str, html_content: str, *q, **kwargs) -> List[Dict[str, Any]]`:
            *   Description: Parses the `html_content` and applies the defined schema to extract structured data using CSS selectors.
    *   3.2.6. Features:
        *   3.2.6.1. Nested Extraction: Supports extracting complex, nested JSON objects by defining "nested" type fields within the schema.
        *   3.2.6.2. List Handling: Supports extracting lists of primitive values (e.g., list of strings from multiple `<li>` tags) or lists of structured objects (e.g., a list of product details, each with its own fields).

*   ### 3.3. Class `JsonXPathExtractionStrategy`
    *   3.3.1. Purpose: Extracts structured data from HTML/XML content using a JSON schema that defines XPath expressions to locate and extract data. It uses `lxml` for parsing and XPath evaluation.
    *   3.3.2. Inheritance: `JsonElementExtractionStrategy` (which inherits from `ExtractionStrategy`)
    *   3.3.3. Initialization (`__init__`):
        *   3.3.3.1. Signature: `JsonXPathExtractionStrategy(schema: Dict[str, Any], **kwargs)`
        *   3.3.3.2. Parameters:
            *   `schema (Dict[str, Any])`: The JSON schema defining extraction rules, where selectors are XPath expressions.
            *   `**kwargs`: Passed to the base class initializer. Includes `input_format` (default: "html").
    *   3.3.4. Schema Definition: The schema structure is identical to `JsonCssExtractionStrategy` (see 3.2.4), but the `baseSelector` and field `selector` values must be valid XPath expressions.
    *   3.3.5. Key Public Methods:
        *   `extract(self, url: str, html_content: str, *q, **kwargs) -> List[Dict[str, Any]]`:
            *   Description: Parses the `html_content` using `lxml` and applies the defined schema to extract structured data using XPath expressions.

*   ### 3.4. Class `JsonLxmlExtractionStrategy`
    *   3.4.1. Purpose: Provides an alternative CSS selector-based extraction strategy leveraging the `lxml` library for parsing and selection, which can offer performance benefits over BeautifulSoup4 in some cases.
    *   3.4.2. Inheritance: `JsonCssExtractionStrategy` (and thus `JsonElementExtractionStrategy`, `ExtractionStrategy`)
    *   3.4.3. Initialization (`__init__`):
        *   3.4.3.1. Signature: `JsonLxmlExtractionStrategy(schema: Dict[str, Any], **kwargs)`
        *   3.4.3.2. Parameters:
            *   `schema (Dict[str, Any])`: The JSON schema defining extraction rules, using CSS selectors.
            *   `**kwargs`: Passed to the base class initializer. Includes `input_format` (default: "html").
    *   3.4.4. Schema Definition: Identical to `JsonCssExtractionStrategy` (see 3.2.4).
    *   3.4.5. Key Public Methods:
        *   `extract(self, url: str, html_content: str, *q, **kwargs) -> List[Dict[str, Any]]`:
            *   Description: Parses the `html_content` using `lxml` and applies the defined schema to extract structured data using lxml's CSS selector capabilities (which often translates CSS to XPath internally).

*   ### 3.5. Class `RegexExtractionStrategy`
    *   3.5.1. Purpose: Extracts data from text content (HTML, Markdown, or plain text) using a collection of regular expression patterns. Each match is returned as a structured dictionary.
    *   3.5.2. Inheritance: `ExtractionStrategy`
    *   3.5.3. Initialization (`__init__`):
        *   3.5.3.1. Signature: `RegexExtractionStrategy(patterns: Union[Dict[str, str], List[Tuple[str, str]], "RegexExtractionStrategy._B"] = _B.NOTHING, input_format: str = "fit_html", **kwargs)`
        *   3.5.3.2. Parameters:
            *   `patterns (Union[Dict[str, str], List[Tuple[str, str]], "_B"], default: _B.NOTHING)`:
                *   Description: Defines the regex patterns to use.
                *   Can be a dictionary mapping labels to regex strings (e.g., `{"email": r"..."}`).
                *   Can be a list of (label, regex_string) tuples.
                *   Can be a bitwise OR combination of `RegexExtractionStrategy._B` enum members for using built-in patterns (e.g., `RegexExtractionStrategy.Email | RegexExtractionStrategy.Url`).
            *   `input_format (str, default: "fit_html")`: Specifies the input format for the content. Options: "html" (raw HTML), "markdown" (Markdown from HTML), "text" (plain text from HTML), "fit_html" (content filtered for relevance before regex application).
            *   `**kwargs`: Passed to the base `ExtractionStrategy`.
    *   3.5.4. Built-in Patterns (`RegexExtractionStrategy._B` Enum - an `IntFlag`):
        *   `EMAIL (auto())`: Matches email addresses. Example pattern: `r"[\\w.+-]+@[\\w-]+\\.[\\w.-]+"`
        *   `PHONE_INTL (auto())`: Matches international phone numbers. Example pattern: `r"\\+?\\d[\\d .()-]{7,}\\d"`
        *   `PHONE_US (auto())`: Matches US phone numbers. Example pattern: `r"\\(?\\d{3}\\)?[-. ]?\\d{3}[-. ]?\\d{4}"`
        *   `URL (auto())`: Matches URLs. Example pattern: `r"https?://[^\\s\\'\"<>]+"`
        *   `IPV4 (auto())`: Matches IPv4 addresses. Example pattern: `r"(?:\\d{1,3}\\.){3}\\d{1,3}"`
        *   `IPV6 (auto())`: Matches IPv6 addresses. Example pattern: `r"[A-F0-9]{1,4}(?::[A-F0-9]{1,4}){7}"`
        *   `UUID (auto())`: Matches UUIDs. Example pattern: `r"[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}"`
        *   `CURRENCY (auto())`: Matches currency amounts. Example pattern: `r"(?:USD|EUR|RM|\\$|||)\\s?\\d+(?:[.,]\\d{2})?"`
        *   `PERCENTAGE (auto())`: Matches percentages. Example pattern: `r"\\d+(?:\\.\\d+)?%"`
        *   `NUMBER (auto())`: Matches numbers (integers, decimals). Example pattern: `r"\\b\\d{1,3}(?:[,.]?\\d{3})*(?:\\.\\d+)?\\b"`
        *   `DATE_ISO (auto())`: Matches ISO 8601 dates (YYYY-MM-DD). Example pattern: `r"\\d{4}-\\d{2}-\\d{2}"`
        *   `DATE_US (auto())`: Matches US-style dates (MM/DD/YYYY or MM/DD/YY). Example pattern: `r"\\d{1,2}/\\d{1,2}/\\d{2,4}"`
        *   `TIME_24H (auto())`: Matches 24-hour time formats (HH:MM or HH:MM:SS). Example pattern: `r"\\b(?:[01]?\\d|2[0-3]):[0-5]\\d(?:[:.][0-5]\\d)?\\b"`
        *   `POSTAL_US (auto())`: Matches US postal codes (ZIP codes). Example pattern: `r"\\b\\d{5}(?:-\\d{4})?\\b"`
        *   `POSTAL_UK (auto())`: Matches UK postal codes. Example pattern: `r"\\b[A-Z]{1,2}\\d[A-Z\\d]? ?\\d[A-Z]{2}\\b"`
        *   `HTML_COLOR_HEX (auto())`: Matches HTML hex color codes. Example pattern: `r"#[0-9A-Fa-f]{6}\\b"`
        *   `TWITTER_HANDLE (auto())`: Matches Twitter handles. Example pattern: `r"@[\\w]{1,15}"`
        *   `HASHTAG (auto())`: Matches hashtags. Example pattern: `r"#[\\w-]+"`
        *   `MAC_ADDR (auto())`: Matches MAC addresses. Example pattern: `r"(?:[0-9A-Fa-f]{2}[:-]){5}[0-9A-Fa-f]{2}"`
        *   `IBAN (auto())`: Matches IBANs. Example pattern: `r"[A-Z]{2}\\d{2}[A-Z0-9]{11,30}"`
        *   `CREDIT_CARD (auto())`: Matches common credit card numbers. Example pattern: `r"\\b(?:4\\d{12}(?:\\d{3})?|5[1-5]\\d{14}|3[47]\\d{13}|6(?:011|5\\d{2})\\d{12})\\b"`
        *   `ALL (_B(-1).value & ~_B.NOTHING.value)`: Includes all built-in patterns except `NOTHING`.
        *   `NOTHING (_B(0).value)`: Includes no built-in patterns.
    *   3.5.5. Key Public Methods:
        *   `extract(self, url: str, content: str, **kwargs) -> List[Dict[str, Any]]`:
            *   Description: Applies all configured regex patterns (built-in and custom) to the input `content`.
            *   Returns: `List[Dict[str, Any]]` - A list of dictionaries, where each dictionary represents a match and contains:
                *   `"url" (str)`: The source URL.
                *   `"label" (str)`: The label of the matching regex pattern.
                *   `"value" (str)`: The actual matched string.
                *   `"span" (Tuple[int, int])`: The start and end indices of the match within the content.
    *   3.5.6. Static Method: `generate_pattern`
        *   3.5.6.1. Signature: `staticmethod generate_pattern(label: str, html: str, query: Optional[str] = None, examples: Optional[List[str]] = None, llm_config: Optional[LLMConfig] = None, **kwargs) -> Dict[str, str]`
        *   3.5.6.2. Purpose: Uses an LLM to automatically generate a Python-compatible regular expression pattern for a given label, based on sample HTML content, an optional natural language query describing the target, and/or examples of desired matches.
        *   3.5.6.3. Parameters:
            *   `label (str)`: A descriptive label for the pattern to be generated (e.g., "product_price", "article_date").
            *   `html (str)`: The HTML content from which the pattern should be inferred.
            *   `query (Optional[str], default: None)`: A natural language description of what kind of data the regex should capture (e.g., "Extract the publication date", "Find all ISBN numbers").
            *   `examples (Optional[List[str]], default: None)`: A list of example strings that the generated regex should successfully match from the provided HTML.
            *   `llm_config (Optional[LLMConfig], default: None)`: Configuration for the LLM to be used. If `None`, uses default `LLMConfig`.
            *   `**kwargs`: Additional arguments passed to the LLM completion request (e.g., `temperature`, `max_tokens`).
        *   3.5.6.4. Returns: `Dict[str, str]` - A dictionary containing the generated pattern, in the format `{label: "regex_pattern_string"}`.

## 4. LLM-Based Extraction Strategies

*   ### 4.1. Class `LLMExtractionStrategy`
    *   4.1.1. Purpose: Employs Large Language Models (LLMs) to extract either structured data according to a schema or relevant blocks of text based on natural language instructions from various content formats (HTML, Markdown, text).
    *   4.1.2. Inheritance: `ExtractionStrategy`
    *   4.1.3. Initialization (`__init__`):
        *   4.1.3.1. Signature: `LLMExtractionStrategy(llm_config: Optional[LLMConfig] = None, instruction: Optional[str] = None, schema: Optional[Union[Dict[str, Any], "BaseModel"]] = None, extraction_type: str = "block", chunk_token_threshold: int = CHUNK_TOKEN_THRESHOLD, overlap_rate: float = OVERLAP_RATE, word_token_rate: float = WORD_TOKEN_RATE, apply_chunking: bool = True, force_json_response: bool = False, **kwargs)`
        *   4.1.3.2. Parameters:
            *   `llm_config (Optional[LLMConfig], default: None)`: Configuration for the LLM. If `None`, a default `LLMConfig` is created.
            *   `instruction (Optional[str], default: None)`: Natural language instructions to guide the LLM's extraction process (e.g., "Extract the main article content", "Summarize the key points").
            *   `schema (Optional[Union[Dict[str, Any], "BaseModel"]], default: None)`: A Pydantic model class or a dictionary representing a JSON schema. Used when `extraction_type` is "schema" to define the desired output structure.
            *   `extraction_type (str, default: "block")`: Determines the extraction mode.
                *   `"block"`: LLM identifies and extracts relevant blocks/chunks of text based on the `instruction`.
                *   `"schema"`: LLM attempts to populate the fields defined in `schema` from the content.
            *   `chunk_token_threshold (int, default: CHUNK_TOKEN_THRESHOLD)`: The target maximum number of tokens for each chunk of content sent to the LLM. `CHUNK_TOKEN_THRESHOLD` is defined in `crawl4ai.config` (default value: 10000).
            *   `overlap_rate (float, default: OVERLAP_RATE)`: The percentage of overlap between consecutive chunks to ensure context continuity. `OVERLAP_RATE` is defined in `crawl4ai.config` (default value: 0.1, i.e., 10%).
            *   `word_token_rate (float, default: WORD_TOKEN_RATE)`: An estimated ratio of words to tokens (e.g., 0.75 words per token). Used for approximating chunk boundaries. `WORD_TOKEN_RATE` is defined in `crawl4ai.config` (default value: 0.75).
            *   `apply_chunking (bool, default: True)`: If `True`, the input content is chunked before being sent to the LLM. If `False`, the entire content is sent (which might exceed token limits for large inputs).
            *   `force_json_response (bool, default: False)`: If `True` and `extraction_type` is "schema", instructs the LLM to strictly adhere to JSON output format.
            *   `**kwargs`: Passed to `ExtractionStrategy` and potentially to the underlying LLM API calls (e.g., `temperature`, `max_tokens` if not set in `llm_config`).
    *   4.1.4. Key Public Methods:
        *   `extract(self, url: str, content: str, *q, **kwargs) -> List[Dict[str, Any]]`:
            *   Description: Processes the input `content`. If `apply_chunking` is `True`, it first chunks the content using the specified `chunking_strategy` (or a default one if `LLMExtractionStrategy` manages it internally). Then, for each chunk (or the whole content if not chunked), it constructs a prompt based on `instruction` and/or `schema` and sends it to the configured LLM.
            *   Returns: `List[Dict[str, Any]]` - A list of dictionaries.
                *   If `extraction_type` is "block", each dictionary typically contains `{"index": int, "content": str, "tags": List[str]}`.
                *   If `extraction_type` is "schema", each dictionary is an instance of the extracted structured data, ideally conforming to the provided `schema`. If the LLM returns multiple JSON objects in a list, they are parsed and returned.
        *   `run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]`:
            *   Description: Processes a list of content `sections` in parallel (using `ThreadPoolExecutor`). Each section is passed to the `extract` method logic.
            *   Returns: `List[Dict[str, Any]]` - Aggregated list of results from processing all sections.
    *   4.1.5. `TokenUsage` Tracking:
        *   `total_usage (TokenUsage)`: [Read-only Public Attribute] - An instance of `TokenUsage` that accumulates the token counts (prompt, completion, total) from all LLM API calls made by this `LLMExtractionStrategy` instance.
        *   `usages (List[TokenUsage])`: [Read-only Public Attribute] - A list containing individual `TokenUsage` objects for each separate LLM API call made during the extraction process. This allows for detailed tracking of token consumption per call.

## 5. `ChunkingStrategy` Interface and Implementations

*   ### 5.1. Interface `ChunkingStrategy`
    *   5.1.1. Purpose: The `ChunkingStrategy` class, found in `crawl4ai.chunking_strategy`, is an abstract base class (ABC) that defines the interface for different content chunking algorithms. Chunking is used to break down large pieces of text or HTML into smaller, manageable segments, often before feeding them to an LLM or other processing steps.
    *   5.1.2. Key Abstract Methods:
        *   `chunk(self, content: str) -> List[str]`:
            *   Description: Abstract method that must be implemented by subclasses to split the input `content` string into a list of string chunks.
            *   Parameters:
                *   `content (str)`: The content to be chunked.
            *   Returns: `List[str]` - A list of content chunks.

*   ### 5.2. Class `RegexChunking`
    *   5.2.1. Purpose: Implements `ChunkingStrategy` by splitting content based on a list of regular expression patterns. It can also attempt to merge smaller chunks to meet a target `chunk_size`.
    *   5.2.2. Inheritance: `ChunkingStrategy`
    *   5.2.3. Initialization (`__init__`):
        *   5.2.3.1. Signature: `RegexChunking(patterns: Optional[List[str]] = None, chunk_size: Optional[int] = None, overlap: Optional[int] = None, word_token_ratio: Optional[float] = WORD_TOKEN_RATE, **kwargs)`
        *   5.2.3.2. Parameters:
            *   `patterns (Optional[List[str]], default: None)`: A list of regex patterns used to split the text. If `None`, defaults to paragraph-based splitting (`["\\n\\n+"]`).
            *   `chunk_size (Optional[int], default: None)`: The target token size for each chunk. If specified, the strategy will try to merge smaller chunks created by regex splitting to approximate this size.
            *   `overlap (Optional[int], default: None)`: The target token overlap between consecutive chunks when `chunk_size` is active.
            *   `word_token_ratio (Optional[float], default: WORD_TOKEN_RATE)`: The estimated ratio of words to tokens, used if `chunk_size` or `overlap` are specified. `WORD_TOKEN_RATE` is defined in `crawl4ai.config` (default value: 0.75).
            *   `**kwargs`: Additional keyword arguments.
    *   5.2.4. Key Public Methods:
        *   `chunk(self, content: str) -> List[str]`:
            *   Description: Splits the input `content` using the configured regex patterns. If `chunk_size` is set, it then merges these initial chunks to meet the target size with the specified overlap.

*   ### 5.3. Class `IdentityChunking`
    *   5.3.1. Purpose: A `ChunkingStrategy` that does not perform any actual chunking. It returns the input content as a single chunk in a list.
    *   5.3.2. Inheritance: `ChunkingStrategy`
    *   5.3.3. Initialization (`__init__`):
        *   5.3.3.1. Signature: `IdentityChunking(**kwargs)`
        *   5.3.3.2. Parameters:
            *   `**kwargs`: Additional keyword arguments.
    *   5.3.4. Key Public Methods:
        *   `chunk(self, content: str) -> List[str]`:
            *   Description: Returns the input `content` as a single-element list: `[content]`.

## 6. Defining Schemas for Extraction

*   6.1. Purpose: Schemas provide a structured way to define what data needs to be extracted from content and how it should be organized. This allows for consistent and predictable output from the extraction process.
*   6.2. Schemas for CSS/XPath/LXML-based Extraction (`JsonCssExtractionStrategy`, etc.):
    *   6.2.1. Format: These strategies use a dictionary-based JSON-like schema.
    *   6.2.2. Key elements: As detailed in section 3.2.4 for `JsonCssExtractionStrategy`:
        *   `name (str)`: Name of the schema.
        *   `baseSelector (str)`: CSS selector (for CSS strategies) or XPath expression (for XPath strategy) identifying the repeating parent elements.
        *   `fields (List[Dict[str, Any]])`: A list defining each field to extract. Each field definition includes:
            *   `name (str)`: Output key for the field.
            *   `selector (str)`: CSS/XPath selector relative to the `baseSelector` or parent "nested" element.
            *   `type (str)`: "text", "attribute", "html", "list", "nested".
            *   `attribute (str, Optional)`: Name of HTML attribute (if type is "attribute").
            *   `fields (List[Dict], Optional)`: For "list" (of objects) or "nested" types.
            *   `transform (str, Optional)`: e.g., "lowercase".
            *   `default (Any, Optional)`: Default value if not found.
*   6.3. Schemas for LLM-based Extraction (`LLMExtractionStrategy`):
    *   6.3.1. Format: `LLMExtractionStrategy` accepts schemas in two main formats when `extraction_type="schema"`:
        *   Pydantic models: The Pydantic model class itself.
        *   Dictionary: A Python dictionary representing a valid JSON schema.
    *   6.3.2. Pydantic Models:
        *   Definition: Users can define a Pydantic `BaseModel` where each field represents a piece of data to be extracted. Field types and descriptions are automatically inferred.
        *   Conversion: `LLMExtractionStrategy` internally converts the Pydantic model to its JSON schema representation (`model_json_schema()`) to guide the LLM.
    *   6.3.3. Dictionary-based JSON Schema:
        *   Structure: Users can provide a dictionary that conforms to the JSON Schema specification. This typically includes a `type: "object"` at the root and a `properties` dictionary defining each field, its type (e.g., "string", "number", "array", "object"), and optionally a `description`.
        *   Usage: This schema is passed to the LLM to instruct it on the desired output format.

## 7. Configuration with `CrawlerRunConfig`

*   7.1. Purpose: The `CrawlerRunConfig` class (from `crawl4ai.async_configs`) is used to configure the behavior of a specific `arun()` or `arun_many()` call on an `AsyncWebCrawler` instance. It allows specifying various runtime parameters, including the extraction and chunking strategies.
*   7.2. Key Attributes:
    *   `extraction_strategy (Optional[ExtractionStrategy], default: None)`:
        *   Purpose: Specifies the `ExtractionStrategy` instance to be used for processing the content obtained during the crawl. If `None`, no structured extraction beyond basic Markdown generation occurs (unless a default is applied by the crawler).
        *   Type: An instance of a class inheriting from `ExtractionStrategy`.
    *   `chunking_strategy (Optional[ChunkingStrategy], default: RegexChunking())`:
        *   Purpose: Specifies the `ChunkingStrategy` instance to be used for breaking down content into smaller pieces before it's passed to an `ExtractionStrategy` (particularly `LLMExtractionStrategy`).
        *   Type: An instance of a class inheriting from `ChunkingStrategy`.
        *   Default: An instance of `RegexChunking()` with its default parameters (paragraph-based splitting).

## 8. LLM-Specific Configuration and Models

*   ### 8.1. Class `LLMConfig`
    *   8.1.1. Purpose: The `LLMConfig` class (from `crawl4ai.async_configs`) centralizes configuration parameters for interacting with Large Language Models (LLMs) through various providers.
    *   8.1.2. Initialization (`__init__`):
        *   8.1.2.1. Signature:
            ```python
            class LLMConfig:
                def __init__(
                    self,
                    provider: str = DEFAULT_PROVIDER,
                    api_token: Optional[str] = None,
                    base_url: Optional[str] = None,
                    temperature: Optional[float] = None,
                    max_tokens: Optional[int] = None,
                    top_p: Optional[float] = None,
                    frequency_penalty: Optional[float] = None,
                    presence_penalty: Optional[float] = None,
                    stop: Optional[List[str]] = None,
                    n: Optional[int] = None,
                ): ...
            ```
        *   8.1.2.2. Parameters:
            *   `provider (str, default: DEFAULT_PROVIDER)`: Specifies the LLM provider and model, e.g., "openai/gpt-4o-mini", "ollama/llama3.3". `DEFAULT_PROVIDER` is "openai/gpt-4o-mini".
            *   `api_token (Optional[str], default: None)`: API token for the LLM provider. If `None`, the system attempts to read it from environment variables (e.g., `OPENAI_API_KEY`, `GEMINI_API_KEY`, `GROQ_API_KEY` based on provider). Can also be prefixed with "env:" (e.g., "env:MY_CUSTOM_LLM_KEY").
            *   `base_url (Optional[str], default: None)`: Custom base URL for the LLM API endpoint, for self-hosted or alternative provider endpoints.
            *   `temperature (Optional[float], default: None)`: Controls randomness in LLM generation. Higher values (e.g., 0.8) make output more random, lower (e.g., 0.2) more deterministic.
            *   `max_tokens (Optional[int], default: None)`: Maximum number of tokens the LLM should generate in its response.
            *   `top_p (Optional[float], default: None)`: Nucleus sampling parameter. An alternative to temperature; controls the cumulative probability mass of tokens considered for generation.
            *   `frequency_penalty (Optional[float], default: None)`: Penalizes new tokens based on their existing frequency in the text so far, decreasing repetition.
            *   `presence_penalty (Optional[float], default: None)`: Penalizes new tokens based on whether they have appeared in the text so far, encouraging new topics.
            *   `stop (Optional[List[str]], default: None)`: A list of sequences where the API will stop generating further tokens.
            *   `n (Optional[int], default: None)`: Number of completions to generate for each prompt.
    *   8.1.3. Helper Methods:
        *   `from_kwargs(kwargs: dict) -> LLMConfig`:
            *   Description: [Static method] Creates an `LLMConfig` instance from a dictionary of keyword arguments.
        *   `to_dict() -> dict`:
            *   Description: Converts the `LLMConfig` instance into a dictionary representation.
        *   `clone(**kwargs) -> LLMConfig`:
            *   Description: Creates a new `LLMConfig` instance as a copy of the current one, allowing specific attributes to be overridden with `kwargs`.

*   ### 8.2. Dataclass `TokenUsage`
    *   8.2.1. Purpose: The `TokenUsage` dataclass (from `crawl4ai.models`) is used to store information about the number of tokens consumed during an LLM API call.
    *   8.2.2. Fields:
        *   `completion_tokens (int, default: 0)`: The number of tokens generated by the LLM in the completion.
        *   `prompt_tokens (int, default: 0)`: The number of tokens in the prompt sent to the LLM.
        *   `total_tokens (int, default: 0)`: The sum of `completion_tokens` and `prompt_tokens`.
        *   `completion_tokens_details (Optional[dict], default: None)`: Provider-specific detailed breakdown of completion tokens, if available.
        *   `prompt_tokens_details (Optional[dict], default: None)`: Provider-specific detailed breakdown of prompt tokens, if available.

## 9. PDF Processing and Extraction

*   ### 9.1. Overview of PDF Processing
    *   9.1.1. Purpose: Crawl4ai provides specialized strategies to handle PDF documents, enabling the fetching of PDF content and subsequent extraction of text, images, and metadata. This allows PDFs to be treated as a primary content source similar to HTML web pages.
    *   9.1.2. Key Components:
        *   `PDFCrawlerStrategy`: For fetching/identifying PDF content.
        *   `PDFContentScrapingStrategy`: For processing PDF content using an underlying PDF processor.
        *   `NaivePDFProcessorStrategy`: The default logic for parsing PDF files.

*   ### 9.2. Class `PDFCrawlerStrategy`
    *   9.2.1. Purpose: An implementation of `AsyncCrawlerStrategy` specifically for handling PDF documents. It doesn't perform typical browser interactions but focuses on fetching PDF content and setting the appropriate response headers to indicate a PDF document, which then allows `PDFContentScrapingStrategy` to process it.
    *   9.2.2. Inheritance: `AsyncCrawlerStrategy` (from `crawl4ai.async_crawler_strategy`)
    *   9.2.3. Initialization (`__init__`):
        *   9.2.3.1. Signature: `PDFCrawlerStrategy(logger: Optional[AsyncLogger] = None)`
        *   9.2.3.2. Parameters:
            *   `logger (Optional[AsyncLogger], default: None)`: An optional logger instance for logging messages.
    *   9.2.4. Key Public Methods:
        *   `crawl(self, url: str, **kwargs) -> AsyncCrawlResponse`:
            *   Description: Fetches the content from the given `url`. If the content is identified as a PDF (either by URL extension or `Content-Type` header for remote URLs), it sets `response_headers={"Content-Type": "application/pdf"}` in the returned `AsyncCrawlResponse`. The `html` field of the response will contain a placeholder message as the actual PDF processing happens in the scraping strategy.
        *   `close(self) -> None`:
            *   Description: Placeholder for cleanup, typically does nothing in this strategy.
        *   `__aenter__(self) -> "PDFCrawlerStrategy"`:
            *   Description: Async context manager entry point.
        *   `__aexit__(self, exc_type, exc_val, exc_tb) -> None`:
            *   Description: Async context manager exit point, calls `close()`.

*   ### 9.3. Class `PDFContentScrapingStrategy`
    *   9.3.1. Purpose: An implementation of `ContentScrapingStrategy` designed to process PDF documents. It uses an underlying `PDFProcessorStrategy` (by default, `NaivePDFProcessorStrategy`) to extract text, images, and metadata from the PDF, then formats this information into a `ScrapingResult`.
    *   9.3.2. Inheritance: `ContentScrapingStrategy` (from `crawl4ai.content_scraping_strategy`)
    *   9.3.3. Initialization (`__init__`):
        *   9.3.3.1. Signature: `PDFContentScrapingStrategy(save_images_locally: bool = False, extract_images: bool = False, image_save_dir: Optional[str] = None, batch_size: int = 4, logger: Optional[AsyncLogger] = None)`
        *   9.3.3.2. Parameters:
            *   `save_images_locally (bool, default: False)`: If `True`, extracted images will be saved to the local filesystem.
            *   `extract_images (bool, default: False)`: If `True`, the strategy will attempt to extract images from the PDF.
            *   `image_save_dir (Optional[str], default: None)`: The directory where extracted images will be saved if `save_images_locally` is `True`. If `None`, a default or temporary directory might be used.
            *   `batch_size (int, default: 4)`: The number of PDF pages to process in parallel by the underlying `NaivePDFProcessorStrategy`.
            *   `logger (Optional[AsyncLogger], default: None)`: An optional logger instance.
    *   9.3.4. Key Attributes:
        *   `pdf_processor (NaivePDFProcessorStrategy)`: An instance of `NaivePDFProcessorStrategy` configured with the provided image and batch settings, used to do the actual PDF parsing.
    *   9.3.5. Key Public Methods:
        *   `scrape(self, url: str, html: str, **params) -> ScrapingResult`:
            *   Description: Takes a `url` (which can be a local file path or a remote HTTP/HTTPS URL pointing to a PDF) and processes it. The `html` parameter is typically a placeholder like "Scraper will handle the real work" as the content comes from the PDF file itself. It downloads remote PDFs to a temporary local file before processing.
            *   Returns: `ScrapingResult` containing the extracted PDF data, including `cleaned_html` (concatenated HTML of pages), `media` (extracted images), `links`, and `metadata`.
        *   `ascrape(self, url: str, html: str, **kwargs) -> ScrapingResult`:
            *   Description: Asynchronous version of `scrape`. Internally calls `scrape` using `asyncio.to_thread`.
    *   9.3.6. Internal Methods (Conceptual):
        *   `_get_pdf_path(self, url: str) -> str`:
            *   Description: If `url` is an HTTP/HTTPS URL, downloads the PDF to a temporary file and returns its path. If `url` starts with "file://", it strips the prefix and returns the local path. Otherwise, assumes `url` is already a local path. Handles download timeouts and errors.

*   ### 9.4. Class `NaivePDFProcessorStrategy`
    *   9.4.1. Purpose: The default implementation of `PDFProcessorStrategy` in Crawl4ai. It uses the PyPDF2 library (and Pillow for image processing) to parse PDF files, extract text content page by page, attempt to extract embedded images, and gather document metadata.
    *   9.4.2. Inheritance: `PDFProcessorStrategy` (from `crawl4ai.processors.pdf.processor`)
    *   9.4.3. Dependencies: Requires `PyPDF2` and `Pillow`. These are installed with the `crawl4ai[pdf]` extra.
    *   9.4.4. Initialization (`__init__`):
        *   9.4.4.1. Signature: `NaivePDFProcessorStrategy(image_dpi: int = 144, image_quality: int = 85, extract_images: bool = True, save_images_locally: bool = False, image_save_dir: Optional[Path] = None, batch_size: int = 4)`
        *   9.4.4.2. Parameters:
            *   `image_dpi (int, default: 144)`: DPI used when rendering PDF pages to images (if direct image extraction is not possible or disabled).
            *   `image_quality (int, default: 85)`: Quality setting (1-100) for images saved in lossy formats like JPEG.
            *   `extract_images (bool, default: True)`: If `True`, attempts to extract embedded images directly from the PDF's XObjects.
            *   `save_images_locally (bool, default: False)`: If `True`, extracted images are saved to disk. Otherwise, they are base64 encoded and returned in the `PDFPage.images` data.
            *   `image_save_dir (Optional[Path], default: None)`: If `save_images_locally` is True, this specifies the directory to save images. If `None`, a temporary directory (prefixed `pdf_images_`) is created and used.
            *   `batch_size (int, default: 4)`: The number of pages to process in parallel when using the `process_batch` method.
    *   9.4.5. Key Public Methods:
        *   `process(self, pdf_path: Path) -> PDFProcessResult`:
            *   Description: Processes the PDF specified by `pdf_path` page by page sequentially.
            *   Returns: `PDFProcessResult` containing metadata and a list of `PDFPage` objects.
        *   `process_batch(self, pdf_path: Path) -> PDFProcessResult`:
            *   Description: Processes the PDF specified by `pdf_path` by handling pages in parallel batches using a `ThreadPoolExecutor` with `max_workers` set to `batch_size`.
            *   Returns: `PDFProcessResult` containing metadata and a list of `PDFPage` objects, assembled in the correct page order.
    *   9.4.6. Internal Methods (Conceptual High-Level):
        *   `_process_page(self, page: PyPDF2PageObject, image_dir: Optional[Path]) -> PDFPage`: Extracts text, images (if `extract_images` is True), and links from a single PyPDF2 page object.
        *   `_extract_images(self, page: PyPDF2PageObject, image_dir: Optional[Path]) -> List[Dict]`: Iterates through XObjects on a page, identifies images, decodes them (handling FlateDecode, DCTDecode, CCITTFaxDecode, JPXDecode), and either saves them locally or base64 encodes them.
        *   `_extract_links(self, page: PyPDF2PageObject) -> List[str]`: Extracts URI actions from page annotations to get hyperlinks.
        *   `_extract_metadata(self, pdf_path: Path, reader: PyPDF2PdfReader) -> PDFMetadata`: Reads metadata from the PDF document information dictionary (e.g., /Title, /Author, /CreationDate).

*   ### 9.5. Data Models for PDF Processing
    *   9.5.1. Dataclass `PDFMetadata` (from `crawl4ai.processors.pdf.processor`)
        *   Fields:
            *   `title (Optional[str], default: None)`
            *   `author (Optional[str], default: None)`
            *   `producer (Optional[str], default: None)`
            *   `created (Optional[datetime], default: None)`
            *   `modified (Optional[datetime], default: None)`
            *   `pages (int, default: 0)`
            *   `encrypted (bool, default: False)`
            *   `file_size (Optional[int], default: None)`
    *   9.5.2. Dataclass `PDFPage` (from `crawl4ai.processors.pdf.processor`)
        *   Fields:
            *   `page_number (int)`
            *   `raw_text (str, default: "")`
            *   `markdown (str, default: "")`: Markdown representation of the page's text content, processed by `clean_pdf_text`.
            *   `html (str, default: "")`: HTML representation of the page's text content, processed by `clean_pdf_text_to_html`.
            *   `images (List[Dict], default_factory: list)`: List of image dictionaries. Each dictionary contains:
                *   `format (str)`: e.g., "png", "jpeg", "tiff", "jp2", "bin".
                *   `width (int)`
                *   `height (int)`
                *   `color_space (str)`: e.g., "/DeviceRGB", "/DeviceGray".
                *   `bits_per_component (int)`
                *   `path (str, Optional)`: If `save_images_locally` was True, path to the saved image file.
                *   `data (str, Optional)`: If `save_images_locally` was False, base64 encoded image data.
                *   `page (int)`: The page number this image was extracted from.
            *   `links (List[str], default_factory: list)`: List of hyperlink URLs found on the page.
            *   `layout (List[Dict], default_factory: list)`: List of dictionaries representing text layout elements, primarily: `{"type": "text", "text": str, "x": float, "y": float}`.
    *   9.5.3. Dataclass `PDFProcessResult` (from `crawl4ai.processors.pdf.processor`)
        *   Fields:
            *   `metadata (PDFMetadata)`
            *   `pages (List[PDFPage])`
            *   `processing_time (float, default: 0.0)`: Time in seconds taken to process the PDF.
            *   `version (str, default: "1.1")`: Version of the PDF processor strategy (e.g., "1.1" for current `NaivePDFProcessorStrategy`).

*   ### 9.6. Using PDF Strategies with `AsyncWebCrawler`
    *   9.6.1. Workflow:
        1.  Instantiate `AsyncWebCrawler`. The `crawler_strategy` parameter of `AsyncWebCrawler` should be set to an instance of `PDFCrawlerStrategy` if you intend to primarily crawl PDF URLs or local PDF files directly. If crawling mixed content where PDFs are discovered via links on HTML pages, the default `AsyncPlaywrightCrawlerStrategy` might be used initially, and then a PDF-specific scraping strategy would be applied when a PDF content type is detected.
        2.  In `CrawlerRunConfig`, set the `scraping_strategy` attribute to an instance of `PDFContentScrapingStrategy`. Configure this strategy with desired options like `extract_images`, `save_images_locally`, etc.
        3.  When `crawler.arun(url="path/to/document.pdf", config=run_config)` is called for a PDF URL or local file path:
            *   `PDFCrawlerStrategy` (if used) or the default crawler strategy fetches the file.
            *   `PDFContentScrapingStrategy.scrape()` is invoked. It uses its internal `NaivePDFProcessorStrategy` instance to parse the PDF.
            *   The extracted text, image data, and metadata are populated into the `CrawlResult` object (e.g., `result.markdown`, `result.media["images"]`, `result.metadata`).
    *   9.6.2. Example Snippet:
        ```python
        import asyncio
        from pathlib import Path
        from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, PDFCrawlerStrategy
        from crawl4ai.content_scraping_strategy import PDFContentScrapingStrategy
        from crawl4ai.processors.pdf import PDFContentScrapingStrategy # Corrected import path

        async def main():
            # Setup for PDF processing
            pdf_crawler_strategy = PDFCrawlerStrategy() # Use if directly targeting PDF URLs
            pdf_scraping_strategy = PDFContentScrapingStrategy(
                extract_images=True,
                save_images_locally=True,
                image_save_dir="./pdf_images_output" # Ensure this directory exists
            )
            Path("./pdf_images_output").mkdir(parents=True, exist_ok=True)

            # If crawling a website that links to PDFs, you might use the default crawler strategy
            # and rely on content-type detection to switch to PDFContentScrapingStrategy if needed.
            # For direct PDF URL:
            async with AsyncWebCrawler(crawler_strategy=pdf_crawler_strategy) as crawler:
                run_config = CrawlerRunConfig(scraping_strategy=pdf_scraping_strategy)
                # Example PDF URL (replace with a real one for testing)
                pdf_url = "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf"
                result = await crawler.arun(url=pdf_url, config=run_config)

                if result.success:
                    print(f"Successfully processed PDF: {result.url}")
                    if result.markdown:
                         print(f"Markdown content (first 500 chars): {result.markdown.raw_markdown[:500]}")
                    if result.media and result.media.images:
                        print(f"Extracted {len(result.media.images)} images.")
                        for img in result.media.images:
                            print(f"  - Image source/path: {img.src or img.path}, Page: {img.page}")
                    if result.metadata:
                        print(f"PDF Metadata: {result.metadata}")
                else:
                    print(f"Failed to process PDF: {result.url}, Error: {result.error_message}")

        # if __name__ == "__main__":
        #     asyncio.run(main())
        ```
```

---


## Data Extraction - Reasoning
Source: crawl4ai_extraction_reasoning_content.llm.md

Okay, I'm ready to generate the "Reasoning & Problem-Solving Framework" document for the `extraction` component, adhering strictly to the outline you provided and the principles for a "reasoning" document.

I will use the information from the previously shared code snippets and documentation to create illustrative examples, explain design rationale, and provide actionable advice.

Here is the `llm_reasoning_extraction.md` document:

```markdown
# Detailed Outline for crawl4ai - extraction Component

**Target Document Type:** reasoning
**Target Output Filename Suggestion:** `llm_reasoning_extraction.md`
**Library Version Context:** 0.6.0+ (based on provided code)
**Outline Generation Date:** 2024-05-24
---

# Mastering Data Extraction with Crawl4AI

## 1. Introduction: Why Structured Data Extraction Matters in Web Crawling
    * 1.1. The Value of Going Beyond Raw HTML: Turning Web Content into Actionable Data
        Web pages, in their raw HTML form, are designed for human consumption. While Crawl4AI excels at converting HTML to clean Markdown for LLMs, often the goal is to extract specific, structured pieces of information. This could be product prices, article headlines, author names, contact details, or any other data points that can be organized into a predictable format. Structured data is more readily usable for databases, APIs, analytics, training machine learning models, or feeding into other automated processes. Simply having the full HTML or Markdown isn't enough when you need to operate on discrete data fields.

    * 1.2. Common Challenges in Web Data Extraction (Dynamic content, varied structures, anti-scraping)
        Extracting data from the web isn't always straightforward. Common hurdles include:
        *   **Varied HTML Structures:** Websites change layouts, and even within a single site, different page types can have vastly different structures. A CSS selector that works today might break tomorrow.
        *   **Dynamic Content:** Much of the web's content is loaded via JavaScript after the initial HTML page. Extractors need to handle this, either by executing JS (as Crawl4AI's browser-based crawlers do) or by finding data in embedded JSON within `<script>` tags.
        *   **Anti-Scraping Measures:** Websites may employ techniques to deter or block automated scraping, requiring more sophisticated approaches.
        *   **Unstructured Data:** Sometimes, the data isn't neatly tagged. It might be buried in free-form text, requiring natural language understanding to identify and extract.
        *   **Scalability and Maintenance:** Writing and maintaining custom parsers for many sites can be a significant engineering effort.

    * 1.3. Crawl4AI's Approach: A Flexible, Strategy-Based Extraction Framework
        Crawl4AI tackles these challenges by offering a flexible and powerful extraction framework built around the concept of "strategies." This allows you to choose the best tool for the job, whether it's precise rule-based extraction or intelligent LLM-powered parsing.
        *   **`ExtractionStrategy` Interface:** This is the core. It defines a common contract for how extraction should happen. Crawl4AI provides several built-in strategies (CSS-based, XPath-based, Regex-based, LLM-based), and you can even implement your own for highly specialized needs. This promotes modularity  you can swap out extraction logic without changing your core crawling code.
        *   **`ChunkingStrategy` Interface:** Specifically for LLM-based extraction, this interface helps prepare content by breaking it into manageable pieces that fit within an LLM's context window. This is crucial for both performance and accuracy when dealing with large documents.
        *   **Balancing Rule-Based and LLM-Powered Extraction:** Crawl4AI doesn't force you into one paradigm. You can use fast and efficient CSS selectors for well-structured sites and then leverage the power of LLMs for complex, unstructured data, or even combine them in hybrid approaches. This flexibility is key to building robust and adaptable web data extraction pipelines.

## 2. Core Concepts in Crawl4AI Extraction
    * 2.1. The `ExtractionStrategy` Interface: Your Key to Custom Extraction
        *   2.1.1. Purpose: Why an interface? Promoting modularity and extensibility.
            The `ExtractionStrategy` interface (defined in `crawl4ai/extraction_strategy.py`) is a fundamental design choice in Crawl4AI. It establishes a common contract for all extraction methods. The primary benefit is **modularity**: your main crawling logic doesn't need to know the specifics of *how* data is extracted. It simply invokes the strategy, and the strategy handles the details. This makes your code cleaner and more maintainable.
            Furthermore, it promotes **extensibility**: if the built-in strategies don't fit your exact needs (e.g., you're dealing with a proprietary data format or a very unique web structure), you can create your own class that implements the `ExtractionStrategy` interface and plug it directly into Crawl4AI.

        *   2.1.2. Key Methods to Understand (Conceptual): `extract()` and `run()`.
            While you typically won't call these directly if using built-in strategies (Crawl4AI handles it), understanding their roles is important if you plan to create custom strategies:
            *   `extract(url: str, html_content: str, *args, **kwargs) -> List[Dict[str, Any]]`: This is the core method that every concrete strategy must implement. It takes the URL and HTML content (or pre-processed content like Markdown, depending on the `input_format` of the strategy) and returns a list of dictionaries, where each dictionary represents an extracted item.
            *   `run(url: str, sections: List[str], *args, **kwargs) -> List[Dict[str, Any]]`: This method is often used for strategies that process content in chunks (like `LLMExtractionStrategy`). It takes a list of content `sections` and typically calls `extract()` for each section, then aggregates the results. For simpler strategies that operate on the whole content at once, `run` might just call `extract` with the joined sections.

        *   2.1.3. When Would You Implement Your Own `ExtractionStrategy`?
            You'd consider creating a custom `ExtractionStrategy` in scenarios like:
            *   **Highly Specialized Data Sources:** If you're extracting data from a non-standard format (e.g., custom XML, binary files, or a very idiosyncratic HTML structure not well-suited for CSS/XPath/Regex).
            *   **Integrating Proprietary Extraction Logic:** If your organization has existing, specialized parsing libraries or algorithms you want to use within the Crawl4AI framework.
            *   **Advanced Performance Optimizations:** For extremely high-volume scraping of a specific site, you might develop a hyper-optimized parser that bypasses more general tools.
            *   **Unique Pre-processing or Post-processing:** If your extraction requires complex data transformations or enrichments beyond what the built-in strategies offer.

    * 2.2. The `ChunkingStrategy` Interface: Preparing Content for LLMs
        *   2.2.1. Why Chunking is Crucial for LLM-Based Extraction
            Large Language Models (LLMs) have a "context window"  a limit on the amount of text they can process at once (e.g., 4096, 8192, or even 128k+ tokens). If you feed an entire long webpage directly to an LLM for extraction:
            *   **Context Overflow:** The content might exceed the LLM's limit, leading to truncation and loss of information, or outright errors.
            *   **Reduced Accuracy:** Even if it fits, an LLM might struggle to find specific details in a very long, noisy document. Its attention can get diluted.
            *   **Higher Cost & Latency:** Processing more tokens means higher API costs (for paid models) and longer response times.
            Chunking addresses this by breaking down the input content into smaller, more focused segments, each of which can be processed by the LLM more effectively.

        *   2.2.2. How Chunking Strategies Work in Crawl4AI
            A `ChunkingStrategy` (defined in `crawl4ai/chunking_strategy.py`) is responsible for taking a single block of text (e.g., the Markdown content of a page) and dividing it into a list of smaller strings (chunks).
            *   The primary method is `chunk(document: str) -> List[str]`.
            *   The `LLMExtractionStrategy` then iterates over these chunks, sending each one (or a batch of them, depending on its internal logic) to the LLM for extraction. The results from each chunk are then typically aggregated.

        *   2.2.3. Overview of Built-in Chunking Strategies
            Crawl4AI provides a couple of ready-to-use chunking strategies:
            *   **`RegexChunking` (default for `LLMExtractionStrategy`):** This strategy (from `crawl4ai/chunking_strategy.py`) uses regular expressions to split text. By default, it might split by paragraphs or other common delimiters. It aims to create semantically meaningful chunks. This is often a good general-purpose choice.
                *   *When to use:* Good for text-heavy documents where paragraph or section breaks are meaningful.
            *   **`IdentityChunking`:** This strategy (from `crawl4ai/chunking_strategy.py`) doesn't actually do any chunking; it returns the input document as a single chunk.
                *   *When to use:*
                    *   When your input documents are already small enough to fit the LLM's context window.
                    *   When you have pre-processed your content into chunks *before* passing it to `LLMExtractionStrategy`.
                    *   When the LLM you're using has a very large context window and performs well on full documents for your specific task.

        *   2.2.4. When to Choose or Implement a Custom `ChunkingStrategy`.
            While the built-in chunkers are useful, you might need a custom `ChunkingStrategy` if:
            *   **Domain-Specific Document Structures:** Your content has unique structural elements that `RegexChunking` doesn't handle well (e.g., legal documents with numbered clauses, scripts with dialogue/scene breaks, log files).
            *   **Semantic Chunking Needs:** You require more sophisticated chunking based on semantic meaning rather than just regex patterns (though this can become complex and might involve NLP techniques within your custom chunker).
            *   **Fixed-Size Overlapping Chunks:** You want to implement a sliding window approach with precise control over chunk size and overlap, which might be beneficial for certain types_of information retrieval.
            *   **Table or List-Aware Chunking:** You need to ensure that tables or lists are not awkwardly split across chunks.

    * 2.3. Schema Definition: The Blueprint for Your Extracted Data
        *   2.3.1. Why a Well-Defined Schema is Essential
            A schema acts as a contract for your data. It defines:
            *   What pieces of information you expect to extract (the field names).
            *   The data type of each piece of information (e.g., string, integer, boolean, list, nested object).
            *   How to find each piece of information (e.g., CSS selector, XPath, or implied for LLM).
            Benefits include:
            *   **Consistency:** Ensures that extracted data always has the same structure, making it easier to process downstream.
            *   **Reliability:** Helps catch errors if a website's structure changes and a selector no longer works, or if an LLM fails to extract a required field.
            *   **Guidance:** For rule-based extractors, it provides the direct rules. For LLM-based extractors, it informs the LLM about the desired output structure, significantly improving the quality and predictability of results.
            *   **Validation:** Pydantic models, used with LLMs, offer automatic data validation.

        *   2.3.2. Defining Schemas for CSS/XPath/LXML Strategies (Dictionary-based)
            For strategies like `JsonCssExtractionStrategy`, `JsonXPathExtractionStrategy`, and `JsonLxmlExtractionStrategy`, the schema is a Python dictionary.
            *   **Structure:**
                ```python
                schema = {
                    "name": "MyExtractorName", # Optional: A name for your schema
                    "baseSelector": "div.product-item", # CSS selector for repeating items (e.g., products on a list page)
                    "fields": [
                        {
                            "name": "product_name",      # Name of the field in the output
                            "selector": "h2.product-title", # CSS/XPath selector relative to baseSelector (or page if no baseSelector)
                            "type": "text"             # "text", "attribute", "html", "nested", "list"
                        },
                        {
                            "name": "product_link",
                            "selector": "a.product-link",
                            "type": "attribute",
                            "attribute": "href"        # Name of the HTML attribute to extract (e.g., 'href' for links)
                        },
                        # ... more fields ...
                    ]
                }
                ```
            *   **Key Fields:**
                *   `baseSelector`: (Optional) If you're extracting a list of similar items (e.g., multiple products, articles), this selector targets the container element for each item. All field selectors will then be relative to this base element. If omitted, field selectors are relative to the whole document.
                *   `fields`: A list of dictionaries, each defining a field to extract.
                    *   `name`: The key for this field in the output JSON.
                    *   `selector`: The CSS selector or XPath expression to locate the data.
                    *   `type`:
                        *   `"text"`: Extracts the text content of the selected element.
                        *   `"attribute"`: Extracts the value of a specified HTML attribute (requires an additional `"attribute": "attr_name"` key).
                        *   `"html"`: Extracts the inner HTML of the selected element.
                        *   `"nested"`: Allows defining a sub-schema for extracting nested structured data (requires an additional `"fields": [...]` key, similar to the top-level fields).
                        *   `"list"`: Indicates that the selector is expected to return multiple elements, and the extraction logic (defined by sub-fields) should be applied to each. Often used with a nested `fields` definition.
            *   **Tips for Designing Dictionary-Based Schemas:**
                *   Be as specific as possible with your selectors to avoid ambiguity.
                *   Start with a simple schema and iteratively add more fields.
                *   Test your selectors in your browser's developer tools first.
                *   Use `baseSelector` for lists to keep field selectors concise and maintainable.
            *   **Example: Schema for extracting blog post titles and authors:**
                ```python
                blog_post_schema = {
                    "name": "BlogPostExtractor",
                    "baseSelector": "article.post",
                    "fields": [
                        {"name": "title", "selector": "h1.entry-title", "type": "text"},
                        {"name": "author", "selector": "span.author-name", "type": "text"},
                        {"name": "publication_date", "selector": "time.published-date", "type": "attribute", "attribute": "datetime"}
                    ]
                }
                ```

        *   2.3.3. Defining Schemas for `LLMExtractionStrategy` (Pydantic Models)
            When using `LLMExtractionStrategy` with `extraction_type="schema"` (the default), you provide a Pydantic model as the schema.
            *   **Advantages of Pydantic:**
                *   **Type Hints:** Clearly define the expected data type for each field.
                *   **Validation:** Pydantic automatically validates that the data extracted by the LLM conforms to your model's types and constraints. If not, it raises an error.
                *   **IDE Support:** Excellent autocompletion and type checking in modern IDEs.
                *   **Serialization:** Easy conversion to and from JSON.
            *   **How Pydantic Models Guide the LLM:** Crawl4AI internally converts your Pydantic model into a JSON schema representation, which is then included in the prompt to the LLM. This tells the LLM the exact structure and field names it should use in its JSON output.
            *   **Example: Pydantic model for product information:**
                ```python
                from pydantic import BaseModel, HttpUrl
                from typing import Optional, List

                class ProductInfo(BaseModel):
                    product_name: str
                    price: Optional[float]
                    description: str
                    image_urls: List[HttpUrl] = []
                    features: Optional[List[str]]
                ```
                When this model is used, the LLM will be instructed to return JSON objects that look like:
                ```json
                {
                  "product_name": "Awesome Laptop",
                  "price": 1299.99,
                  "description": "A very fast and light laptop.",
                  "image_urls": ["https://example.com/image1.jpg"],
                  "features": ["16GB RAM", "512GB SSD"]
                }
                ```

        *   2.3.4. Best Practices for Schema Design Across Strategy Types.
            *   **Be Specific with Field Names:** Use clear, descriptive names that reflect the data.
            *   **Start Simple:** Begin with a few key fields and expand as needed.
            *   **Handle Optional Data:** For fields that might not always be present, define them as optional in your Pydantic model (e.g., `Optional[str]`) or ensure your non-LLM logic handles missing elements gracefully (e.g., by providing default values or allowing `None`).
            *   **Consider Data Types:** Choose appropriate types (string, number, boolean, list, nested object) to ensure data integrity.
            *   **Test Iteratively:** Regularly test your schemas with real web content to catch issues early.

## 3. Non-LLM Based Extraction Strategies: Precision and Speed
    * 3.1. When to Choose Non-LLM Strategies
        Non-LLM (or rule-based) strategies are excellent choices when:
        *   **Website Structure is Consistent:** The target website has a stable and predictable HTML structure. Changes are infrequent.
        *   **Performance is Key:** These strategies are generally much faster and less resource-intensive than LLM-based approaches as they don't involve API calls to external services or loading large models.
        *   **Cost is a Major Factor:** Non-LLM strategies have no per-extraction operational cost beyond your own compute resources.
        *   **Data Points are Simple and Directly Targetable:** You need to extract clearly identifiable pieces of text, attributes, or simple lists.
        *   **You Have Expertise in CSS Selectors or XPath:** If you or your team are comfortable writing and maintaining these selectors.
        *   **No Semantic Interpretation Needed:** The data can be located purely by its position or tags in the HTML, without needing to understand the meaning of the surrounding text.

    * 3.2. Mastering `JsonCssExtractionStrategy`
        *   3.2.1. Understanding Its Strengths: Leveraging CSS Selectors
            `JsonCssExtractionStrategy` is often the first choice for non-LLM extraction due to the widespread familiarity with CSS selectors.
            *   **Strengths:**
                *   Relatively easy to learn and write.
                *   Well-supported by browsers' developer tools for testing.
                *   Efficient for most common extraction tasks.
            *   **Underlying Library:** Crawl4AI typically uses BeautifulSoup4 or LXML for parsing HTML and applying CSS selectors, providing robust and performant parsing.

        *   3.2.2. Workflow: Extracting Data with CSS
            *   **Step 1: Analyzing the Target HTML Structure:**
                *   Use your browser's developer tools (e.g., "Inspect Element") to examine the HTML of the page you want to scrape.
                *   Identify the HTML tags, classes, and IDs that uniquely contain the data you need.
                *   Example: If you want to extract an article's title, you might find it's always within an `<h1>` tag with class `article-title`.
            *   **Step 2: Crafting your Dictionary-Based Schema with CSS Selectors:**
                *   Define your schema as a Python dictionary, as described in section 2.3.2.
                *   Fill in the `selector` for each field with the appropriate CSS selector.
                ```python
                article_schema = {
                    "baseSelector": "article.post", # Target each article
                    "fields": [
                        {"name": "title", "selector": "h1.entry-title", "type": "text"},
                        {"name": "author_link", "selector": "a.author-url", "type": "attribute", "attribute": "href"}
                    ]
                }
                ```
            *   **Step 3: Configuring `CrawlerRunConfig` to use `JsonCssExtractionStrategy`:**
                ```python
                from crawl4ai import CrawlerRunConfig
                from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

                extraction_strategy = JsonCssExtractionStrategy(schema=article_schema)
                run_config = CrawlerRunConfig(extraction_strategy=extraction_strategy)
                ```
            *   **Step 4: Interpreting the Results:**
                *   The `result.extracted_content` will be a JSON string containing a list of dictionaries, where each dictionary matches your schema.
                ```python
                import json
                # Assuming 'result' is the output from crawler.arun()
                if result.extracted_content:
                    data = json.loads(result.extracted_content)
                    for item in data:
                        print(f"Title: {item.get('title')}, Author Link: {item.get('author_link')}")
                ```

        *   3.2.3. Handling Nested Data Structures
            You can extract nested data by defining a field with `type: "nested"` and providing another `fields` list within it.
            *   **How to define:** The `selector` for the nested field targets the container of the nested data. The sub-fields' selectors are then relative to this nested container.
            *   **Example: Extracting comments and their authors:**
                ```python
                comment_schema = {
                    "baseSelector": "div.comment-thread",
                    "fields": [
                        {"name": "comment_id", "selector": "div.comment", "type": "attribute", "attribute": "data-comment-id"},
                        {
                            "name": "main_comment",
                            "selector": "div.comment-body", # Selector for the main comment container
                            "type": "nested",
                            "fields": [
                                {"name": "author", "selector": "span.comment-author", "type": "text"},
                                {"name": "text", "selector": "p.comment-text", "type": "text"}
                            ]
                        },
                        {
                            "name": "replies",
                            "selector": "div.reply", # Selector for each reply
                            "type": "list", # Indicates multiple replies
                            "fields": [ # Schema for each reply item
                                {"name": "reply_author", "selector": "span.reply-author", "type": "text"},
                                {"name": "reply_text", "selector": "p.reply-text", "type": "text"}
                            ]
                        }
                    ]
                }
                ```

        *   3.2.4. Extracting Lists of Items
            The `baseSelector` is key for extracting lists.
            *   **`baseSelector`:** Targets each individual item in the list (e.g., each `<li>` in a `<ul>`, each `div.product-card`).
            *   **Relative Field Selectors:** All selectors within the `fields` list are then evaluated *relative* to each element matched by `baseSelector`.
            *   **Example: Extracting a list of products from a category page:**
                ```python
                product_list_schema = {
                    "name": "ProductList",
                    "baseSelector": "div.product-listing div.product-item-container", # Each product card
                    "fields": [
                        {"name": "product_name", "selector": "h3.product-name a", "type": "text"},
                        {"name": "price", "selector": "span.price", "type": "text"},
                        {"name": "url", "selector": "h3.product-name a", "type": "attribute", "attribute": "href"}
                    ]
                }
                ```
                This would produce a list of product dictionaries.

        *   3.2.5. Code Example: Extracting News Headlines and Links from Hacker News (Illustrative)
            ```python
            import asyncio
            import json
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig
            from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
            from crawl4ai.cache_manager import CacheMode

            async def extract_hn_news():
                hn_schema = {
                    "name": "HackerNewsFrontPage",
                    "baseSelector": "tr.athing", # Each story row in Hacker News
                    "fields": [
                        {
                            "name": "rank",
                            "selector": "span.rank",
                            "type": "text"
                        },
                        {
                            "name": "title",
                            "selector": "span.titleline > a", # Get the first 'a' tag within titleline
                            "type": "text"
                        },
                        {
                            "name": "url",
                            "selector": "span.titleline > a",
                            "type": "attribute",
                            "attribute": "href"
                        },
                        # Example for next row (subtext) data - shows using a more complex relative selector
                        {
                            "name": "points",
                            "selector": "xpath=./following-sibling::tr[1]/td[@class='subtext']/span[@class='score']",
                            "type": "text" # Note: Using XPath within CSS strategy for advanced relative selection
                                           # This is a conceptual example; pure CSS might be trickier for direct sibling access.
                                           # A more common CSS approach would be to have a slightly broader baseSelector
                                           # or separate extraction steps if nesting is too complex for pure CSS.
                        }
                    ]
                }

                extraction_strategy = JsonCssExtractionStrategy(schema=hn_schema)
                browser_config = BrowserConfig(headless=True)
                run_config = CrawlerRunConfig(
                    extraction_strategy=extraction_strategy,
                    cache_mode=CacheMode.BYPASS # For fresh data in this example
                )

                async with AsyncWebCrawler(config=browser_config) as crawler:
                    result = await crawler.arun(
                        url="https://news.ycombinator.com/",
                        config=run_config
                    )

                if result.success and result.extracted_content:
                    articles = json.loads(result.extracted_content)
                    print(f"Extracted {len(articles)} articles from Hacker News:")
                    for i, article in enumerate(articles[:5]): # Print first 5
                        print(f"  {i+1}. {article.get('title')} ({article.get('points', 'N/A points')}) - {article.get('url')}")
                else:
                    print(f"Failed to extract articles: {result.error_message}")

            if __name__ == "__main__":
                asyncio.run(extract_hn_news())
            ```
            *Self-correction during thought process: The original `points` selector was a bit too complex for a pure CSS example within `JsonCssExtractionStrategy`. While some libraries might allow mixing, it's better to illustrate clear CSS or mention that for such relative sibling traversals, XPath might be more direct, or the schema/baseSelector might need restructuring.*

        *   3.2.6. Best Practices for Writing Robust CSS Selectors.
            *   **Prefer IDs if Stable:** `#unique-id` is usually the most robust if available and unique.
            *   **Use Specific but Not Overly Specific Classes:** `.meaningful-class` is good. Avoid overly long chains like `div.container > div.row > div.col-md-8 > article.post > h1` if `h1.post-title` is unique enough.
            *   **Attribute Selectors:** `input[name="email"]` can be very precise.
            *   **Avoid Relying on Order (unless necessary):** `:nth-child()` can be brittle if the page structure changes slightly. Use it sparingly.
            *   **Test Thoroughly:** Use browser dev tools to validate your selectors on various pages of the target site.

        *   3.2.7. Troubleshooting: Common Issues and Solutions
            *   **Selector Returning `None` or Empty List:**
                *   *Cause:* Selector is incorrect, element doesn't exist, or content is loaded dynamically *after* initial HTML.
                *   *Solution:* Double-check selector in dev tools. For dynamic content, ensure Crawl4AI's browser is rendering JS (default) or use `wait_for` in `CrawlerRunConfig`.
            *   **Handling Dynamic Class Names:**
                *   *Cause:* Sites using CSS-in-JS or frameworks might generate dynamic class names (e.g., `_header_a83hf8`).
                *   *Solution:* Look for stable parent elements or use attribute selectors that target parts of class names (e.g., `div[class*="header_"]`), or rely on structural selectors (e.g., `article > h1`). This is where XPath or LLM strategies might be more robust.
            *   **Extracting Incorrect Data:**
                *   *Cause:* Selector is too broad and matches multiple elements.
                *   *Solution:* Make your selector more specific. Use direct child `>` or adjacent sibling `+` combinators if appropriate.

    * 3.3. Leveraging `JsonXPathExtractionStrategy`
        *   3.3.1. When XPath Shines: Complex Selections and Navigating the DOM
            XPath (XML Path Language) is a powerful query language for selecting nodes from an XML or HTML document. It excels where CSS selectors might fall short:
            *   **Complex Relationships:** Selecting elements based on their ancestors, siblings, or preceding/following elements (e.g., "find the `div` that follows an `h2` with text 'Price'").
            *   **Text Content Matching:** Selecting elements based on their text content (e.g., `//button[contains(text(), 'Add to Cart')]`).
            *   **Navigating Up the DOM:** Easily selecting parent or ancestor elements.
            *   **Using Functions:** XPath has built-in functions for string manipulation, counting, etc.

        *   3.3.2. Key Differences from CSS Strategy (Syntax, capabilities).
            *   **Syntax:** XPath uses a path-like syntax (e.g., `/html/body/div[1]/h1`) whereas CSS uses selectors like `div.my-class > h1`.
            *   **Capabilities:** XPath is generally more powerful for traversing the DOM in complex ways. CSS is often simpler for common class/ID/tag selections.
            *   **Performance:** For simple selections, CSS can sometimes be faster. For complex traversals, a well-written XPath might be more efficient than a convoluted CSS equivalent. Crawl4AI uses LXML for XPath, which is highly performant.

        *   3.3.3. Workflow: Similar to CSS, but with XPath expressions.
            The workflow is identical to `JsonCssExtractionStrategy`, except your schema's `selector` fields will contain XPath expressions.
            *   **Step 1: Analyzing HTML:** Use browser developer tools. Many browsers allow you to right-click an element and "Copy XPath."
            *   **Step 2: Crafting your Dictionary-Based Schema with XPath:**
                ```python
                xpath_schema = {
                    "baseSelector": "//article[@class='blog-entry']", # XPath for each article
                    "fields": [
                        {"name": "title", "selector": ".//h1[contains(@class, 'title')]/text()", "type": "text"},
                        {"name": "author_url", "selector": ".//a[contains(@class, 'author-profile')]/@href", "type": "attribute"}
                        # Note: type "attribute" for XPath will get the attribute value if selector ends with /@attr
                        # type "text" will get text content. If selector selects an element, text() can be appended.
                    ]
                }
                ```
                *Important for XPath `type` handling:*
                *   If your XPath selector directly targets an attribute (e.g., `//a/@href`), `type: "attribute"` is redundant but harmless; the attribute value is returned.
                *   If your XPath selector targets an element and you want its text, use `type: "text"` (or append `/text()` to your XPath).
                *   If your XPath targets an element and you want an attribute of *that* element, you'd use `type: "attribute"` and specify the `attribute` key, e.g., `{"selector": "//img", "type": "attribute", "attribute": "src"}`.

            *   **Step 3: Configuration in `CrawlerRunConfig`:**
                ```python
                from crawl4ai.extraction_strategy import JsonXPathExtractionStrategy
                extraction_strategy = JsonXPathExtractionStrategy(schema=xpath_schema)
                run_config = CrawlerRunConfig(extraction_strategy=extraction_strategy)
                ```

        *   3.3.4. Code Example: Extracting Data Using XPath Functions (e.g., `contains()`, `text()`)
            ```python
            import asyncio
            import json
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig
            from crawl4ai.extraction_strategy import JsonXPathExtractionStrategy
            from crawl4ai.cache_manager import CacheMode

            async def extract_with_xpath():
                # Example HTML content
                sample_html = """
                <html><body>
                    <div class="product">
                        <h2>Product A</h2>
                        <span class="price">Price: $19.99</span>
                        <a href="/product/a" class="details-link">View Details</a>
                    </div>
                    <div class="product">
                        <h2>Product B</h2>
                        <span class="price">Price: $29.99</span>
                        <a href="/product/b" class="details-link">More Info</a>
                    </div>
                </body></html>
                """

                product_schema_xpath = {
                    "name": "ProductXPathExtractor",
                    "baseSelector": "//div[@class='product']",
                    "fields": [
                        {"name": "name", "selector": ".//h2/text()", "type": "text"},
                        # Extracts text after "Price: "
                        {"name": "price_value", "selector": "substring-after(.//span[contains(@class,'price')]/text(), 'Price: $')", "type": "text"},
                        {"name": "details_url", "selector": ".//a[contains(@class,'details-link') or contains(text(),'More Info')]/@href", "type": "attribute"}
                    ]
                }
                extraction_strategy = JsonXPathExtractionStrategy(schema=product_schema_xpath)
                run_config = CrawlerRunConfig(extraction_strategy=extraction_strategy, cache_mode=CacheMode.BYPASS)

                async with AsyncWebCrawler(config=BrowserConfig(headless=True)) as crawler:
                    # Using raw HTML input for this example
                    result = await crawler.arun(url=f"raw://{sample_html}", config=run_config)

                if result.success and result.extracted_content:
                    products = json.loads(result.extracted_content)
                    print("Extracted Products using XPath:")
                    for product in products:
                        print(product)
                else:
                    print(f"XPath extraction failed: {result.error_message}")

            if __name__ == "__main__":
                asyncio.run(extract_with_xpath())
            ```

        *   3.3.5. Tips for Effective XPath Usage.
            *   **Start with `.` for relative paths:** Within a `baseSelector`, field selectors should usually start with `./` to be relative to the current base element.
            *   **Use `text()` to get text content:** `//div/text()` gets the direct text children. `//div//text()` gets all text within the div.
            *   **Select attributes with `/@attribute_name`:** `//img/@src`.
            *   **Leverage functions:** `contains()`, `starts-with()`, `substring-after()`, `normalize-space()` are very useful.
            *   **Be mindful of namespaces** if working with XML-heavy HTML or actual XML.

    * 3.4. Understanding `JsonLxmlExtractionStrategy`
        The `JsonLxmlExtractionStrategy` is essentially a specialized version of `JsonCssExtractionStrategy` that explicitly uses the LXML library for parsing and CSS selection.
        *   3.4.1. Potential Performance Gains: When to consider it.
            LXML is known for its speed. For very large HTML documents or high-throughput scraping scenarios where parsing speed is a bottleneck, `JsonLxmlExtractionStrategy` *might* offer better performance than the default BeautifulSoup-backed CSS selector engine (though BeautifulSoup itself can use LXML as a parser). The actual difference can vary.
        *   3.4.2. Usage and Configuration: Similarities and differences with `JsonCssExtractionStrategy`.
            Usage is identical to `JsonCssExtractionStrategy`. You provide the same dictionary-based schema with CSS selectors. Crawl4AI handles the backend difference.
            ```python
            from crawl4ai.extraction_strategy import JsonLxmlExtractionStrategy # Import this
            
            # Schema is the same as for JsonCssExtractionStrategy
            my_schema = { ... } 
            extraction_strategy = JsonLxmlExtractionStrategy(schema=my_schema)
            run_config = CrawlerRunConfig(extraction_strategy=extraction_strategy)
            ```
        *   3.4.3. When to benchmark against `JsonCssExtractionStrategy`.
            If you suspect CSS selection is a performance bottleneck in your Crawl4AI application, and you're processing a large volume of pages or very large pages, it's worth benchmarking `JsonLxmlExtractionStrategy` against the default `JsonCssExtractionStrategy` to see if it provides a noticeable speedup in your specific environment and use case.

    * 3.5. Precise Targeting with `RegexExtractionStrategy`
        *   3.5.1. The Power of Regular Expressions: When Are They the Right Tool?
            Regular expressions are ideal when:
            *   **Data is in Unstructured or Semi-Structured Text:** The information isn't neatly tagged with specific HTML elements or classes (e.g., extracting an email address from a paragraph of text).
            *   **Targeting Specific Patterns:** You need to find data that conforms to a known pattern, like email addresses, phone numbers, dates, URLs, postal codes, UUIDs, product SKUs, etc.
            *   **HTML Structure is Unreliable:** If the HTML tags around the data change frequently, but the data itself has a consistent textual pattern.
            *   **Fallback or Augmentation:** Can be used to extract data that CSS/XPath selectors miss, or to clean/validate data extracted by other means.

        *   3.5.2. Utilizing Built-in Patterns
            `RegexExtractionStrategy` (from `crawl4ai.extraction_strategy`) comes with a handy `BuiltInPatterns` IntFlag enum. This allows you to easily enable common extraction patterns without writing the regex yourself.
            *   **Overview:** Refer to `RegexExtractionStrategy._B` (or `RegexExtractionStrategy.BuiltInPatterns` if aliased publicly) for the available flags like `EMAIL`, `PHONE_US`, `URL`, `IPV4`, `UUID`, `DATE_ISO`, `CURRENCY`, etc. Each flag corresponds to a pre-defined, tested regex pattern.
            *   **How to use:** You pass the bitwise OR of the desired patterns to the `pattern` argument of the `RegexExtractionStrategy` constructor.
            *   **Code Example: Extracting all email addresses and US phone numbers from a webpage's text:**
                ```python
                import asyncio
                import json
                from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
                from crawl4ai.extraction_strategy import RegexExtractionStrategy

                async def extract_contact_info():
                    # Combine built-in patterns
                    patterns_to_use = RegexExtractionStrategy.BuiltInPatterns.EMAIL | \
                                      RegexExtractionStrategy.BuiltInPatterns.PHONE_US
                    
                    extraction_strategy = RegexExtractionStrategy(pattern=patterns_to_use)
                    
                    # This strategy works best on plain text, so use 'markdown' or 'text' input_format
                    # if using with the standard crawler flow, or pass plain text directly.
                    run_config = CrawlerRunConfig(
                        extraction_strategy=extraction_strategy,
                        # input_format='text' # Alternative: let the strategy handle HTML to text
                    )

                    sample_text_content = """
                    Contact us at support@example.com or call (800) 555-1212.
                    Our sales team can be reached at sales@example.com.
                    For urgent matters, dial 1-800-555-1234.
                    Our website is https://example.com.
                    """

                    async with AsyncWebCrawler() as crawler:
                        # Here, we're directly using the 'extract' method for simplicity with raw text
                        # In a full crawl, you'd use crawler.arun() with the run_config
                        extracted_data = extraction_strategy.extract(
                            url="raw://text_content", # Dummy URL for raw content
                            html_content=sample_text_content # Provide text directly
                        )
                    
                    print("Extracted Contact Info:")
                    for item in extracted_data:
                        print(f"  Label: {item['label']}, Value: {item['value']}, Span: {item['span']}")

                if __name__ == "__main__":
                    asyncio.run(extract_contact_info())
                ```
                **Output structure for `RegexExtractionStrategy`:**
                Each extracted item is a dictionary:
                `{"url": "source_url", "label": "pattern_label", "value": "matched_string", "span": [start_index, end_index]}`

        *   3.5.3. Defining and Using Custom Regex Patterns
            If built-in patterns aren't sufficient, you can provide your own.
            *   **Passing a Dictionary:** Supply a dictionary where keys are labels (strings) for your patterns, and values are the regex pattern strings.
            *   **Tips for Writing Regex:**
                *   Use non-capturing groups `(?:...)` if you don't need to capture a part of the match.
                *   Be mindful of greediness (e.g., use `*?` or `+?` for non-greedy matches).
                *   Test your regex thoroughly with tools like regex101.com.
                *   Remember that regex patterns are raw strings in Python (e.g., `r"\b\d{5}\b"`).
            *   **Code Example: Extracting custom product SKUs (e.g., SKU-XXXX-YYYY):**
                ```python
                import asyncio
                import json
                from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
                from crawl4ai.extraction_strategy import RegexExtractionStrategy

                async def extract_skus():
                    custom_patterns = {
                        "product_sku": r"SKU-\d{4}-[A-Z]{4}"
                    }
                    extraction_strategy = RegexExtractionStrategy(custom=custom_patterns)
                    run_config = CrawlerRunConfig(extraction_strategy=extraction_strategy)

                    sample_text = "Product Alpha SKU-1234-ABCD and Product Beta SKU-5678-EFGH."
                    
                    # Direct usage for simplicity
                    extracted_data = extraction_strategy.extract(url="raw://text", html_content=sample_text)

                    print("Extracted SKUs:")
                    for item in extracted_data:
                        print(item)
                
                if __name__ == "__main__":
                    asyncio.run(extract_skus())
                ```

        *   3.5.4. Leveraging `generate_pattern()` for Dynamic Regex Creation
            The static method `RegexExtractionStrategy.generate_pattern(examples: List[str], labels: List[str] = None, llm_config: LLMConfig = None, **kwargs) -> str` (or Dict[str, str] if labels are provided) is a powerful utility that uses an LLM to generate a regex pattern for you based on examples.
            *   **How it Works:** You provide a list of example strings that you want to match. Optionally, you can provide corresponding labels if you want to generate multiple patterns for different types of data. The method then queries an LLM (configurable via `llm_config`) to infer a regex pattern that would capture those examples.
            *   **Use Cases:**
                *   You have a clear set of examples of the data you want to extract but are not a regex expert.
                *   You need to quickly prototype an extraction for a new data type.
                *   The pattern is complex, and you want an AI-assisted starting point.
            *   **Code Example: Generating a regex pattern from a list of example IDs:**
                ```python
                import asyncio
                from crawl4ai.extraction_strategy import RegexExtractionStrategy
                from crawl4ai import LLMConfig # Assuming LLMConfig is correctly imported

                async def generate_and_use_regex():
                    example_ids = ["ID_123_XYZ", "ID_456_ABC", "ID_789_DEF"]
                    
                    # Configure LLM for pattern generation (replace with your actual config)
                    # For open-source, set api_token=None or your specific setup
                    llm_for_regex = LLMConfig(provider="openai/gpt-3.5-turbo", api_token="YOUR_OPENAI_API_KEY") 
                                            # Or: provider="ollama/llama3", api_token=None

                    try:
                        # Generate a single pattern
                        generated_pattern_str = await RegexExtractionStrategy.generate_pattern(
                            examples=example_ids,
                            llm_config=llm_for_regex,
                            # Optional: Add a query to guide the LLM
                            query="Generate a regex to capture these types of IDs."
                        )
                        print(f"Generated Regex for IDs: {generated_pattern_str}")

                        # You can then use this generated_pattern_str in RegexExtractionStrategy:
                        # custom_patterns = {"custom_id": generated_pattern_str}
                        # strategy = RegexExtractionStrategy(custom=custom_patterns)
                        # ... then use the strategy ...

                        # Example for generating multiple labeled patterns
                        example_data = {
                            "order_id": ["ORD-001", "ORD-002"],
                            "user_id": ["USR_A", "USR_B"]
                        }
                        generated_patterns_dict = await RegexExtractionStrategy.generate_pattern(
                            examples=list(example_data.values()), # Pass lists of examples
                            labels=list(example_data.keys()),    # Corresponding labels
                            llm_config=llm_for_regex
                        )
                        print(f"Generated Labeled Regex Patterns: {generated_patterns_dict}")
                        # strategy_multi = RegexExtractionStrategy(custom=generated_patterns_dict)

                    except Exception as e:
                        print(f"Error generating pattern: {e}")
                        print("Make sure your LLMConfig is correctly set up and the LLM is accessible.")


                if __name__ == "__main__":
                    asyncio.run(generate_and_use_regex())
                ```
            *   **Limitations and Considerations:**
                *   **LLM Dependency:** Requires a configured and accessible LLM.
                *   **Quality Varies:** The quality of the generated regex depends on the LLM's capabilities and the quality/quantity of your examples.
                *   **Review and Test:** Always review and test LLM-generated regex patterns thoroughly before deploying them in production. They might be overly broad or miss edge cases.
                *   **Cost/Latency:** Involves an LLM call, so it's not for runtime pattern generation in a tight loop. Best used for one-off generation or infrequent updates.

        *   3.5.5. Best Practices for `RegexExtractionStrategy`.
            *   **Target Plain Text:** Regex works best on clean text. If applying to HTML, consider extracting text content first or using the `input_format="text"` or `input_format="markdown"` options in `LLMExtractionStrategy` if combining.
            *   **Be Specific:** Craft regex to be as specific as possible to avoid false positives.
            *   **Use Non-Capturing Groups:** `(?:...)` can improve performance if you don't need to capture certain parts of the match.
            *   **Test with Diverse Examples:** Ensure your regex works for various valid inputs and doesn't match invalid ones.

        *   3.5.6. Debugging Regex: Ensuring Accuracy and Avoiding Over-matching.
            *   **Online Regex Testers:** Use tools like regex101.com or pythex.org to build and test your patterns interactively with sample text.
            *   **Break Down Complex Patterns:** If a regex is very complex, test its components separately.
            *   **Log Matched Values:** During development, print out the `value` extracted by your regex to verify it's capturing what you intend.
            *   **Consider Edge Cases:** Think about variations in formatting, optional components, or unusual inputs that your regex might encounter.

## 4. LLM-Based Extraction Strategies: Handling Complexity and Ambiguity
    * 4.1. When to Turn to LLMs for Data Extraction
        LLM-based extraction strategies shine when:
        *   **Unstructured or Inconsistently Structured Content:** The data isn't in neat HTML tables or consistently tagged elements. It might be embedded in paragraphs, reviews, or forum posts.
        *   **Need for Semantic Understanding:** You need to extract information based on its meaning, not just its position or HTML tags (e.g., "What is the main sentiment of this review?" or "Extract the key arguments from this article.").
        *   **Rapid Prototyping:** When defining precise CSS/XPath selectors is too time-consuming or the site structure is unknown/volatile, an LLM can often get you started quickly with a descriptive prompt.
        *   **Extracting Nuanced Information:** For tasks like summarization, topic extraction, or identifying relationships between entities.
        *   **Schema Flexibility:** When the desired output structure is complex or might evolve, LLMs (especially with Pydantic schema guidance) can adapt more easily than hand-crafted rules.
        *   **Handling Diverse Sources:** If you need to extract similar information from many different websites with varying layouts, a well-crafted LLM prompt can be more generalizable than site-specific selectors.

    * 4.2. Deep Dive into `LLMExtractionStrategy`
        *   4.2.1. Core Idea: Instructing an LLM to be Your Extractor.
            The `LLMExtractionStrategy` (from `crawl4ai.extraction_strategy`) leverages the power of Large Language Models. Instead of writing explicit rules (like CSS selectors), you provide:
            1.  **Content:** The text (HTML, Markdown, or plain text) to extract from.
            2.  **Instruction:** A natural language prompt telling the LLM *what* to extract and *how* to structure it.
            3.  **(Optional but Recommended) Schema:** A Pydantic model defining the desired output structure, which helps the LLM produce consistent and validated JSON.
            The LLM then processes the content based on your instructions and attempts to return the data in the requested format.

        *   4.2.2. Configuring the LLM: The `LLMConfig` Object
            The `LLMConfig` object (from `crawl4ai.types` or `crawl4ai.async_configs`) is crucial for telling Crawl4AI which LLM to use and how to interact with it.
            ```python
            from crawl4ai import LLMConfig

            # Example for OpenAI
            openai_config = LLMConfig(
                provider="openai/gpt-4o-mini", # Or "openai/gpt-3.5-turbo", etc.
                api_token="sk-YOUR_OPENAI_API_KEY", # Best practice: use os.environ.get("OPENAI_API_KEY")
                # Optional parameters:
                # temperature=0.7, 
                # max_tokens=1024 
            )

            # Example for a local Ollama model
            ollama_config = LLMConfig(
                provider="ollama/llama3", # Assumes Ollama is running and llama3 model is pulled
                api_token=None, # Not needed for local Ollama by default
                base_url="http://localhost:11434" # Default Ollama API endpoint
            )

            # Example for Groq
            groq_config = LLMConfig(
                provider="groq/llama3-8b-8192",
                api_token=os.environ.get("GROQ_API_KEY")
            )
            ```
            *   **`provider` (str):** Specifies the LLM provider and model (e.g., `"openai/gpt-4o-mini"`, `"ollama/llama3"`, `"groq/llama3-8b-8192"`). Crawl4AI uses LiteLLM under the hood, supporting a wide range of models.
            *   **`api_token` (Optional[str]):** Your API key for the chosen provider. For local models like Ollama, this is often not needed.
                *   **Best Practice:** Store API keys in environment variables (e.g., `os.environ.get("OPENAI_API_KEY")`) rather than hardcoding them.
            *   **`base_url` (Optional[str]):** For self-hosted LLMs or providers with custom API endpoints (like local Ollama), specify the base URL of the API.
            *   **LLM Behavior Parameters:**
                *   `temperature` (Optional[float]): Controls randomness. Lower values (e.g., 0.2) make output more deterministic/focused; higher values (e.g., 0.8) make it more creative. For extraction, lower temperatures are usually preferred.
                *   `max_tokens` (Optional[int]): Maximum number of tokens to generate in the completion.
                *   `top_p` (Optional[float]): Nucleus sampling. An alternative to temperature.
                *   `frequency_penalty` (Optional[float]), `presence_penalty` (Optional[float]): Penalize new tokens based on their existing frequency or presence in the text so far, influencing topic diversity.
            *   **Choosing Parameters for Extraction:** For structured data extraction, you generally want the LLM to be factual and stick to the provided schema. Good starting points:
                *   `temperature`: 0.0 to 0.3
                *   `max_tokens`: Sufficient to cover your expected output size.

        *   4.2.3. The Art of the `instruction`: Guiding the LLM
            The `instruction` string you provide to `LLMExtractionStrategy` is critical. It's your primary way of telling the LLM what you want.
            *   **Why Clarity is Paramount:** LLMs are powerful but work best with clear, specific, and unambiguous instructions. Vague instructions lead to inconsistent or incorrect results.
            *   **Elements of a Good Extraction Instruction:**
                1.  **State the Goal Clearly:** "Extract the following information about each product..."
                2.  **Define Output Format (if not using a rigid schema for `extraction_type="block"`):** "Provide the output as a list of bullet points." or "Return a JSON object with keys 'name' and 'price'." (Though for JSON, using a Pydantic schema is better).
                3.  **Provide Examples (Few-Shot Prompting):** Show the LLM exactly what you mean. This is one of the most effective ways to improve accuracy.
                    ```
                    Instruction: "Extract the name and price from the text. Example:
                    Text: 'The SuperWidget costs $19.99 and is amazing.'
                    Output: {'name': 'SuperWidget', 'price': 19.99}"
                    ```
                4.  **Specify Handling of Missing/Ambiguous Data:** "If a price is not found, use null for the price field." or "If multiple authors are listed, return them as a list of strings."
                5.  **Be Concise but Complete:** Avoid unnecessary jargon, but ensure all critical details are present.
            *   **Examples: Good vs. Improvable Instructions:**
                *   *Improvable:* "Get product data."
                *   *Good:* "Extract the product name, price (as a float, omitting currency symbols), and a brief 2-sentence summary for each product listed in the provided HTML. If a price is not available, set the price field to null. Return the data as a list of JSON objects, each adhering to the Pydantic schema provided."

        *   4.2.4. Defining Your Target Output: `schema` (Pydantic Models) vs. `extraction_type="block"`
            `LLMExtractionStrategy` supports two main modes for `extraction_type`:
            *   **Schema-based Extraction (`extraction_type="schema"`, default):**
                *   **How it works:** You provide a Pydantic model to the `schema` parameter. Crawl4AI converts this model to a JSON schema and includes it in the prompt, instructing the LLM to format its output accordingly.
                *   **Benefits:**
                    *   **Structured Output:** Ensures the LLM returns data in a predictable, usable JSON format.
                    *   **Type Safety:** Pydantic validates the LLM's output against your defined types.
                    *   **Clarity:** Makes the desired output structure explicit to the LLM.
                *   **Code Example: Using a Pydantic model to extract author, title, and publication date from an article.**
                    ```python
                    from pydantic import BaseModel, Field
                    from typing import Optional
                    from datetime import date

                    class ArticleMeta(BaseModel):
                        title: str = Field(..., description="The main title of the article")
                        author: Optional[str] = Field(None, description="The primary author of the article")
                        publication_date: Optional[date] = Field(None, description="The date the article was published, in YYYY-MM-DD format")

                    # In LLMExtractionStrategy:
                    # llm_strategy = LLMExtractionStrategy(
                    #     llm_config=my_llm_config,
                    #     schema=ArticleMeta.model_json_schema(), # Pass the JSON schema representation
                    #     instruction="Extract article metadata according to the provided JSON schema.",
                    #     extraction_type="schema" 
                    # )
                    ```
                    *Self-correction: The `schema` parameter expects the JSON schema dictionary, not the Pydantic model class itself directly. `ArticleMeta.model_json_schema()` provides this.*
                    *(Further correction based on `crawl4ai/extraction_strategy.py` `LLMExtractionStrategy`): The `schema` parameter actually *can* take a Pydantic `BaseModel` type or a dictionary. The internal logic handles converting the Pydantic model to a JSON schema if needed. So, `schema=ArticleMeta` would also work, or even `schema=ArticleMeta.model_json_schema()`.*
                    For clarity and directness with Pydantic:
                    ```python
                    # Corrected usage for LLMExtractionStrategy with Pydantic
                    llm_strategy = LLMExtractionStrategy(
                        llm_config=my_llm_config,
                        schema=ArticleMeta, # Pass the Pydantic model class directly
                        instruction="Extract article metadata according to the provided Pydantic model structure.",
                        extraction_type="schema"
                    )
                    ```

            *   **Block-based Extraction (`extraction_type="block"`):**
                *   **When to use:** Useful when you want the LLM to identify and extract larger, coherent blocks of text rather than specific, fine-grained fields. Examples:
                    *   The main textual content of an article, excluding ads and sidebars.
                    *   All user reviews for a product.
                    *   A specific section of a long document based on a topic.
                *   **How it differs:** Instead of a rigid schema, your `instruction` guides the LLM on what kind of blocks to look for. The output will typically be a list of strings, where each string is an extracted block.
                *   **Code Example: Extracting all paragraphs discussing "environmental impact" from an article.**
                    ```python
                    # llm_strategy = LLMExtractionStrategy(
                    #     llm_config=my_llm_config,
                    #     instruction="Extract all paragraphs from the text that discuss the environmental impact of the product. Each paragraph should be a separate item in the output list.",
                    #     extraction_type="block" 
                    # )
                    ```
                    The `extracted_content` would then be a JSON string representing a list of text blocks, e.g., `["Paragraph 1 about impact...", "Another paragraph..."]`.

        *   4.2.5. Managing LLM Context: `ChunkingStrategy` in Action
            The `LLMExtractionStrategy` has two key parameters for controlling how it uses the `ChunkingStrategy`:
            *   **`chunk_token_threshold` (int, default from `config.CHUNK_TOKEN_THRESHOLD`):** This is the target maximum size (in tokens, roughly) for each chunk sent to the LLM. The `ChunkingStrategy` will try to create chunks that don't exceed this.
            *   **`overlap_rate` (float, default from `config.OVERLAP_RATE`):** This determines how much overlap there should be between consecutive chunks. An overlap (e.g., 0.1 for 10%) can help ensure that information at the boundaries of chunks isn't missed.
            *   **Strategies for Choosing Values:**
                *   Consult your LLM's documentation for its maximum context window size. Set `chunk_token_threshold` safely below this (e.g., 70-80% of the max).
                *   A small `overlap_rate` (e.g., 0.05 to 0.2) is often beneficial. Too much overlap increases redundant processing and cost.
                *   If your chosen `ChunkingStrategy` (like `RegexChunking` by paragraphs) naturally creates chunks much smaller than the `chunk_token_threshold`, the threshold might not be hit often, but it still acts as an upper bound.
            *   **Interaction with `ChunkingStrategy` implementations:**
                *   **`RegexChunking` (default for `LLMExtractionStrategy`):** It will first split the input document by its regex patterns (e.g., newlines, paragraphs). Then, it will try to merge these smaller pieces into chunks that are close to, but not exceeding, `chunk_token_threshold`, incorporating the `overlap_rate`.
                *   **`IdentityChunking`:** This strategy ignores `chunk_token_threshold` and `overlap_rate` and passes the content as a single chunk. Use this if your content is already appropriately sized or if your LLM handles very large contexts well for your task.
            *   **Code Example: Setting up chunking for a long article to be summarized by an LLM.**
                ```python
                from crawl4ai.chunking_strategy import RegexChunking
                # Assuming my_llm_config is defined
                
                # A chunker that aims for ~1500 token chunks with 10% overlap
                custom_chunker = RegexChunking(
                    # RegexChunking specific params can be set here if needed, 
                    # but LLMExtractionStrategy's params often suffice.
                )

                llm_summarizer_strategy = LLMExtractionStrategy(
                    llm_config=my_llm_config,
                    instruction="Summarize the following text block in 3 key bullet points.",
                    extraction_type="block", # We want blocks of summaries
                    chunking_strategy=custom_chunker, # Explicitly set if not default
                    chunk_token_threshold=1500, 
                    overlap_rate=0.1
                )
                ```

        *   4.2.6. Workflow Walkthrough:
            *   **Step 1: Define Your Extraction Goal and Target Schema/Output:**
                *   What specific information do you need? (e.g., product names, prices, features).
                *   If using `extraction_type="schema"`, create a Pydantic model.
                *   If using `extraction_type="block"`, define what characterizes a "block" you want.
            *   **Step 2: Configure `LLMConfig` and `LLMExtractionStrategy`:**
                *   Choose your LLM provider and model in `LLMConfig`.
                *   Set API keys and any custom `base_url`.
                *   Craft a clear `instruction` for `LLMExtractionStrategy`.
                *   Provide the `schema` (Pydantic model) or set `extraction_type="block"`.
                *   Configure `chunk_token_threshold`, `overlap_rate`, and select a `chunking_strategy` if the default isn't suitable.
            *   **Step 3: Integrate with `CrawlerRunConfig`:**
                ```python
                run_config = CrawlerRunConfig(
                    extraction_strategy=llm_strategy_instance,
                    # ... other run_config settings ...
                )
                ```
            *   **Step 4: Run the Crawl and Parse `extracted_content`:**
                ```python
                # result = await crawler.arun(url="...", config=run_config)
                # if result.success and result.extracted_content:
                #     try:
                #         extracted_data = json.loads(result.extracted_content)
                #         # Process extracted_data (which will be a list of dicts if schema-based,
                #         # or list of strings if block-based)
                #     except json.JSONDecodeError:
                #         print("LLM did not return valid JSON.")
                ```
            *   **Step 5: Analyze `TokenUsage`:**
                After the extraction (especially during development), inspect the `TokenUsage` object from the `LLMExtractionStrategy` instance to understand costs.
                ```python
                # llm_strategy_instance.show_usage() # Prints a summary
                # total_prompt_tokens = llm_strategy_instance.total_usage.prompt_tokens
                ```

        *   4.2.7. Code Example: Extracting Key Highlights from News Articles
            ```python
            import asyncio
            import json
            import os
            from pydantic import BaseModel, Field
            from typing import List, Optional
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig, LLMConfig
            from crawl4ai.extraction_strategy import LLMExtractionStrategy
            from crawl4ai.chunking_strategy import RegexChunking
            from crawl4ai.cache_manager import CacheMode

            class ArticleHighlight(BaseModel):
                highlight: str = Field(..., description="A key highlight or main point from the article.")
                category: Optional[str] = Field(None, description="A potential category for this highlight (e.g., Technology, Politics, Sports)")

            class ArticleHighlights(BaseModel):
                article_title: Optional[str] = Field(None, description="The main title of the article, if identifiable.")
                highlights: List[ArticleHighlight] = Field(..., description="A list of 3-5 key highlights from the article.")

            async def extract_article_highlights():
                # Ensure OPENAI_API_KEY is set in your environment
                if not os.getenv("OPENAI_API_KEY"):
                    print("OPENAI_API_KEY environment variable not set. Skipping LLM example.")
                    return

                llm_config = LLMConfig(
                    provider="openai/gpt-3.5-turbo", # More cost-effective for this example
                    api_token=os.getenv("OPENAI_API_KEY"),
                    temperature=0.2
                )

                extraction_strategy = LLMExtractionStrategy(
                    llm_config=llm_config,
                    schema=ArticleHighlights, # Pass the Pydantic model class
                    instruction="From the provided news article content, identify the main title and extract 3 to 5 key highlights. For each highlight, also try to assign a general category.",
                    extraction_type="schema",
                    chunking_strategy=RegexChunking(), # Default, but explicit here
                    chunk_token_threshold=2000, # Adjust based on article length and model
                    overlap_rate=0.1,
                    input_format="markdown" # LLMs often work well with clean Markdown
                )

                browser_config = BrowserConfig(headless=True, user_agent_mode="random") # Use a real user agent
                run_config = CrawlerRunConfig(
                    extraction_strategy=extraction_strategy,
                    cache_mode=CacheMode.BYPASS, # Fresh crawl for demo
                    word_count_threshold=50 # Ensure we have some content
                )

                # A news article known for having decent text content
                news_url = "https://www.nbcnews.com/tech/tech-news" 

                async with AsyncWebCrawler(config=browser_config) as crawler:
                    print(f"Crawling {news_url} to extract highlights...")
                    result = await crawler.arun(url=news_url, config=run_config)

                if result.success and result.extracted_content:
                    try:
                        data = json.loads(result.extracted_content)
                        # Since we expect a single ArticleHighlights object from the whole page
                        if isinstance(data, list) and len(data) > 0: 
                             # LiteLLM might wrap single objects in a list if schema is complex, take first.
                            article_data = ArticleHighlights.model_validate(data[0])
                        elif isinstance(data, dict):
                            article_data = ArticleHighlights.model_validate(data)
                        else:
                            raise ValueError("Unexpected data format from LLM")

                        print(f"\nExtracted Highlights for: {article_data.article_title or 'Unknown Title'}")
                        for hl_obj in article_data.highlights:
                            print(f"  - [{hl_obj.category or 'General'}] {hl_obj.highlight}")
                        
                        extraction_strategy.show_usage() # Show token usage

                    except (json.JSONDecodeError, ValueError) as e:
                        print(f"Error parsing LLM output: {e}")
                        print("Raw LLM output:", result.extracted_content)
                elif result.success and not result.extracted_content:
                    print("LLM extraction returned no content. The page might have been too short or content unsuitable.")
                else:
                    print(f"Failed to crawl or extract: {result.error_message}")

            if __name__ == "__main__":
                asyncio.run(extract_article_highlights())
            ```

        *   4.2.8. Understanding and Optimizing Costs: The `TokenUsage` Model
            When using LLMs, especially commercial APIs, tracking token usage is vital for cost management. The `TokenUsage` model (from `crawl4ai.models`) stores this information.
            *   **Fields:**
                *   `prompt_tokens` (int): Number of tokens in the input prompt sent to the LLM.
                *   `completion_tokens` (int): Number of tokens in the output generated by the LLM.
                *   `total_tokens` (int): Sum of prompt and completion tokens.
                *   `prompt_tokens_details`, `completion_tokens_details` (Optional[dict]): Provider-specific detailed token counts if available.
            *   **How to Interpret:** After an `LLMExtractionStrategy` run, you can access `strategy_instance.total_usage` for aggregated counts across all chunks/calls, or `strategy_instance.usages` for a list of `TokenUsage` objects per call.
                ```python
                # After running the strategy
                llm_strategy.show_usage() 
                # print(f"Total prompt tokens: {llm_strategy.total_usage.prompt_tokens}")
                # print(f"Total completion tokens: {llm_strategy.total_usage.completion_tokens}")
                ```
            *   **Strategies for Reducing Token Consumption:**
                1.  **Precise Prompts/Instructions:** Shorter, more focused prompts consume fewer tokens.
                2.  **Efficient Chunking:** Optimize `chunk_token_threshold` and `overlap_rate`. Avoid overly small chunks (too many API calls) or excessive overlap.
                3.  **Pre-filtering Content:** If possible, use non-LLM methods (CSS, XPath) to isolate the most relevant sections of HTML *before* sending to the LLM. Pass this cleaner, shorter text.
                4.  **Choose Smaller/Cheaper Models:** For simpler extraction tasks, a less powerful (and cheaper) model might suffice (e.g., GPT-3.5-turbo instead of GPT-4, or a smaller Llama variant).
                5.  **Limit `max_tokens` in `LLMConfig`:** If you know your expected output is short, set a reasonable `max_tokens` to prevent the LLM from generating overly verbose responses.
                6.  **Ask for Concise Output:** Instruct the LLM to be brief or to only return the specified fields.

        *   4.2.9. Best Practices for `LLMExtractionStrategy`
            *   **Iterative Prompt Refinement:** Start with a simple prompt and schema. Test it. Refine the prompt based on the LLM's output until you get the desired results. This is often a trial-and-error process.
            *   **Few-Shot Examples:** Including 2-3 examples of desired input/output *within your instruction* can dramatically improve LLM performance and adherence to your schema.
            *   **Specificity is Key:** The more specific your instruction and schema (especially field descriptions in Pydantic models), the better the LLM will understand your intent.
            *   **Model Selection:** Different LLMs excel at different tasks. Some are better at following complex instructions, others at creative generation. Experiment if results aren't optimal. For pure extraction into a schema, models fine-tuned for function calling or JSON mode are often best.
            *   **Handle Failures Gracefully:** LLM outputs can sometimes be unpredictable. Implement try-except blocks for JSON parsing and Pydantic validation. Consider fallback logic if extraction fails.
            *   **Use `input_format` Wisely:**
                *   `input_format="markdown"` (default for `LLMExtractionStrategy` if `CrawlerRunConfig.markdown_generator` is set): Good for general text extraction, as Markdown is cleaner than raw HTML.
                *   `input_format="html"`: Useful if the LLM needs to see HTML tags (e.g., for extracting attributes or if table structure is critical and Markdown conversion loses it).
                *   `input_format="text"`: For when you only care about the raw textual content.
                *   `input_format="fit_html"`: Uses a preprocessed HTML more suitable for schema extraction, usually smaller.

        *   4.2.10. Troubleshooting LLM Extraction:
            *   **LLM Not Following Instructions / Incorrect Format:**
                *   *Cause:* Prompt too vague, ambiguous, or complex. LLM might not support forced JSON mode well (though LiteLLM tries to handle this).
                *   *Solution:* Simplify prompt. Add clear examples (few-shot). Use a Pydantic schema to strongly guide JSON output. Try a different model. Ensure `force_json_response=True` in `LLMExtractionStrategy` if your provider supports it robustly or if you are using a Pydantic schema.
            *   **Incorrect or Incomplete Data:**
                *   *Cause:* Instruction missing details, LLM misunderstanding, content chunking splitting relevant info.
                *   *Solution:* Refine instruction. Check `chunk_token_threshold` and `overlap_rate`. Ensure field descriptions in Pydantic schema are clear.
            *   **Handling Hallucinations or Fabricated Information:**
                *   *Cause:* LLMs can sometimes "invent" data if it's not present or if the prompt is leading.
                *   *Solution:* Instruct the LLM to use `null` or a specific placeholder (e.g., "N/A") for missing fields. Lower the `temperature`. Validate extracted data against known facts if possible.
            *   **Schema Validation Errors (Pydantic):**
                *   *Cause:* LLM output doesn't match the Pydantic model's types or constraints.
                *   *Solution:* Check the LLM's raw JSON output. Refine the prompt to better match the schema. Make Pydantic fields `Optional` if data might be missing.
            *   **API Errors / Rate Limits:**
                *   *Cause:* Invalid API key, insufficient credits, hitting provider rate limits.
                *   *Solution:* Check API key and account status. Implement backoff/retry logic (Crawl4AI does some of this internally). Reduce request frequency.

## 5. Choosing Your Extraction Weapon: A Decision Guide
    * 5.1. Factors to Consider:
        *   **Structure and Consistency of Target Data:**
            *   *Well-structured, consistent HTML?* => Favor Non-LLM (CSS, XPath).
            *   *Messy, inconsistent, or unstructured text?* => Favor LLM.
        *   **Complexity of Information to be Extracted:**
            *   *Simple fields, direct attributes?* => Non-LLM.
            *   *Nuanced relationships, summaries, sentiment, inferred data?* => LLM.
        *   **Development Time vs. Runtime Cost:**
            *   *Quick prototype needed, site structure complex/unknown?* => LLM can be faster to start.
            *   *High volume, long-term, cost-sensitive?* => Non-LLM, once set up, is cheaper to run.
        *   **Need for Semantic Understanding vs. Pattern Matching:**
            *   *Data identifiable by patterns (emails, dates, SKUs)?* => `RegexExtractionStrategy`.
            *   *Data requires understanding context or meaning?* => LLM.
        *   **Scalability and Performance Requirements:**
            *   *Need to scrape thousands of pages per minute?* => Non-LLM strategies are inherently faster. LLM API calls add latency.
            *   *Occasional or smaller-scale extraction?* => LLM latency might be acceptable.
        *   **Maintainability:**
            *   *Site changes frequently?* => LLM prompts *might* be more resilient than specific CSS/XPath selectors, but both can break. Regex is often robust if the underlying text pattern is stable.
        *   **Team Expertise:**
            *   *Strong in CSS/XPath/Regex?* => Leverage those skills with Non-LLM.
            *   *More comfortable with natural language prompts?* => LLM might be a good fit.

    * 5.2. Decision Table: Non-LLM vs. LLM Strategies
        | Feature                | Non-LLM (CSS, XPath, Regex)                       | LLM-Based (`LLMExtractionStrategy`)            |
        | ---------------------- | ------------------------------------------------- | ---------------------------------------------- |
        | **Best For**           | Well-structured, consistent HTML; pattern matching | Unstructured/complex data; semantic understanding |
        | **Development Speed**  | Slower if selectors are complex; faster for regex | Faster for initial prototype with good prompts    |
        | **Runtime Speed**      | Very Fast                                         | Slower (API latency, model inference)        |
        | **Runtime Cost**       | Negligible (CPU/Mem)                              | Can be significant (API calls, GPU if local)   |
        | **Accuracy**           | High if selectors are good; precise for regex   | Depends on prompt, model, content quality    |
        | **Resilience to Change**| Brittle to HTML changes (CSS/XPath)               | Potentially more resilient; prompt dependent   |
        | **Complexity Handled** | Lower for semantic, higher for pattern (regex)    | High for semantic and complex relationships    |
        | **Schema Enforcement** | Via schema definition                             | Strong via Pydantic schema; flexible otherwise |

    * 5.3. Hybrid Approaches: Combining the Best of Both Worlds
        Often, the most robust and efficient solution involves a hybrid approach:
        *   **Example 1: CSS/XPath Pre-filtering for LLM:**
            Use CSS or XPath selectors to isolate the main content block of an article (e.g., `<article class="main-story">`). Pass only this cleaned, focused HTML/Markdown to the `LLMExtractionStrategy`.
            *   *Why?* Reduces the amount of text the LLM needs to process, saving tokens (cost/latency) and potentially improving accuracy by removing noise.
            ```python
            # Conceptual - how you might structure the thought process
            # 1. Use AsyncWebCrawler with a CrawlerRunConfig that only does basic scraping (no LLM extraction yet)
            #    and uses a css_selector to get the main content.
            # 2. Get the result.cleaned_html (which is now just the main content).
            # 3. Pass this cleaned_html to a separate LLMExtractionStrategy call.
            # (Crawl4AI doesn't directly support "chaining" strategies in one run_config,
            # so this would involve multiple processing steps orchestrated by your code.)
            ```
        *   **Example 2: Regex for Simple Entities, LLM for Complex:**
            Use `RegexExtractionStrategy` to quickly and cheaply pull out all email addresses, phone numbers, and dates. Then, use `LLMExtractionStrategy` on the remaining text (or the full text) to extract more nuanced information like "the primary topic of discussion" or "the relationship between person A and company B."
        *   **How to Implement Hybrid:** Typically, you would run the crawl in stages or have a custom orchestrator.
            1.  First pass: Use a non-LLM strategy (e.g., `JsonCssExtractionStrategy` to get specific blocks, or just rely on `result.markdown`).
            2.  Second pass: Take the output from the first pass and feed it to an `LLMExtractionStrategy` (or another non-LLM strategy). You might do this by invoking the `extract` method of the second strategy directly with the content from the first.

## 6. The `NoExtractionStrategy`: When You Just Need the HTML/Markdown
    * 6.1. Purpose: Disabling structured data extraction.
        The `NoExtractionStrategy` (from `crawl4ai.extraction_strategy`) is a placeholder strategy that, as its name suggests, performs no actual data extraction. `result.extracted_content` will be `None` or an empty representation.
    * 6.2. Use Cases:
        *   **Archiving Raw Web Content:** If your goal is simply to fetch and store the raw HTML or the cleaned Markdown of pages.
        *   **Markdown Generation is Primary:** If you're primarily using Crawl4AI for its HTML-to-Markdown conversion capabilities and don't need structured data beyond that.
        *   **Feeding to External Pipelines:** If you have a separate, downstream system that will handle the data extraction and you just need Crawl4AI to fetch and pre-process the web pages.
        *   **Baseline/Testing:** Useful as a baseline when developing or debugging other parts of your crawling pipeline.
    * 6.3. How to Configure It.
        ```python
        from crawl4ai import CrawlerRunConfig
        from crawl4ai.extraction_strategy import NoExtractionStrategy

        run_config_no_extraction = CrawlerRunConfig(
            extraction_strategy=NoExtractionStrategy()
        )
        # When crawler.arun(url="...", config=run_config_no_extraction) is called,
        # result.extracted_content will likely be None.
        # You would primarily use result.html or result.markdown.
        ```

## 7. Integrating Extraction into Your Crawls
    * 7.1. The Role of `CrawlerRunConfig`
        The `CrawlerRunConfig` object is central to customizing how each individual crawl operation behaves. For extraction, its key parameters are:
        *   **`extraction_strategy: Optional[ExtractionStrategy]`:** You assign an instance of your chosen extraction strategy here (e.g., `JsonCssExtractionStrategy(...)`, `LLMExtractionStrategy(...)`). If `None`, no structured extraction specific to this strategy is performed, but default behaviors like Markdown generation might still occur.
        *   **`chunking_strategy: Optional[ChunkingStrategy]`:** Primarily used by `LLMExtractionStrategy`. If you want to use a non-default chunker (other than `RegexChunking`), you instantiate it and assign it here.
        *   **`input_format` (within `LLMExtractionStrategy`):** While not directly in `CrawlerRunConfig`, the `LLMExtractionStrategy` itself takes an `input_format` parameter (`"markdown"`, `"html"`, `"text"`, `"fit_html"`) that determines what version of the page content is fed to the LLM. `CrawlerRunConfig`'s `markdown_generator` influences the quality of the Markdown available.

    * 7.2. Data Flow: From Web Page to Extracted Data
        Here's a simplified conceptual data flow:
        ```
        [Web Page URL]
             |
             v
        AsyncWebCrawler.arun(config=CrawlerRunConfig)
             |
             v
        [Browser Engine (Playwright)] -- Fetches HTML, executes JS --> [Raw HTML]
             |
             v
        CrawlerRunConfig.scraping_strategy (e.g., WebScrapingStrategy)
             |--> Cleans HTML --> [Cleaned HTML]
             |--> (Optional) Generates Markdown via CrawlerRunConfig.markdown_generator --> [Markdown]
             |--> Extracts Links, Basic Media --> [Links, Media Objects]
             |
             v
        (If LLMExtractionStrategy with chunking)
        CrawlerRunConfig.chunking_strategy / LLMExtractionStrategy.chunking_strategy
             |--> Chunks the input_format content (e.g., Markdown) --> [List of Text Chunks]
             |
             v
        CrawlerRunConfig.extraction_strategy (e.g., LLMExtractionStrategy or JsonCssExtractionStrategy)
             |--> Processes HTML/Markdown/Chunks --> [Structured Data (JSON String)]
             |
             v
        [CrawlResult]
             - .html (raw)
             - .cleaned_html
             - .markdown (object with .raw_markdown, .fit_markdown etc.)
             - .extracted_content (JSON string from extraction_strategy)
             - .links
             - .media
        ```

    * 7.3. Complete Code Example: A Full Crawl with a Chosen Extraction Strategy
        ```python
        import asyncio
        import json
        from crawl4ai import (
            AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, LLMConfig, CacheMode
        )
        from crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy
        from crawl4ai.chunking_strategy import RegexChunking
        from pydantic import BaseModel, Field
        from typing import List, Optional

        # Define a Pydantic schema for LLM extraction
        class NewsSummary(BaseModel):
            title: str = Field(description="The main headline of the news article.")
            summary_points: List[str] = Field(description="A list of 2-3 key bullet points summarizing the article.")

        async def comprehensive_extraction_example():
            # --- Configuration ---
            browser_config = BrowserConfig(
                headless=True,
                user_agent_mode="random" # Use a realistic user agent
            )

            # Non-LLM: CSS-based extraction schema for basic info
            basic_info_schema = {
                "name": "PageLinks",
                "baseSelector": "a[href]", # Get all links
                "fields": [
                    {"name": "text", "selector": "self", "type": "text"}, # 'self' refers to the baseSelector element
                    {"name": "href", "selector": "self", "type": "attribute", "attribute": "href"}
                ]
            }
            css_extraction_strategy = JsonCssExtractionStrategy(schema=basic_info_schema)

            # LLM-based extraction for summarization (ensure API key is set for OpenAI)
            llm_config_openai = LLMConfig(provider="openai/gpt-3.5-turbo", api_token=os.getenv("OPENAI_API_KEY"))
            if not llm_config_openai.api_token: # Fallback to a local/free model if no key
                print("Warning: OPENAI_API_KEY not found. LLM summarization might be skipped or use a different model if configured.")
                # Optionally, configure a fallback like Ollama here if you have it running
                # llm_config_ollama = LLMConfig(provider="ollama/llama2", base_url="http://localhost:11434")
                # llm_summarization_strategy = LLMExtractionStrategy(...) using llm_config_ollama
                llm_summarization_strategy = None # Or a NoExtractionStrategy
            else:
                llm_summarization_strategy = LLMExtractionStrategy(
                    llm_config=llm_config_openai,
                    schema=NewsSummary, # Use Pydantic model
                    instruction="Analyze the provided news article content (likely in Markdown). Extract its main title and provide 2-3 key summary bullet points.",
                    extraction_type="schema",
                    chunking_strategy=RegexChunking(), # Default, good for articles
                    chunk_token_threshold=1500,
                    input_format="markdown"
                )
            
            # --- Create CrawlerRunConfig ---
            # We'll demonstrate two runs: one with CSS, one with LLM
            run_config_css = CrawlerRunConfig(
                extraction_strategy=css_extraction_strategy,
                cache_mode=CacheMode.BYPASS
            )
            run_config_llm = CrawlerRunConfig(
                extraction_strategy=llm_summarization_strategy,
                cache_mode=CacheMode.BYPASS
            )

            target_url = "https://www.bbc.com/news" # Example news site

            # --- Execute Crawl ---
            async with AsyncWebCrawler(config=browser_config) as crawler:
                print(f"--- Running CSS Extraction on {target_url} ---")
                result_css = await crawler.arun(url=target_url, config=run_config_css)
                if result_css.success and result_css.extracted_content:
                    links = json.loads(result_css.extracted_content)
                    print(f"Found {len(links)} links. First 3:")
                    for link_data in links[:3]:
                        print(f"  Text: {link_data.get('text', '')[:30]}..., Href: {link_data.get('href')}")
                else:
                    print(f"CSS Extraction failed or no content: {result_css.error_message}")

                if llm_summarization_strategy: # Only run if LLM is configured
                    print(f"\n--- Running LLM Summarization on {target_url} (using its Markdown) ---")
                    # The LLM strategy will use the Markdown from the previous crawl result if input_format is markdown
                    # or it would re-fetch if it was a different format or strategy.
                    # For simplicity here, we assume the crawler internally handles content reuse or re-fetch as needed
                    # based on the input_format.
                    # A more explicit way would be to pass result_css.markdown to llm_summarization_strategy.extract()
                    
                    result_llm = await crawler.arun(url=target_url, config=run_config_llm)
                    if result_llm.success and result_llm.extracted_content:
                        try:
                            summary_data_list = json.loads(result_llm.extracted_content)
                            # LLM might return a list if it finds multiple "articles" or if schema is treated as listable
                            if summary_data_list and isinstance(summary_data_list, list):
                                summary_data = NewsSummary.model_validate(summary_data_list[0]) # Take first for demo
                                print(f"Title: {summary_data.article_title}")
                                print("Summary Points:")
                                for point in summary_data.summary_points:
                                    print(f"  - {point}")
                            elif summary_data_list and isinstance(summary_data_list, dict): # Single object returned
                                summary_data = NewsSummary.model_validate(summary_data_list)
                                print(f"Title: {summary_data.article_title}")
                                print("Summary Points:")
                                for point in summary_data.summary_points:
                                    print(f"  - {point}")

                        except (json.JSONDecodeError, Exception) as e: # Broader exception for Pydantic validation
                            print(f"Error parsing LLM summary output: {e}")
                            print("Raw LLM output:", result_llm.extracted_content)
                        llm_summarization_strategy.show_usage()
                    else:
                        print(f"LLM Summarization failed or no content: {result_llm.error_message}")
                else:
                    print("\nLLM Summarization strategy not configured, skipping that part.")


        if __name__ == "__main__":
            asyncio.run(comprehensive_extraction_example())
        ```

## 8. Specialized Extraction: Working with PDF Content
    * 8.1. Understanding PDF Processing in Crawl4AI
        Crawl4AI provides dedicated strategies for handling PDF documents, as PDFs are a common format for reports, papers, and other important web content. The key components are:
        *   **`PDFCrawlerStrategy` (in `crawl4ai.processors.pdf.__init__.py`):**
            *   **Role:** This strategy is used as the `crawler_strategy` in `AsyncWebCrawler` when you intend to directly process a PDF URL. It doesn't crawl HTML pages to find PDFs; rather, it's designed to fetch a document *known* to be a PDF (or a URL that might serve a PDF). It primarily handles the downloading of the PDF content. The actual parsing is delegated to a "scraping" strategy.
            *   It sets the `Content-Type` in the response headers to `application/pdf` to signal to subsequent strategies that this is PDF content.
        *   **`PDFContentScrapingStrategy` (in `crawl4ai.processors.pdf.__init__.py`):**
            *   **Role:** This strategy is used as the `scraping_strategy` in `CrawlerRunConfig` when you're targeting PDFs. It takes the raw PDF bytes (fetched by `PDFCrawlerStrategy` or provided directly) and processes them.
            *   **Leveraging `NaivePDFProcessorStrategy`:** Internally, `PDFContentScrapingStrategy` uses `NaivePDFProcessorStrategy` (from `crawl4ai.processors.pdf.processor`) to do the heavy lifting of PDF parsing.
            *   **`NaivePDFProcessorStrategy` (from `crawl4ai.processors.pdf.processor`):** This is the workhorse. It uses the PyPDF2 library (and Pillow for images) to extract:
                *   **Text Content:** Page by page.
                *   **Images:** Can extract embedded images.
                *   **Metadata:** Document properties like title, author, creation date.
            *   **Key Outputs in `ScrapingResult`:** When `PDFContentScrapingStrategy` is used, the `ScrapingResult` object (which is then available as `result.cleaned_html` or `result.markdown` to the `ExtractionStrategy`, and also structured in `result.metadata` and `result.media`) will be populated as follows:
                *   `result.cleaned_html`: Contains an HTML representation of the PDF content, with each page typically wrapped in a `<div class="pdf-page">`. Images might be embedded as base64 or linked if saved locally.
                *   `result.markdown`: A Markdown representation of the PDF text content (via `DefaultMarkdownGenerator` applied to the HTML from `cleaned_html`).
                *   `result.metadata`: A dictionary containing metadata extracted from the PDF, mirroring the `PDFMetadata` model (title, author, pages, etc.).
                *   `result.media`: Will contain image information under `media["images"]` if image extraction is enabled.

    * 8.2. Configuring PDF Extraction
        Configuration options are primarily set on the `PDFContentScrapingStrategy` (which passes them to `NaivePDFProcessorStrategy`).
        *   **`extract_images` (bool, default: `False`):** Set to `True` to attempt to extract images from the PDF. This can increase processing time.
        *   **`save_images_locally` (bool, default: `False`):** If `extract_images` is `True`, setting this to `True` will save extracted images to disk.
        *   **`image_save_dir` (Optional[str], default: `None`):** Specifies the directory to save images if `save_images_locally` is `True`. If `None`, a temporary directory might be used by `NaivePDFProcessorStrategy` (or it might use a default configured path if the strategy has one). It's best to provide an explicit path.
        *   **`image_dpi` (int, default: `144` in `NaivePDFProcessorStrategy`):** Dots Per Inch for rendered images (if PDF pages are rendered as images, which is not the primary mode of `NaivePDFProcessorStrategy`'s image extraction; it usually extracts existing embedded images. This DPI might be more relevant for future strategies that render pages). For `NaivePDFProcessorStrategy`, this DPI is used if it falls back to rendering pages as images, for example if direct image extraction fails or for specific image types.
        *   **`batch_size` (int, default: `4` in `NaivePDFProcessorStrategy`):** Controls how many pages are processed in parallel by worker threads when using `process_batch`. This can speed up processing of multi-page PDFs.

        ```python
        from crawl4ai.processors.pdf import PDFContentScrapingStrategy

        pdf_scraping_config = PDFContentScrapingStrategy(
            extract_images=True,
            save_images_locally=True,
            image_save_dir="./pdf_extracted_images", # Ensure this directory exists
            # image_dpi=200 # Higher DPI for better quality, larger files
            batch_size=8    # Process more pages in parallel
        )
        ```

    * 8.3. Workflow: Extracting Content from PDFs
        1.  **Set `PDFCrawlerStrategy` in `AsyncWebCrawler`:** This tells the crawler to use the PDF-specific fetching logic.
            ```python
            from crawl4ai.processors.pdf import PDFCrawlerStrategy
            # crawler = AsyncWebCrawler(crawler_strategy=PDFCrawlerStrategy())
            ```
        2.  **Set `PDFContentScrapingStrategy` in `CrawlerRunConfig`:** This tells the scraping phase to use the PDF parser.
            ```python
            # run_config = CrawlerRunConfig(scraping_strategy=pdf_scraping_config)
            ```
        3.  **Run the Crawl:**
            ```python
            # result = await crawler.arun(url="https://example.com/mydoc.pdf", config=run_config)
            ```
        4.  **Accessing Extracted Data:**
            *   **Text:** `result.markdown.raw_markdown` (often the most useful for LLMs) or iterate through `result.cleaned_html` to get page-specific HTML.
            *   **Metadata:** `result.metadata` will be a dictionary (e.g., `result.metadata.get("title")`). This comes from `PDFProcessResult.metadata`.
            *   **Images:** `result.media["images"]` will be a list of image dictionaries if `extract_images=True`. Each image dict might contain `src` (path if saved locally, or base64 data URI), `alt`, `page` (page number where image was found).

    * 8.4. Code Example: Crawling a PDF and Extracting its Text and Metadata
        ```python
        import asyncio
        import os
        from pathlib import Path
        from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
        from crawl4ai.processors.pdf import PDFCrawlerStrategy, PDFContentScrapingStrategy

        async def crawl_and_extract_pdf():
            # Example PDF URL (replace with a real, accessible PDF URL for testing)
            # For this example, let's assume a local PDF file to avoid network dependency.
            # Create a dummy PDF for testing if you don't have one handy
            # (Actual PDF creation is outside Crawl4AI scope, this is just for the example)
            
            # For a real URL:
            # pdf_url = "https://arxiv.org/pdf/1706.03762.pdf" # "Attention is All You Need" paper
            
            # For a local file:
            dummy_pdf_path = Path("dummy_test.pdf")
            if not dummy_pdf_path.exists():
                 try:
                    from reportlab.pdfgen import canvas
                    c = canvas.Canvas(str(dummy_pdf_path))
                    c.drawString(100, 750, "Hello World. This is page 1 of a dummy PDF.")
                    c.showPage()
                    c.drawString(100, 750, "This is page 2 with an important keyword: Crawl4AI.")
                    c.save()
                    print(f"Created dummy PDF: {dummy_pdf_path.resolve()}")
                 except ImportError:
                    print("reportlab not installed. Cannot create dummy PDF. Please provide a real PDF URL or local path.")
                    return

            pdf_url = f"file://{dummy_pdf_path.resolve()}"


            # Configure PDF processing
            pdf_image_output_dir = Path("./pdf_images_output")
            pdf_image_output_dir.mkdir(parents=True, exist_ok=True)

            pdf_scraping = PDFContentScrapingStrategy(
                extract_images=True, 
                save_images_locally=True, 
                image_save_dir=str(pdf_image_output_dir)
            )

            # Configure crawler run
            pdf_run_config = CrawlerRunConfig(
                scraping_strategy=pdf_scraping,
                cache_mode=CacheMode.BYPASS # Ensure fresh processing for demo
            )

            # Use PDFCrawlerStrategy for direct PDF handling
            # Note: BrowserConfig is less relevant here if directly fetching PDF, 
            # but AsyncWebCrawler still needs it.
            browser_cfg = BrowserConfig(headless=True) 
            
            async with AsyncWebCrawler(
                config=browser_cfg, 
                crawler_strategy=PDFCrawlerStrategy() # Crucial for PDF URLs
            ) as crawler:
                print(f"Processing PDF: {pdf_url}")
                result = await crawler.arun(url=pdf_url, config=pdf_run_config)

            if result.success:
                print("\n--- PDF Processing Successful ---")
                print(f"URL Processed: {result.url}")
                
                # Access metadata
                if result.metadata:
                    print("\nMetadata:")
                    print(f"  Title: {result.metadata.get('title', 'N/A')}")
                    print(f"  Author: {result.metadata.get('author', 'N/A')}")
                    print(f"  Pages: {result.metadata.get('pages', 'N/A')}")

                # Access text (via Markdown)
                if result.markdown:
                    print(f"\nMarkdown Content (first 300 chars):\n{result.markdown.raw_markdown[:300]}...")
                
                # Access images
                if result.media and result.media.get("images"):
                    print(f"\nExtracted {len(result.media['images'])} image(s):")
                    for img_info in result.media["images"]:
                        print(f"  - Src: {img_info.get('src', 'N/A')} (Page: {img_info.get('page', 'N/A')})")
                else:
                    print("\nNo images extracted or found.")
            else:
                print(f"\n--- PDF Processing Failed ---")
                print(f"Error: {result.error_message}")

            # Clean up dummy PDF
            if dummy_pdf_path.exists():
                # dummy_pdf_path.unlink() # Commented out to allow inspection
                print(f"Dummy PDF at {dummy_pdf_path.resolve()} can be manually deleted.")


        if __name__ == "__main__":
            asyncio.run(crawl_and_extract_pdf())
        ```

    * 8.5. When to Combine PDF Processing with Other Extraction Strategies
        The output of `PDFContentScrapingStrategy` (specifically `result.markdown.raw_markdown` or `result.cleaned_html`) can be fed into *another* `ExtractionStrategy` for more refined data extraction.
        *   **Using `LLMExtractionStrategy` on PDF Text:**
            *   *Why:* PDFs often contain unstructured text. An LLM can summarize, answer questions, or extract specific entities from the PDF's textual content.
            *   *How:*
                1.  Crawl the PDF using `PDFCrawlerStrategy` and `PDFContentScrapingStrategy`.
                2.  Take `result.markdown.raw_markdown`.
                3.  Instantiate an `LLMExtractionStrategy` with your desired schema and instruction.
                4.  Call `llm_strategy.extract(url=pdf_url, html_content=result.markdown.raw_markdown)` (using `html_content` as the parameter name, even though it's Markdown here, or ensure your LLM strategy is configured for `input_format="markdown"`).
        *   **Applying `RegexExtractionStrategy` to PDF Text:**
            *   *Why:* To find specific patterns (emails, phone numbers, case IDs, etc.) within the extracted text of the PDF.
            *   *How:* Similar to the LLM approach, use the text output from PDF processing as input to `RegexExtractionStrategy.extract()`.

## 9. Advanced Customization: Building Your Own Strategies
    * 9.1. Implementing a Custom `ExtractionStrategy`
        *   9.1.1. Why Create a Custom Strategy?
            *   **Unsupported Data Formats:** You're dealing with a data format Crawl4AI doesn't natively understand (e.g., custom binary formats, obscure XML dialects, non-standard text encodings that need special pre-processing).
            *   **Proprietary Internal APIs:** Your target data comes from an internal system with a unique API response structure that doesn't map well to JSON/CSS/XPath.
            *   **Highly Domain-Specific Logic:** The extraction rules are too complex or specific to your domain to be easily expressed with general-purpose selectors or even LLM prompts (e.g., extracting data from scientific diagrams based on their visual components, which might require CV models).
            *   **Performance-Critical Custom Parsing:** For extremely high-volume scraping of a single, known format, a hand-tuned parser might outperform general tools.

        *   9.1.2. Key Steps:
            1.  **Inherit from `ExtractionStrategy`:**
                ```python
                from crawl4ai.extraction_strategy import ExtractionStrategy, LLMConfig # Assuming LLMConfig is needed
                from typing import List, Dict, Any
                
                class MyCustomExtractionStrategy(ExtractionStrategy):
                    # ...
                ```
            2.  **Implement `__init__` (Optional but common):**
                To accept any configuration your strategy needs.
                ```python
                # def __init__(self, my_param: str, **kwargs):
                #     super().__init__(**kwargs) # Pass kwargs for base class (like input_format)
                #     self.my_param = my_param
                ```
            3.  **Implement the `extract` method:** This is the core logic.
                ```python
                # def extract(self, url: str, html_content: str, *q, **kwargs) -> List[Dict[str, Any]]:
                #     # Your custom parsing logic here
                #     # html_content will be whatever 'input_format' you specified (e.g., 'html', 'markdown')
                #     # or the raw content if not specified.
                #     processed_data = []
                #     # ... parse html_content ...
                #     # Example:
                #     # if "special_keyword" in html_content:
                #     #     processed_data.append({"url": url, "found_keyword": True, "snippet": html_content[:100]})
                #     return processed_data
                ```
            4.  **Implement `run` method (Optional):**
                The base `ExtractionStrategy.run` method simply takes a list of `sections` (chunks) and calls `self.extract` on their concatenation. You might override `run` if:
                *   You want to process chunks in parallel.
                *   Your strategy inherently works on chunks and needs to aggregate results differently.
                *   You need to manage state across chunk processing.
                ```python
                # async def run(self, url: str, sections: List[str], *q, **kwargs) -> List[Dict[str, Any]]:
                #     # Example: Process sections in parallel (conceptual, requires async/threading)
                #     all_results = []
                #     # In a real async scenario, you'd use asyncio.gather or similar
                #     for section in sections:
                #         # Note: self.extract is not async by default in base class. 
                #         # If your extract is I/O bound and async, you can await it.
                #         # Otherwise, use to_thread or a ThreadPoolExecutor for true parallelism.
                #         # For simplicity, this example is synchronous.
                #         all_results.extend(self.extract(url, section, **kwargs)) 
                #     return all_results
                ```
                *Note:* The base `ExtractionStrategy.run` is synchronous. If your custom `extract` method is I/O bound and you want true parallelism in `run`, you'll need to handle `asyncio` or threading appropriately. The `LLMExtractionStrategy` has a more complex `run` method for handling LLM calls.

        *   9.1.3. Simple Code Example: A Custom Strategy to Extract All `<meta>` Tags
            ```python
            import asyncio
            from bs4 import BeautifulSoup
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
            from crawl4ai.extraction_strategy import ExtractionStrategy
            from typing import List, Dict, Any

            class MetaTagExtractor(ExtractionStrategy):
                def __init__(self, **kwargs):
                    # This strategy will work on HTML
                    super().__init__(input_format="html", **kwargs)

                def extract(self, url: str, html_content: str, *q, **kwargs) -> List[Dict[str, Any]]:
                    if not html_content:
                        return []
                    
                    soup = BeautifulSoup(html_content, 'lxml') # Or 'html.parser'
                    meta_tags_data = []
                    for tag in soup.find_all('meta'):
                        meta_info = {"url": url, "attributes": dict(tag.attrs)}
                        if tag.get("name"):
                            meta_info["name"] = tag.get("name")
                        if tag.get("property"):
                            meta_info["property"] = tag.get("property")
                        if tag.get("content"):
                            meta_info["content"] = tag.get("content")
                        meta_tags_data.append(meta_info)
                    return meta_tags_data

            async def main_custom_meta_extractor():
                strategy = MetaTagExtractor()
                run_config = CrawlerRunConfig(extraction_strategy=strategy)
                
                async with AsyncWebCrawler() as crawler:
                    result = await crawler.arun(url="https://example.com", config=run_config)
                
                if result.success and result.extracted_content:
                    import json
                    meta_data = json.loads(result.extracted_content)
                    print(f"Extracted {len(meta_data)} meta tags:")
                    for tag_data in meta_data[:3]: # Print first 3
                        print(json.dumps(tag_data, indent=2))
                else:
                    print(f"Extraction failed: {result.error_message}")

            if __name__ == "__main__":
                asyncio.run(main_custom_meta_extractor())
            ```

    * 9.2. Implementing a Custom `ChunkingStrategy`
        *   9.2.1. When Default Chunkers Aren't Enough.
            *   **Domain-Specific Document Structures:** Your documents have clear semantic boundaries not easily captured by generic regex (e.g., chapters in a book, acts/scenes in a play, specific log entry formats).
            *   **Needing Semantic Boundaries:** You want to split text based on topic shifts or semantic coherence, which might require more advanced NLP techniques within your chunker (though this can be complex).
            *   **Table or List-Aware Chunking:** You have large tables or lists and want to ensure they are either kept whole within a chunk or split at sensible row/item boundaries, rather than arbitrarily in the middle of a cell or list item.
            *   **Fine-Grained Control Over Overlap:** You need a specific overlapping strategy (e.g., sentence-level overlap) not provided by the `overlap_rate` parameter of `LLMExtractionStrategy`.

        *   9.2.2. Key Steps:
            1.  **Inherit from `ChunkingStrategy`:**
                ```python
                from crawl4ai.chunking_strategy import ChunkingStrategy
                from typing import List
                
                class MyCustomChunker(ChunkingStrategy):
                    # ...
                ```
            2.  **Implement `__init__` (Optional):**
                To store any configuration for your chunker.
                ```python
                # def __init__(self, chunk_delimiter: str = "\n\n"):
                #     self.chunk_delimiter = chunk_delimiter
                ```
            3.  **Implement the `chunk` method:** This is where your custom chunking logic goes.
                ```python
                # def chunk(self, document: str) -> List[str]:
                #     # Your logic to split 'document' into a list of strings
                #     # Example:
                #     # return document.split(self.chunk_delimiter)
                #     pass
                ```

        *   9.2.3. Simple Code Example: A Chunking Strategy that Splits by `<h1>` Tags (assuming HTML input)
            This example demonstrates chunking HTML content. In practice, `LLMExtractionStrategy` usually receives Markdown or text, so you'd adapt this logic or ensure your `LLMExtractionStrategy.input_format` is set to `"html"`.
            ```python
            import asyncio
            from bs4 import BeautifulSoup
            from crawl4ai.chunking_strategy import ChunkingStrategy
            from crawl4ai.extraction_strategy import LLMExtractionStrategy # For context
            from crawl4ai import LLMConfig, CrawlerRunConfig, AsyncWebCrawler
            from typing import List

            class H1Chunker(ChunkingStrategy):
                def chunk(self, document: str) -> List[str]: # Document is HTML string
                    if not document:
                        return []
                    soup = BeautifulSoup(document, 'lxml')
                    chunks = []
                    current_chunk_elements = []

                    for element in soup.body.find_all(recursive=False) if soup.body else []:
                        if element.name == 'h1' and current_chunk_elements:
                            chunks.append("".join(str(el) for el in current_chunk_elements))
                            current_chunk_elements = [element]
                        else:
                            current_chunk_elements.append(element)
                    
                    if current_chunk_elements: # Add the last chunk
                        chunks.append("".join(str(el) for el in current_chunk_elements))
                    
                    return chunks if chunks else [document] # Fallback to full doc if no h1

            # Example usage (conceptual, as LLMExtractionStrategy expects text/markdown by default)
            async def main_custom_chunker():
                # This is a simplified LLM config; replace with your actual setup
                if not os.getenv("OPENAI_API_KEY"):
                    print("OPENAI_API_KEY not set. Skipping H1Chunker LLM example.")
                    return

                llm_config = LLMConfig(provider="openai/gpt-3.5-turbo", api_token=os.getenv("OPENAI_API_KEY"))
                
                # Note: We set input_format to 'html' for H1Chunker to receive HTML.
                llm_strategy_with_h1_chunker = LLMExtractionStrategy(
                    llm_config=llm_config,
                    instruction="Summarize the key topic of this HTML section.",
                    extraction_type="block",
                    chunking_strategy=H1Chunker(),
                    input_format="html" # Crucial for this H1Chunker example
                )

                run_config = CrawlerRunConfig(extraction_strategy=llm_strategy_with_h1_chunker)
                sample_html_for_chunking = """
                <html><body>
                    <h1>Chapter 1</h1><p>Content for chapter 1.</p><p>More content.</p>
                    <h1>Chapter 2</h1><p>Content for chapter 2.</p><div><p>Nested content.</p></div>
                    <h1>Chapter 3</h1><p>Final chapter content.</p>
                </body></html>
                """
                async with AsyncWebCrawler() as crawler:
                    result = await crawler.arun(url=f"raw://{sample_html_for_chunking}", config=run_config)
                
                if result.success and result.extracted_content:
                    import json
                    summaries = json.loads(result.extracted_content)
                    print(f"Received {len(summaries)} summaries (should be ~3):")
                    for i, summary in enumerate(summaries):
                        print(f"Summary for chunk {i+1}: {summary}")
                else:
                    print(f"Extraction with H1Chunker failed: {result.error_message}")

            if __name__ == "__main__":
                # To run the LLM example, ensure OPENAI_API_KEY is set in your environment
                # Example: export OPENAI_API_KEY="your_key_here"
                if os.getenv("OPENAI_API_KEY"):
                     asyncio.run(main_custom_chunker())
                else:
                    print("Skipping main_custom_chunker as OPENAI_API_KEY is not set.")

            ```

## 10. Best Practices for Robust and Efficient Extraction
    * 10.1. **Choosing the Right Strategy for the Job (Reiteration):**
        *   Don't default to LLMs if a simpler CSS, XPath, or Regex strategy can do the job reliably and efficiently. LLMs add cost and latency.
        *   Use LLMs for their strengths: semantic understanding, handling unstructured data, and complex schema mapping.
        *   Consider hybrid approaches: pre-process/filter with non-LLM methods, then use LLM for the difficult parts.
    * 10.2. **Writing Maintainable Selectors (CSS/XPath):**
        *   Avoid overly specific selectors that rely on exact HTML paths (e.g., `div > div > div > span`). These break easily.
        *   Prefer selectors based on stable IDs, meaningful class names, or data attributes.
        *   Keep selectors as simple and direct as possible.
        *   Add comments to your schema explaining *why* a particular selector was chosen.
    * 10.3. **Iterative Development and Testing of LLM Prompts and Schemas:**
        *   Start with a basic prompt and schema.
        *   Test on a few representative pages/content snippets.
        *   Analyze the LLM's output (and `TokenUsage`).
        *   Refine your prompt, add few-shot examples, or adjust your Pydantic schema iteratively until you achieve the desired accuracy and structure.
        *   Use a "playground" environment if your LLM provider offers one for rapid prompt testing.
    * 10.4. **Handling Site Changes Gracefully:**
        *   Websites change. Expect your selectors or even LLM prompts to break eventually.
        *   Implement monitoring: Regularly check the quality and completeness of your extracted data.
        *   Have a plan for updating selectors/prompts when breakages occur.
        *   Consider using more abstract selectors (e.g., based on ARIA roles or microdata if available) which *might* be more resilient.
    * 10.5. **Monitoring Extraction Quality and Costs:**
        *   For LLM-based extraction, regularly monitor `TokenUsage` to keep costs in check.
        *   Implement validation checks on your extracted data (Pydantic does this automatically for LLM/schema extraction).
        *   Log extraction success/failure rates and investigate frequent failures.
        *   Periodically sample extracted data to ensure ongoing quality.

## 11. Troubleshooting Common Extraction Issues
    * 11.1. **Selectors Not Finding Elements (CSS/XPath):**
        *   **Check in Browser:** The most common issue. Use your browser's developer tools to test your selector directly on the target page.
        *   **Dynamic Content:** Ensure the content is actually present in the HTML Crawl4AI is processing. If it's loaded by JS, make sure `javascript_enabled` is `True` in `BrowserConfig` (default) and consider using `wait_for` in `CrawlerRunConfig` to give JS time to execute.
        *   **Typos:** Double-check for typos in your selectors.
        *   **Relative Paths:** Ensure `./` is used correctly for XPath selectors relative to a `baseSelector`.
        *   **Shadow DOM:** CSS selectors generally don't pierce Shadow DOM. You might need to use JS execution to query within Shadow DOM elements.
    * 11.2. **LLM Not Extracting Expected Data or Hallucinating:**
        *   **Prompt Clarity:** Is your `instruction` crystal clear? Is it ambiguous?
        *   **Few-Shot Examples:** Add 2-3 high-quality examples to your prompt.
        *   **Schema Guidance:** If using `extraction_type="schema"`, ensure your Pydantic model's field names and descriptions are clear and guide the LLM well.
        *   **Model Choice:** Try a different LLM. Some models are better at instruction-following or JSON generation.
        *   **Temperature:** Lower the `temperature` in `LLMConfig` (e.g., to 0.0 or 0.1) for more deterministic output.
        *   **Content Chunking:** Is relevant information being split across chunks? Adjust `chunk_token_threshold` or `overlap_rate`.
        *   **Input Quality:** Is the input text (Markdown/HTML) clean and relevant? Pre-processing can help.
    * 11.3. **Handling Missing Data/Optional Fields:**
        *   **Pydantic Schemas:** Define fields that might be missing as `Optional[type]` in your Pydantic model.
        *   **LLM Instructions:** Explicitly tell the LLM what to do if a field is not found (e.g., "If the author is not mentioned, return null for the author field.").
        *   **Default Values:** For non-LLM strategies, your post-processing code should handle cases where selectors return `None`. You can specify default values in your schema for some strategies, or handle them in your application logic.
    * 11.4. **Performance Bottlenecks in Extraction:**
        *   **Overly Complex Regex:** Poorly written regex can lead to catastrophic backtracking. Optimize or simplify.
        *   **Inefficient CSS/XPath:** Very complex or broad selectors can be slow.
        *   **LLM Latency:** API calls to LLMs are inherently slower.
            *   Use smaller, faster models if acceptable.
            *   Optimize prompts and chunking to reduce token count.
            *   Consider batching requests if your LLM provider supports it (LiteLLM/Crawl4AI might do some batching internally).
        *   **Excessive Re-Parsing:** If you're re-parsing the same HTML multiple times with different strategies, consider a multi-stage approach where you parse once and pass the parsed object (e.g., BeautifulSoup soup) around. (Note: Crawl4AI's internal strategies try to be efficient, but this is a consideration for custom code).
    * 11.5. **Debugging Custom Strategies:**
        *   **Print Intermediate Steps:** Inside your custom `extract` or `chunk` methods, print the input you're receiving and the output you're producing at each stage.
        *   **Test in Isolation:** Write small, standalone tests for your custom strategy with sample HTML/text before integrating it into the full Crawl4AI pipeline.
        *   **Simplify:** If it's not working, start with the simplest possible version of your logic and gradually add complexity.
        *   **Leverage `self.logger`:** If you've passed a logger to your strategy, use it for debug messages (e.g., `if self.logger: self.logger.debug(...)`).

## 12. Conclusion: Unleashing the Power of Your Web Data
    * 12.1. Recap of Crawl4AI's Extraction Capabilities.
        Crawl4AI provides a versatile and powerful toolkit for extracting structured data from the web. Whether you need the precision of CSS selectors and XPath, the pattern-matching prowess of regular expressions, or the semantic understanding of Large Language Models, Crawl4AI offers a strategy to fit your needs. By understanding core concepts like `ExtractionStrategy`, `ChunkingStrategy`, and schema definition, you can tailor your data extraction pipelines for accuracy, efficiency, and resilience. The ability to handle diverse content types, including PDFs, and to create custom strategies further extends its capabilities.

    * 12.2. Encouragement for Building Sophisticated Data Pipelines.
        Don't be afraid to experiment and combine different strategies. The most robust solutions often involve a hybrid approach  perhaps using CSS to isolate key content areas, then regex to pull out simple entities, and finally an LLM to understand complex relationships or summarize nuanced text. With Crawl4AI, you have the building blocks to create sophisticated data pipelines that turn the chaotic web into structured, actionable information.

    * 12.3. Pointers to Further Reading (Deep Dives, API Reference, Community).
        *   **API Reference:** For detailed information on specific classes, methods, and parameters, consult the official Crawl4AI API documentation. ([Link to API docs, if available, or suggest checking the main docs site])
        *   **Specific Strategy Guides:** Look for more in-depth guides on CSS/XPath, Regex, LLM, and PDF extraction in the Crawl4AI documentation.
        *   **Community & GitHub:** Join the Crawl4AI community! Ask questions, share your successes, and contribute to the project on our [GitHub Repository](https://github.com/unclecode/crawl4ai). Check existing [GitHub Issues](https://github.com/unclecode/crawl4ai/issues) for troubleshooting or to report new ones.
        *   **Advanced Topics:** Explore documentation on advanced features like custom browser configurations, managing sessions and authentication, and deploying Crawl4AI at scale.
```

---


## Markdown Generation - Memory
Source: crawl4ai_markdown_memory_content.llm.md

Okay, I have processed the instructions and the provided information. I will now generate the detailed "Foundational Memory" document for the `markdown` component of `crawl4ai`, adhering strictly to the outline you've provided and focusing on factual API details.

```markdown
# Detailed Outline for crawl4ai - markdown Component

**Target Document Type:** memory
**Target Output Filename Suggestion:** `llm_memory_markdown.md`
**Library Version Context:** 0.6.3
**Outline Generation Date:** 2025-05-24
---

## 1. Introduction to Markdown Generation in Crawl4ai

*   1.1. Purpose: This section outlines the `markdown` component of the `crawl4ai` library. Its primary role is to convert HTML content, obtained during web crawling, into various Markdown formats. These formats are designed to be suitable for consumption by Large Language Models (LLMs), as well as for other applications requiring structured text from web pages.
*   1.2. Key Abstractions:
    *   `MarkdownGenerationStrategy`: An abstract base class that defines the interface for different markdown generation algorithms and approaches. This allows for customizable Markdown conversion processes.
    *   `DefaultMarkdownGenerator`: The standard, out-of-the-box implementation of `MarkdownGenerationStrategy`. It handles the conversion of HTML to Markdown, including features like link-to-citation conversion and integration with content filtering.
    *   `MarkdownGenerationResult`: A Pydantic data model that encapsulates the various outputs of the markdown generation process, such as raw markdown, markdown with citations, and markdown derived from filtered content.
    *   `CrawlerRunConfig.markdown_generator`: An attribute within the `CrawlerRunConfig` class that allows users to specify which instance of a `MarkdownGenerationStrategy` should be used for a particular crawl operation.
*   1.3. Relationship with Content Filtering: The markdown generation process can be integrated with `RelevantContentFilter` strategies. When a content filter is applied, it first refines the input HTML, and then this filtered HTML is used to produce a `fit_markdown` output, providing a more focused version of the content.

## 2. Core Interface: `MarkdownGenerationStrategy`

*   2.1. Purpose: The `MarkdownGenerationStrategy` class is an abstract base class (ABC) that defines the contract for all markdown generation strategies within `crawl4ai`. It ensures that any custom markdown generator will adhere to a common interface, making them pluggable into the crawling process.
*   2.2. Source File: `crawl4ai/markdown_generation_strategy.py`
*   2.3. Initialization (`__init__`)
    *   2.3.1. Signature:
        ```python
        class MarkdownGenerationStrategy(ABC):
            def __init__(
                self,
                content_filter: Optional[RelevantContentFilter] = None,
                options: Optional[Dict[str, Any]] = None,
                verbose: bool = False,
                content_source: str = "cleaned_html",
            ):
                # ...
        ```
    *   2.3.2. Parameters:
        *   `content_filter (Optional[RelevantContentFilter]`, default: `None`)`: An optional `RelevantContentFilter` instance. If provided, this filter will be used to process the HTML before generating the `fit_markdown` and `fit_html` outputs in the `MarkdownGenerationResult`.
        *   `options (Optional[Dict[str, Any]]`, default: `None`)`: A dictionary for strategy-specific custom options. This allows subclasses to receive additional configuration parameters. Defaults to an empty dictionary if `None`.
        *   `verbose (bool`, default: `False`)`: If `True`, enables verbose logging for the markdown generation process.
        *   `content_source (str`, default: `"cleaned_html"`)`: A string indicating the source of HTML to use for Markdown generation. Common values might include `"raw_html"` (original HTML from the page), `"cleaned_html"` (HTML after initial cleaning by the scraping strategy), or `"fit_html"` (HTML after being processed by `content_filter`). The actual available sources depend on the `ScrapingResult` provided to the markdown generator.
*   2.4. Abstract Methods:
    *   2.4.1. `generate_markdown(self, input_html: str, base_url: str = "", html2text_options: Optional[Dict[str, Any]] = None, content_filter: Optional[RelevantContentFilter] = None, citations: bool = True, **kwargs) -> MarkdownGenerationResult`
        *   Purpose: This abstract method must be implemented by concrete subclasses. It is responsible for taking an HTML string and converting it into various Markdown representations, encapsulated within a `MarkdownGenerationResult` object.
        *   Parameters:
            *   `input_html (str)`: The HTML string content to be converted to Markdown.
            *   `base_url (str`, default: `""`)`: The base URL of the crawled page. This is crucial for resolving relative URLs, especially when converting links to citations.
            *   `html2text_options (Optional[Dict[str, Any]]`, default: `None`)`: A dictionary of options to be passed to the underlying HTML-to-text conversion engine (e.g., `CustomHTML2Text`).
            *   `content_filter (Optional[RelevantContentFilter]`, default: `None`)`: An optional `RelevantContentFilter` instance. If provided, this filter is used to generate `fit_markdown` and `fit_html`. This parameter overrides any filter set during the strategy's initialization for this specific call.
            *   `citations (bool`, default: `True`)`: A boolean flag indicating whether to convert Markdown links into a citation format (e.g., `[text]^[1]^`) with a corresponding reference list.
            *   `**kwargs`: Additional keyword arguments to allow for future extensions or strategy-specific parameters.
        *   Returns: (`MarkdownGenerationResult`) An object containing the results of the Markdown generation, including `raw_markdown`, `markdown_with_citations`, `references_markdown`, and potentially `fit_markdown` and `fit_html`.

## 3. Default Implementation: `DefaultMarkdownGenerator`

*   3.1. Purpose: `DefaultMarkdownGenerator` is the standard concrete implementation of `MarkdownGenerationStrategy`. It provides a robust mechanism for converting HTML to Markdown, featuring link-to-citation conversion and the ability to integrate with `RelevantContentFilter` strategies for focused content output.
*   3.2. Source File: `crawl4ai/markdown_generation_strategy.py`
*   3.3. Inheritance: Inherits from `MarkdownGenerationStrategy`.
*   3.4. Initialization (`__init__`)
    *   3.4.1. Signature:
        ```python
        class DefaultMarkdownGenerator(MarkdownGenerationStrategy):
            def __init__(
                self,
                content_filter: Optional[RelevantContentFilter] = None,
                options: Optional[Dict[str, Any]] = None,
                # content_source parameter from parent is available
                # verbose parameter from parent is available
            ):
                super().__init__(content_filter, options, content_source=kwargs.get("content_source", "cleaned_html"), verbose=kwargs.get("verbose", False))
        ```
        *(Note: The provided code snippet for `DefaultMarkdownGenerator.__init__` does not explicitly list `verbose` and `content_source`, but they are passed to `super().__init__` through `**kwargs` in the actual library code, so their effective signature matches the parent.)*
    *   3.4.2. Parameters:
        *   `content_filter (Optional[RelevantContentFilter]`, default: `None`)`: As defined in `MarkdownGenerationStrategy`.
        *   `options (Optional[Dict[str, Any]]`, default: `None`)`: As defined in `MarkdownGenerationStrategy`.
        *   `verbose (bool`, default: `False`)`: (Passed via `kwargs` to parent) As defined in `MarkdownGenerationStrategy`.
        *   `content_source (str`, default: `"cleaned_html"`)`: (Passed via `kwargs` to parent) As defined in `MarkdownGenerationStrategy`.
*   3.5. Key Class Attributes:
    *   3.5.1. `LINK_PATTERN (re.Pattern)`: A compiled regular expression pattern used to find Markdown links. The pattern is `r'!\[(.[^\]]*)\]\(([^)]*?)(?:\s*\"(.*)\")?\)'`.
*   3.6. Key Public Methods:
    *   3.6.1. `generate_markdown(self, input_html: str, base_url: str = "", html2text_options: Optional[Dict[str, Any]] = None, content_filter: Optional[RelevantContentFilter] = None, citations: bool = True, **kwargs) -> MarkdownGenerationResult`
        *   Purpose: Implements the conversion of HTML to Markdown. It uses `CustomHTML2Text` for the base conversion, handles link-to-citation transformation, and integrates with an optional `RelevantContentFilter` to produce `fit_markdown`.
        *   Parameters:
            *   `input_html (str)`: The HTML content to convert.
            *   `base_url (str`, default: `""`)`: Base URL for resolving relative links.
            *   `html2text_options (Optional[Dict[str, Any]]`, default: `None`)`: Options for the `CustomHTML2Text` converter. If not provided, it uses `self.options`.
            *   `content_filter (Optional[RelevantContentFilter]`, default: `None`)`: Overrides the instance's `content_filter` for this call.
            *   `citations (bool`, default: `True`)`: Whether to convert links to citations.
            *   `**kwargs`: Additional arguments (not currently used by this specific implementation beyond parent class).
        *   Core Logic:
            1.  Instantiates `CustomHTML2Text` using `base_url` and the resolved `html2text_options` (merged from method arg, `self.options`, and defaults).
            2.  Converts `input_html` to `raw_markdown` using the `CustomHTML2Text` instance.
            3.  If `citations` is `True`, calls `self.convert_links_to_citations(raw_markdown, base_url)` to get `markdown_with_citations` and `references_markdown`.
            4.  If `citations` is `False`, `markdown_with_citations` is set to `raw_markdown`, and `references_markdown` is an empty string.
            5.  Determines the active `content_filter` (parameter or instance's `self.content_filter`).
            6.  If an active `content_filter` exists:
                *   Calls `active_filter.filter_content(input_html)` to get a list of filtered HTML strings.
                *   Joins these strings with `\n` and wraps them in `<div>` tags to form `fit_html`.
                *   Uses a new `CustomHTML2Text` instance to convert `fit_html` into `fit_markdown`.
            7.  Otherwise, `fit_html` and `fit_markdown` are set to `None` (or empty strings based on implementation details).
            8.  Constructs and returns a `MarkdownGenerationResult` object with all generated Markdown variants.
    *   3.6.2. `convert_links_to_citations(self, markdown: str, base_url: str = "") -> Tuple[str, str]`
        *   Purpose: Transforms standard Markdown links within the input `markdown` string into a citation format (e.g., `[Link Text]^[1]^`) and generates a corresponding numbered list of references.
        *   Parameters:
            *   `markdown (str)`: The input Markdown string.
            *   `base_url (str`, default: `""`)`: The base URL used to resolve relative link URLs before they are added to the reference list.
        *   Returns: (`Tuple[str, str]`) A tuple where the first element is the Markdown string with links converted to citations, and the second element is a string containing the formatted list of references.
        *   Internal Logic:
            *   Uses the `LINK_PATTERN` regex to find all Markdown links.
            *   For each link, it resolves the URL using `fast_urljoin(base, url)` if `base_url` is provided and the link is relative.
            *   Assigns a unique citation number to each unique URL.
            *   Replaces the original link markup with the citation format (e.g., `[Text]^[Number]^`).
            *   Constructs a Markdown formatted reference list string.
*   3.7. Role of `CustomHTML2Text`:
    *   `CustomHTML2Text` is a customized version of an HTML-to-Markdown converter, likely based on the `html2text` library.
    *   It's instantiated by `DefaultMarkdownGenerator` to perform the core HTML to plain Markdown conversion.
    *   Its behavior is controlled by options passed via `html2text_options` in `generate_markdown` or `self.options` of the `DefaultMarkdownGenerator`. These options can include `body_width`, `ignore_links`, `ignore_images`, etc., influencing the final Markdown output. (Refer to `crawl4ai/html2text.py` for specific options).

## 4. Output Data Model: `MarkdownGenerationResult`

*   4.1. Purpose: `MarkdownGenerationResult` is a Pydantic `BaseModel` designed to structure and encapsulate the various Markdown outputs generated by any `MarkdownGenerationStrategy`. It provides a consistent way to access different versions of the converted content.
*   4.2. Source File: `crawl4ai/models.py`
*   4.3. Fields:
    *   4.3.1. `raw_markdown (str)`: The direct result of converting the input HTML to Markdown, before any citation processing or specific content filtering (by the generator itself) is applied. This represents the most basic Markdown version of the content.
    *   4.3.2. `markdown_with_citations (str)`: Markdown content where hyperlinks have been converted into a citation style (e.g., `[Link Text]^[1]^`). This is typically derived from `raw_markdown`.
    *   4.3.3. `references_markdown (str)`: A string containing a formatted list of references (e.g., numbered list of URLs) corresponding to the citations found in `markdown_with_citations`.
    *   4.3.4. `fit_markdown (Optional[str]`, default: `None`)`: Markdown content generated from HTML that has been processed by a `RelevantContentFilter`. This version is intended to be more concise or focused on relevant parts of the original content. It is `None` if no content filter was applied or if the filter resulted in no content.
    *   4.3.5. `fit_html (Optional[str]`, default: `None`)`: The HTML content that remains after being processed by a `RelevantContentFilter`. `fit_markdown` is generated from this `fit_html`. It is `None` if no content filter was applied or if the filter resulted in no content.
*   4.4. Methods:
    *   4.4.1. `__str__(self) -> str`:
        *   Purpose: Defines the string representation of a `MarkdownGenerationResult` object.
        *   Signature: `__str__(self) -> str`
        *   Returns: (`str`) The content of the `raw_markdown` field.

## 5. Integration with Content Filtering (`RelevantContentFilter`)

*   5.1. Purpose of Integration: `DefaultMarkdownGenerator` allows integration with `RelevantContentFilter` strategies to produce a `fit_markdown` output. This enables generating Markdown from a version of the HTML that has been refined or focused based on relevance criteria defined by the filter (e.g., keywords, semantic similarity, or LLM-based assessment).
*   5.2. Mechanism:
    *   A `RelevantContentFilter` instance can be passed to `DefaultMarkdownGenerator` either during its initialization (via the `content_filter` parameter) or directly to its `generate_markdown` method. The filter passed to `generate_markdown` takes precedence if both are provided.
    *   When an active filter is present, `DefaultMarkdownGenerator.generate_markdown` calls the filter's `filter_content(input_html)` method. This method is expected to return a list of HTML string chunks deemed relevant.
    *   These chunks are then joined (typically with `\n` and wrapped in `<div>` tags) to form the `fit_html` string.
    *   This `fit_html` is then converted to Markdown using `CustomHTML2Text`, and the result is stored as `fit_markdown`.
*   5.3. Impact on `MarkdownGenerationResult`:
    *   If a `RelevantContentFilter` is successfully used:
        *   `MarkdownGenerationResult.fit_markdown` will contain the Markdown derived from the filtered HTML.
        *   `MarkdownGenerationResult.fit_html` will contain the actual filtered HTML string.
    *   If no filter is used, or if the filter returns an empty list of chunks (indicating no content passed the filter), `fit_markdown` and `fit_html` will be `None` (or potentially empty strings, depending on the exact implementation details of joining an empty list).
*   5.4. Supported Filter Types (High-Level Mention):
    *   `PruningContentFilter`: A filter that likely removes irrelevant HTML sections based on predefined rules or structural analysis (e.g., removing common boilerplate like headers, footers, navbars).
    *   `BM25ContentFilter`: A filter that uses the BM25 ranking algorithm to score and select HTML chunks based on their relevance to a user-provided query.
    *   `LLMContentFilter`: A filter that leverages a Large Language Model to assess the relevance of HTML chunks, potentially based on a user query or a general understanding of content importance.
    *   *Note: Detailed descriptions and usage of each filter strategy are covered in their respective documentation sections.*

## 6. Configuration via `CrawlerRunConfig`

*   6.1. `CrawlerRunConfig.markdown_generator`
    *   Purpose: This attribute of the `CrawlerRunConfig` class allows a user to specify a custom `MarkdownGenerationStrategy` instance to be used for the markdown conversion phase of a crawl. This provides flexibility in how HTML content is transformed into Markdown.
    *   Type: `MarkdownGenerationStrategy` (accepts any concrete implementation of this ABC).
    *   Default Value: If not specified, an instance of `DefaultMarkdownGenerator()` is used by default within the `AsyncWebCrawler`'s `aprocess_html` method when `config.markdown_generator` is `None`.
    *   Usage Example:
        ```python
        from crawl4ai import CrawlerRunConfig, DefaultMarkdownGenerator, AsyncWebCrawler
        from crawl4ai.content_filter_strategy import BM25ContentFilter
        import asyncio

        # Example: Configure a markdown generator with a BM25 filter
        bm25_filter = BM25ContentFilter(user_query="Python programming language")
        custom_md_generator = DefaultMarkdownGenerator(content_filter=bm25_filter)

        run_config_with_custom_md = CrawlerRunConfig(
            markdown_generator=custom_md_generator,
            # Other run configurations...
        )

        async def example_crawl():
            async with AsyncWebCrawler() as crawler:
                result = await crawler.arun(
                    url="https://en.wikipedia.org/wiki/Python_(programming_language)",
                    config=run_config_with_custom_md
                )
                if result.success and result.markdown:
                    print("Raw Markdown (snippet):", result.markdown.raw_markdown[:200])
                    if result.markdown.fit_markdown:
                        print("Fit Markdown (snippet):", result.markdown.fit_markdown[:200])
        
        # asyncio.run(example_crawl())
        ```

## 7. Influencing Markdown Output for LLM Consumption

*   7.1. Role of `DefaultMarkdownGenerator.options` and `html2text_options`:
    *   The `options` parameter in `DefaultMarkdownGenerator.__init__` and the `html2text_options` parameter in its `generate_markdown` method are used to pass configuration settings directly to the underlying `CustomHTML2Text` instance.
    *   `html2text_options` provided to `generate_markdown` will take precedence over `self.options` set during initialization.
    *   These options control various aspects of the HTML-to-Markdown conversion, such as line wrapping, handling of links, images, and emphasis, which can be crucial for preparing text for LLMs.
*   7.2. Key `CustomHTML2Text` Options (via `html2text_options` or `DefaultMarkdownGenerator.options`):
    *   `bodywidth (int`, default: `0` when `DefaultMarkdownGenerator` calls `CustomHTML2Text` for `raw_markdown` and `fit_markdown` if not otherwise specified): Determines the width for wrapping lines. A value of `0` disables line wrapping, which is often preferred for LLM processing as it preserves sentence structure across lines.
    *   `ignore_links (bool`, default: `False` in `CustomHTML2Text`): If `True`, all hyperlinks (`<a>` tags) are removed from the output, leaving only their anchor text.
    *   `ignore_images (bool`, default: `False` in `CustomHTML2Text`): If `True`, all image tags (`<img>`) are removed from the output.
    *   `ignore_emphasis (bool`, default: `False` in `CustomHTML2Text`): If `True`, emphasized text (e.g., `<em>`, `<strong>`) is rendered as plain text without Markdown emphasis characters (like `*` or `_`).
    *   `bypass_tables (bool`, default: `False` in `CustomHTML2Text`): If `True`, tables are not formatted as Markdown tables but are rendered as a series of paragraphs, which might be easier for some LLMs to process.
    *   `default_image_alt (str`, default: `""` in `CustomHTML2Text`): Specifies a default alt text for images that do not have an `alt` attribute.
    *   `protect_links (bool`, default: `False` in `CustomHTML2Text`): If `True`, URLs in links are not processed or modified.
    *   `single_line_break (bool`, default: `True` in `CustomHTML2Text`): If `True`, single newlines in HTML are converted to Markdown line breaks (two spaces then a newline). This can help preserve some formatting.
    *   `mark_code (bool`, default: `True` in `CustomHTML2Text`): If `True`, `<code>` and `<pre>` blocks are appropriately marked in Markdown.
    *   `escape_snob (bool`, default: `False` in `CustomHTML2Text`): If `True`, more aggressive escaping of special Markdown characters is performed.
    *   *Note: This list is based on common `html2text` options; refer to `crawl4ai/html2text.py` for the exact implementation and default behaviors within `CustomHTML2Text`.*
*   7.3. Impact of `citations (bool)` in `generate_markdown`:
    *   When `citations=True` (default in `DefaultMarkdownGenerator.generate_markdown`):
        *   Standard Markdown links `[text](url)` are converted to `[text]^[citation_number]^`.
        *   A `references_markdown` string is generated, listing all unique URLs with their corresponding citation numbers. This helps LLMs trace information back to its source and can reduce token count if URLs are long or repetitive.
    *   When `citations=False`:
        *   Links remain in their original Markdown format `[text](url)`.
        *   `references_markdown` will be an empty string.
        *   This might be preferred if the LLM needs to directly process the URLs or if the citation format is not desired.
*   7.4. Role of `content_source` in `MarkdownGenerationStrategy`:
    *   This parameter (defaulting to `"cleaned_html"` in `DefaultMarkdownGenerator`) specifies which HTML version is used as the primary input for the `generate_markdown` method.
    *   `"cleaned_html"`: Typically refers to HTML that has undergone initial processing by the `ContentScrapingStrategy` (e.g., removal of scripts, styles, and potentially some boilerplate based on the scraping strategy's rules). This is usually the recommended source for general Markdown conversion.
    *   `"raw_html"`: The original, unmodified HTML content fetched from the web page. Using this source would bypass any initial cleaning done by the scraping strategy.
    *   `"fit_html"`: This source is relevant when a `RelevantContentFilter` is used. `fit_html` is the HTML output *after* the `RelevantContentFilter` has processed the `input_html` (which itself is determined by `content_source`). If `content_source` is, for example, `"cleaned_html"`, then `fit_html` is the result of filtering that cleaned HTML. `fit_markdown` is then generated from this `fit_html`.
*   7.5. `fit_markdown` vs. `raw_markdown`/`markdown_with_citations`:
    *   `raw_markdown` (or `markdown_with_citations` if `citations=True`) is generated from the HTML specified by `content_source` (e.g., `"cleaned_html"`). It represents a general conversion of that source.
    *   `fit_markdown` is generated *only if* a `RelevantContentFilter` is active (either set in `DefaultMarkdownGenerator` or passed to `generate_markdown`). It is derived from the `fit_html` (the output of the content filter).
    *   **Choosing which to use for LLMs:**
        *   Use `fit_markdown` when you need a concise, highly relevant subset of the page's content tailored to a specific query or set of criteria defined by the filter. This can reduce noise and token count for the LLM.
        *   Use `raw_markdown` or `markdown_with_citations` when you need a more comprehensive representation of the page's textual content, or when no specific filtering criteria are applied.
```

---


## Markdown Generation - Reasoning
Source: crawl4ai_markdown_reasoning_content.llm.md

```markdown
# Detailed Outline for crawl4ai - markdown Component

**Target Document Type:** reasoning
**Target Output Filename Suggestion:** `llm_reasoning_markdown_generation.md`
**Library Version Context:** 0.6.3
**Outline Generation Date:** 2025-05-24
---

## 1. Introduction to Markdown Generation in Crawl4AI

*   1.1. **Why Markdown Generation Matters for LLMs**
    *   1.1.1. The role of clean, structured text for Large Language Model consumption.
        *   **Explanation:** LLMs perform significantly better when input data is well-structured and free of irrelevant noise (like HTML tags, scripts, or complex layouts not meant for textual understanding). Markdown, with its simple syntax, provides a human-readable and machine-parseable format that captures essential semantic structure (headings, lists, paragraphs, code blocks, tables) without the clutter of full HTML. This makes it easier for LLMs to understand the content's hierarchy, identify key information, and perform tasks like summarization, question-answering, or RAG (Retrieval Augmented Generation) more accurately and efficiently.
    *   1.1.2. Benefits of Markdown: readability, structure preservation, common format.
        *   **Explanation:**
            *   **Readability:** Markdown is designed to be easily readable in its raw form, making it simple for developers and users to inspect and understand the crawled content.
            *   **Structure Preservation:** It effectively preserves the semantic structure of the original HTML (headings, lists, emphasis, etc.), which is crucial context for LLMs.
            *   **Common Format:** Markdown is a widely adopted standard, ensuring compatibility with a vast ecosystem of tools, editors, and LLM input pipelines.
    *   1.1.3. How Crawl4AI's Markdown generation facilitates RAG and other LLM applications.
        *   **Explanation:** For RAG, Crawl4AI's Markdown output, especially when combined with content filtering, provides clean, relevant text chunks that can be easily embedded and indexed. This improves the quality of retrieved context for LLM prompts. For fine-tuning or direct prompting, the structured Markdown helps the LLM focus on the core content, leading to better quality responses and reducing token consumption by eliminating HTML overhead.

*   1.2. **Overview of Crawl4AI's Markdown Generation Pipeline**
    *   1.2.1. High-level flow: HTML -> (Optional Filtering) -> Markdown Conversion -> (Optional Citation Handling).
        *   **Explanation:**
            1.  **Input HTML:** The process starts with either raw HTML from the crawled page or a cleaned/selected HTML segment.
            2.  **Optional Content Filtering:** Before Markdown conversion, a `RelevantContentFilter` can be applied to the HTML. This step aims to remove boilerplate, ads, or irrelevant sections, resulting in `fit_html`. This is crucial for generating `fit_markdown`.
            3.  **Markdown Conversion:** The selected HTML (either the original, cleaned, or filtered `fit_html`) is converted into Markdown using an underlying `html2text` library, specifically `CustomHTML2Text` in Crawl4AI for enhanced control.
            4.  **Optional Citation Handling:** If enabled, inline links in the generated Markdown are converted to a citation format (e.g., `text [^1^]`), and a separate list of references is created.
    *   1.2.2. Key components involved: `MarkdownGenerationStrategy`, `DefaultMarkdownGenerator`, `CustomHTML2Text`, `RelevantContentFilter`.
        *   **Explanation:**
            *   **`MarkdownGenerationStrategy`:** An interface defining how Markdown should be generated. Allows for custom implementations.
            *   **`DefaultMarkdownGenerator`:** The standard implementation of `MarkdownGenerationStrategy`, using `CustomHTML2Text`. It orchestrates filtering (if provided) and citation handling.
            *   **`CustomHTML2Text`:** An enhanced version of the `html2text` library, providing fine-grained control over the HTML-to-Markdown conversion.
            *   **`RelevantContentFilter`:** An interface for strategies that filter HTML content before it's converted to Markdown, producing `fit_html` and consequently `fit_markdown`.
    *   1.2.3. How `CrawlerRunConfig` ties these components together.
        *   **Explanation:** The `CrawlerRunConfig` object allows you to specify which `MarkdownGenerationStrategy` (and by extension, which filters and `CustomHTML2Text` options) should be used for a particular crawl run via its `markdown_generator` parameter. This provides run-specific control over the Markdown output.

*   1.3. **Goals of this Guide**
    *   1.3.1. Understanding how to configure and customize Markdown output.
        *   **Explanation:** This guide will walk you through the various configuration options available, from choosing HTML sources and content filters to fine-tuning the `html2text` conversion itself.
    *   1.3.2. Best practices for generating LLM-friendly Markdown.
        *   **Explanation:** We'll discuss tips and techniques to produce Markdown that is optimally structured and cleaned for consumption by Large Language Models.
    *   1.3.3. Troubleshooting common Markdown generation issues.
        *   **Explanation:** We'll cover common problems encountered during Markdown generation (e.g., noisy output, missing content) and provide strategies for diagnosing and resolving them.

## 2. Core Concepts in Markdown Generation

*   2.1. **The `MarkdownGenerationStrategy` Interface**
    *   2.1.1. **Purpose and Design Rationale:**
        *   Why use a strategy pattern for Markdown generation? (Flexibility, extensibility).
            *   **Explanation:** The strategy pattern allows Crawl4AI to define a common interface for Markdown generation while enabling different concrete implementations. This means users can easily swap out the default Markdown generator for a custom one without altering the core crawler logic. It promotes flexibility and makes the system extensible for future Markdown conversion needs or integration with other libraries.
        *   Core problem it solves: Decoupling Markdown generation logic from the crawler.
            *   **Explanation:** By abstracting Markdown generation into a strategy, the `AsyncWebCrawler` itself doesn't need to know the specifics of *how* Markdown is created. It simply delegates the task to the configured strategy. This separation of concerns makes the codebase cleaner and easier to maintain.
    *   2.1.2. **When to Implement a Custom `MarkdownGenerationStrategy`:**
        *   Scenarios requiring completely different Markdown conversion logic.
            *   **Example:** If you need to convert HTML to a very specific dialect of Markdown not supported by `html2text`, or if you want to use a different underlying conversion library entirely.
        *   Integrating third-party Markdown conversion libraries.
            *   **Example:** If you prefer to use a library like `turndown` or `mistune` for its specific features or output style.
        *   Advanced pre/post-processing of Markdown.
            *   **Example:** If you need to perform complex transformations on the Markdown *after* initial generation, such as custom table formatting, complex footnote handling beyond standard citations, or domain-specific semantic tagging within the Markdown.
    *   2.1.3. **How to Implement a Custom `MarkdownGenerationStrategy`:**
        *   Key methods to override (`generate_markdown`).
            *   **Explanation:** The primary method to implement is `generate_markdown(self, input_html: str, base_url: str = "", html2text_options: Optional[Dict[str, Any]] = None, content_filter: Optional[RelevantContentFilter] = None, citations: bool = True, **kwargs) -> MarkdownGenerationResult`. This method will receive the HTML (based on `content_source`), and it's responsible for returning a `MarkdownGenerationResult` object.
        *   Input parameters and expected output (`MarkdownGenerationResult`).
            *   **Explanation:** Your custom strategy will receive the `input_html`, the `base_url` (for resolving relative links if needed), `html2text_options` (which you can choose to use or ignore), an optional `content_filter`, and a `citations` flag. It must return an instance of `MarkdownGenerationResult` populated with the relevant Markdown strings.
        *   *Code Example:*
            ```python
            from crawl4ai import MarkdownGenerationStrategy, MarkdownGenerationResult, RelevantContentFilter
            from typing import Optional, Dict, Any

            class MyCustomMarkdownStrategy(MarkdownGenerationStrategy):
                def __init__(self, content_source: str = "cleaned_html", **kwargs):
                    super().__init__(content_source=content_source, **kwargs)
                    # Initialize any custom resources if needed

                def generate_markdown(
                    self,
                    input_html: str,
                    base_url: str = "",
                    html2text_options: Optional[Dict[str, Any]] = None, # You can use or ignore these
                    content_filter: Optional[RelevantContentFilter] = None,
                    citations: bool = True, # You can decide how to handle this
                    **kwargs
                ) -> MarkdownGenerationResult:
                    
                    # 1. Apply content filter if provided and desired
                    fit_html_output = ""
                    if content_filter:
                        # Assuming content_filter.filter_content returns a list of HTML strings
                        filtered_html_blocks = content_filter.filter_content(input_html) 
                        fit_html_output = "\n".join(filtered_html_blocks)
                    
                    # 2. Your custom HTML to Markdown conversion logic
                    # This is where you'd use your preferred library or custom logic
                    raw_markdown_text = f"# Custom Markdown for {base_url}\n\n{input_html[:200]}..." # Placeholder
                    
                    markdown_with_citations_text = raw_markdown_text # Placeholder for citation logic
                    references_markdown_text = "" # Placeholder for references

                    # If you used a filter, also generate fit_markdown
                    fit_markdown_text = ""
                    if fit_html_output:
                        fit_markdown_text = f"# Custom Filtered Markdown\n\n{fit_html_output[:200]}..." # Placeholder

                    return MarkdownGenerationResult(
                        raw_markdown=raw_markdown_text,
                        markdown_with_citations=markdown_with_citations_text,
                        references_markdown=references_markdown_text,
                        fit_markdown=fit_markdown_text,
                        fit_html=fit_html_output
                    )

            # Usage:
            # custom_md_generator = MyCustomMarkdownStrategy()
            # run_config = CrawlerRunConfig(markdown_generator=custom_md_generator)
            ```
        *   Common pitfalls when creating custom strategies.
            *   **Explanation:**
                *   Forgetting to handle all fields in `MarkdownGenerationResult` (even if some are empty strings).
                *   Incorrectly managing `base_url` for relative links if your custom converter doesn't handle it.
                *   Performance bottlenecks if your custom logic is inefficient.
                *   Not properly integrating with the `content_filter` if one is provided.
    *   2.1.4. **Understanding `content_source` in `MarkdownGenerationStrategy`**
        *   2.1.4.1. Purpose: What HTML source should be used for Markdown generation?
            *   **Explanation:** The `content_source` attribute of a `MarkdownGenerationStrategy` (including `DefaultMarkdownGenerator`) tells the strategy which version of the HTML to use as the primary input for generating `raw_markdown` and `markdown_with_citations`.
        *   2.1.4.2. Available options: `"cleaned_html"`, `"raw_html"`, `"fit_html"`.
            *   **`"cleaned_html"` (Default):** This is the HTML after Crawl4AI's internal `ContentScrapingStrategy` (e.g., `WebScrapingStrategy` or `LXMLWebScrapingStrategy`) has processed it. This usually involves removing scripts, styles, and applying structural cleaning or selection based on `target_elements` or `css_selector` in `CrawlerRunConfig`.
            *   **`"raw_html"`:** The original, unmodified HTML fetched from the page. This is useful if you want to apply your own complete cleaning and Markdown conversion pipeline.
            *   **`"fit_html"`:** The HTML *after* a `RelevantContentFilter` (if provided to the `MarkdownGenerationStrategy`) has processed the input HTML (which would be `cleaned_html` or `raw_html` depending on the initial source). This option is powerful when you want Markdown generated *only* from the most relevant parts of the page.
        *   2.1.4.3. **Decision Guide: Choosing the Right `content_source`**:
            *   **When to use `"cleaned_html"`:** This is the recommended default for most LLM use cases. It provides a good balance of structured content without excessive noise, as common boilerplate is often removed by the scraping strategy.
            *   **When to use `"raw_html"`:** Choose this if you need absolute control over the HTML input for your Markdown converter, or if Crawl4AI's default cleaning removes elements you wish to keep. Be aware that this might result in noisier Markdown.
            *   **When to use `"fit_html"`:** Opt for this when you are using a `RelevantContentFilter` with your `MarkdownGenerationStrategy` and you want the `raw_markdown` and `markdown_with_citations` to be based *only* on the filtered content. This is distinct from just using the `fit_markdown` field in the result, as it makes the filtered content the *primary* source for all main Markdown outputs.
            *   **Impact on performance and output quality:**
                *   `"raw_html"` might be slightly faster if Crawl4AI's cleaning is complex, but could lead to lower quality Markdown due to more noise.
                *   `"cleaned_html"` offers a good trade-off.
                *   `"fit_html"` depends on the performance of the `RelevantContentFilter` itself.
        *   2.1.4.4. *Example Scenarios:*
            *   **General Summarization:** `"cleaned_html"` is usually best.
            *   **Highly Specific Q&A on a Section:** Use a `RelevantContentFilter` to produce `fit_html`, then set `content_source="fit_html"` (or just use the `fit_markdown` from the result if `raw_markdown` from `"cleaned_html"` is also desired).
            *   **Archiving Raw Structure:** `"raw_html"` might be chosen if the goal is to convert the entire, unmodified page structure to Markdown, perhaps for later, more nuanced processing.

*   2.2. **The `MarkdownGenerationResult` Model**
    *   2.2.1. **Understanding its Purpose:** Why a structured result object?
        *   **Explanation:** A structured object like `MarkdownGenerationResult` is used instead of a single Markdown string to provide different views or versions of the generated Markdown, catering to various use cases. This allows users to pick the representation that best suits their needs (e.g., with or without citations, raw vs. filtered) without re-processing. It also clearly separates the main content from metadata like references or the intermediate `fit_html`.
    *   2.2.2. **Deep Dive into `MarkdownGenerationResult` Fields:**
        *   `raw_markdown`:
            *   **What it is:** This is the direct, primary Markdown output generated from the `content_source` (e.g., `cleaned_html`) defined in the `MarkdownGenerationStrategy`. It does *not* have inline links converted to citation format.
            *   **How to use it:** Use this when you need the most "vanilla" Markdown, perhaps for LLMs that are sensitive to citation formats or if you plan to implement your own link/reference handling.
            *   **When it's useful:** For direct input to LLMs that don't require source attribution within the text, or as a base for further custom Markdown processing.
        *   `markdown_with_citations`:
            *   **What it is:** This takes the `raw_markdown` and converts its inline links (e.g., `[link text](http://example.com)`) into a citation format (e.g., `link text [^1^]`).
            *   **How it's generated:** The `DefaultMarkdownGenerator` (via `CustomHTML2Text`) scans `raw_markdown` for links, assigns unique numerical IDs to each unique URL, replaces the inline link with the text and citation marker, and populates `references_markdown`.
            *   **How to use it:** This is often the most useful Markdown for LLM tasks requiring RAG or for generating human-readable documents where sources are important. Combine it with `references_markdown`.
            *   *Example:*
                ```html
                <!-- Input HTML fragment -->
                <p>Crawl4AI is an <a href="https://github.com/unclecode/crawl4ai">open-source</a> library.</p>
                ```
                ```markdown
                // Resulting markdown_with_citations (simplified)
                Crawl4AI is an open-source [^1^] library.
                ```
        *   `references_markdown`:
            *   **What it is:** A separate Markdown string that lists all unique URLs found and converted to citations, formatted typically as a numbered list.
            *   **How to use it:** Append this string to the end of `markdown_with_citations` to create a complete document with a bibliography or reference section.
            *   **Why it's separate:** This provides flexibility. You can choose to display references at the end, in a sidebar, or not at all.
            *   *Example:*
                ```markdown
                ## References

                [^1^]: https://github.com/unclecode/crawl4ai
                ```
        *   `fit_markdown`:
            *   **What it is:** This is Markdown generated *exclusively* from the `fit_html`. `fit_html` itself is the output of a `RelevantContentFilter` if one was provided to the `MarkdownGenerationStrategy`. If no filter was used, `fit_markdown` will likely be empty or reflect the `raw_markdown`.
            *   **How to use it:** When your primary goal is to feed an LLM with the most relevant, filtered content. This is excellent for tasks like generating concise summaries or providing highly focused context for RAG.
            *   **Relationship with `raw_markdown`:** If a filter is active, `fit_markdown` is based on a *subset* or *transformed version* of the HTML that `raw_markdown` was based on (assuming `content_source` wasn't `"fit_html"`). If `content_source` *was* `"fit_html"`, then `raw_markdown` and `fit_markdown` would be derived from the same filtered HTML, but `fit_markdown` might still undergo different processing if the strategy handles it distinctly.
            *   *Example:* Imagine a news article page. `raw_markdown` might contain the article, comments, ads, and navigation. If a `BM25ContentFilter` is used with a query about "stock market impact", `fit_markdown` would ideally only contain paragraphs related to that topic, stripped of other page elements.
        *   `fit_html`:
            *   **What it is:** The actual HTML string *after* a `RelevantContentFilter` (like `PruningContentFilter` or `LLMContentFilter`) has processed the input HTML. If no filter is applied, this field will be empty.
            *   **How to use it:** Primarily for debugging your content filters. You can inspect `fit_html` to see exactly what HTML content was deemed "relevant" by your filter before it was converted to `fit_markdown`. It can also be useful if you need this filtered HTML for purposes other than Markdown generation.
            *   **Why it's included:** It provides transparency into the filtering process and allows advanced users to work with the intermediate filtered HTML directly.

## 3. The `DefaultMarkdownGenerator` - Your Go-To Solution

*   3.1. **Understanding the `DefaultMarkdownGenerator`**
    *   3.1.1. **Purpose and Design:** The `DefaultMarkdownGenerator` is Crawl4AI's standard, out-of-the-box mechanism for converting HTML content into various Markdown representations. It's designed to be a robust and generally applicable solution for most common use cases, especially when targeting LLM consumption.
    *   3.1.2. Core Functionality: Its primary task is to orchestrate the HTML-to-Markdown conversion. It internally uses an instance of `CustomHTML2Text` (Crawl4AI's enhanced `html2text` wrapper) to perform the actual conversion.
    *   3.1.3. How it handles citations and references by default.
        *   **Explanation:** If the `citations` parameter in its `generate_markdown` method is `True` (which it is by default), `DefaultMarkdownGenerator` will post-process the initially generated Markdown to convert inline links into citation markers (e.g., `[^1^]`) and generate a corresponding `references_markdown` block. This is done by its internal `CustomHTML2Text` instance.

*   3.2. **Configuring `DefaultMarkdownGenerator`**
    *   3.2.1. **Initialization Options:**
        *   `content_filter (Optional[RelevantContentFilter])`:
            *   **Why use it:** To refine the HTML *before* it's converted to Markdown. This is essential if you want `fit_markdown` (and consequently `fit_html`) to contain only the most relevant parts of the page, leading to a more focused Markdown output.
            *   **How it integrates:** When `generate_markdown` is called, if a `content_filter` is present, `DefaultMarkdownGenerator` first passes the `input_html` (determined by `content_source`) to this filter. The filter returns a list of HTML strings (or a single string if merged). This filtered HTML becomes the `fit_html`. Then, `fit_markdown` is generated from this `fit_html`. The `raw_markdown` and `markdown_with_citations` are still generated from the original `content_source` unless `content_source` itself is set to `"fit_html"`.
            *   *Impact:* Directly influences `fit_markdown` and `fit_html` fields in `MarkdownGenerationResult`. Can significantly reduce the noise and improve the relevance of the final Markdown for LLMs.
            *   *Code Example:*
                ```python
                from crawl4ai import DefaultMarkdownGenerator, CrawlerRunConfig
                from crawl4ai.content_filter_strategy import PruningContentFilter

                # Initialize a filter
                pruning_filter = PruningContentFilter(threshold_type="fixed", threshold=0.5)
                
                # Initialize DefaultMarkdownGenerator with the filter
                md_generator_with_filter = DefaultMarkdownGenerator(content_filter=pruning_filter)

                # This generator will now produce 'fit_markdown' based on pruning.
                # run_config = CrawlerRunConfig(markdown_generator=md_generator_with_filter)
                # result = await crawler.arun(url="...", config=run_config)
                # print(result.markdown.fit_markdown) 
                ```
        *   `options (Optional[Dict[str, Any]])`:
            *   **What it is:** This dictionary allows you to pass configuration options directly to the underlying `CustomHTML2Text` instance. These options control the specifics of the HTML-to-Markdown conversion process.
            *   **How to use it:** Provide a dictionary where keys are `html2text` option names (e.g., `body_width`, `ignore_links`) and values are their desired settings.
            *   *See Section 6: Mastering `CustomHTML2Text` for detailed options.*
        *   `content_source (str)`:
            *   **Reiteration:** As discussed in section 2.1.4, this determines the primary HTML input for `raw_markdown` and `markdown_with_citations`.
            *   **How it interacts with `content_filter`:**
                *   If `content_source` is, for example, `"cleaned_html"` and a `content_filter` is also provided, the `content_filter` will process this `"cleaned_html"` to produce `fit_html`. The `fit_markdown` field in `MarkdownGenerationResult` will be based on this `fit_html`.
                *   However, `raw_markdown` and `markdown_with_citations` will still be based on the original `"cleaned_html"` (unless `content_source` was explicitly set to `"fit_html"`). This allows you to have both a "fuller" Markdown and a "filtered" Markdown from a single generation step.

*   3.3. **Common Workflows with `DefaultMarkdownGenerator`**
    *   3.3.1. **Workflow: Generating Basic Markdown with Citations**
        *   Steps: Instantiate `DefaultMarkdownGenerator` (or use the crawler's default). The crawler calls its `generate_markdown` method. Access `result.markdown.markdown_with_citations` and `result.markdown.references_markdown`.
        *   *Code Example:*
            ```python
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator

            async def basic_markdown_workflow():
                # DefaultMarkdownGenerator is used implicitly if none is specified in CrawlerRunConfig
                # Or explicitly:
                md_generator = DefaultMarkdownGenerator() 
                run_config = CrawlerRunConfig(markdown_generator=md_generator)

                async with AsyncWebCrawler() as crawler:
                    result = await crawler.arun(url="https://example.com", config=run_config)
                    if result.success:
                        print("--- Markdown with Citations ---")
                        print(result.markdown.markdown_with_citations[:500]) # Show first 500 chars
                        print("\n--- References ---")
                        print(result.markdown.references_markdown)
                    else:
                        print(f"Crawl failed: {result.error_message}")
            ```
    *   3.3.2. **Workflow: Generating Focused Markdown using a Content Filter**
        *   Steps:
            1.  Choose and instantiate a `RelevantContentFilter` (e.g., `BM25ContentFilter`).
            2.  Instantiate `DefaultMarkdownGenerator`, passing the filter to its `content_filter` parameter.
            3.  Set this `DefaultMarkdownGenerator` instance in `CrawlerRunConfig.markdown_generator`.
            4.  After crawling, access `result.markdown.fit_markdown`.
        *   Key configuration considerations for the filter and generator:
            *   For `BM25ContentFilter`, ensure you provide a relevant `user_query`.
            *   Adjust filter thresholds (e.g., `bm25_threshold`) as needed.
            *   The `content_source` for `DefaultMarkdownGenerator` will be the input to the filter.
        *   *Code Example:*
            ```python
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator, CacheMode
            from crawl4ai.content_filter_strategy import BM25ContentFilter

            async def filtered_markdown_workflow():
                user_query = "information about Crawl4AI library"
                bm25_filter = BM25ContentFilter(user_query=user_query, bm25_threshold=0.1)
                
                md_generator = DefaultMarkdownGenerator(content_filter=bm25_filter)
                
                run_config = CrawlerRunConfig(
                    markdown_generator=md_generator,
                    cache_mode=CacheMode.BYPASS # For consistent demo results
                )

                async with AsyncWebCrawler() as crawler:
                    # Using a page that hopefully has content related to the query
                    result = await crawler.arun(url="https://github.com/unclecode/crawl4ai", config=run_config) 
                    if result.success:
                        print("--- Fit Markdown (BM25 Filtered) ---")
                        print(result.markdown.fit_markdown) # This is the key output
                        # You can also inspect fit_html to see what the filter selected
                        # print("\n--- Fit HTML ---")
                        # print(result.markdown.fit_html[:500])
                    else:
                        print(f"Crawl failed: {result.error_message}")
            ```
    *   3.3.3. **Workflow: Customizing Markdown Style via `html2text_options`**
        *   Steps: Instantiate `DefaultMarkdownGenerator` passing a dictionary of `html2text` options to its `options` parameter.
        *   *Code Example:*
            ```python
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator

            async def custom_style_markdown_workflow():
                # Example: Disable line wrapping and ignore images
                html2text_opts = {
                    "body_width": 0,      # Disable line wrapping
                    "ignore_images": True # Don't include image markdown ![alt](src)
                }
                md_generator = DefaultMarkdownGenerator(options=html2text_opts)
                
                run_config = CrawlerRunConfig(markdown_generator=md_generator)

                async with AsyncWebCrawler() as crawler:
                    result = await crawler.arun(url="https://example.com", config=run_config)
                    if result.success:
                        print("--- Custom Styled Markdown (No Wrap, No Images) ---")
                        print(result.markdown.raw_markdown[:500]) # raw_markdown will reflect these options
                    else:
                        print(f"Crawl failed: {result.error_message}")
            ```
*   3.4. **Best Practices for `DefaultMarkdownGenerator`**
    *   **When to use `DefaultMarkdownGenerator` vs. a custom strategy:**
        *   Use `DefaultMarkdownGenerator` for most cases. It's robust and highly configurable through `content_filter` and `html2text_options`.
        *   Opt for a custom strategy only if you need fundamentally different conversion logic or integration with external Markdown libraries that `CustomHTML2Text` doesn't cover.
    *   **Tips for choosing the right `content_source` and `content_filter`:**
        *   Start with `content_source="cleaned_html"` (default) and no filter.
        *   If the output is too noisy, introduce a `RelevantContentFilter`. `PruningContentFilter` is a good first step for general boilerplate. Use `BM25ContentFilter` or `LLMContentFilter` for more targeted filtering based on semantic relevance.
        *   If your filter is very effective and you *only* want Markdown from the filtered content, consider setting `content_source="fit_html"` in your `DefaultMarkdownGenerator` instance.
    *   **How to leverage `MarkdownGenerationResult` effectively:**
        *   For LLM input where source attribution is important, use `markdown_with_citations` + `references_markdown`.
        *   For tasks needing maximum conciseness based on relevance, use `fit_markdown` (after configuring a `content_filter`).
        *   Use `raw_markdown` if you need the "purest" Markdown conversion without citation processing.
        *   Inspect `fit_html` to debug your content filters.

## 4. Integrating Content Filters for Smarter Markdown (`fit_markdown`)

*   4.1. **The "Why": Purpose of Content Filtering Before Markdown Generation**
    *   4.1.1. Reducing noise and improving relevance for LLMs.
        *   **Explanation:** Web pages often contain much more than just the main article content (e.g., navigation, ads, footers, related articles). These can be detrimental to LLM performance, increasing token count, processing time, and potentially confusing the model. Content filters aim to isolate the core, relevant information.
    *   4.1.2. Generating more concise and focused Markdown (`fit_markdown`).
        *   **Explanation:** By filtering the HTML *before* converting it to Markdown, the resulting `fit_markdown` is inherently more concise and focused on what the filter deemed important. This is ideal for tasks where brevity and relevance are key.
    *   4.1.3. How `fit_html` is generated and its role.
        *   **Explanation:** When a `RelevantContentFilter` is used with a `MarkdownGenerationStrategy`, the strategy first passes the input HTML (e.g., `cleaned_html`) to the filter's `filter_content` method. This method returns a list of HTML strings (or a single merged string). This output is stored as `fit_html` in the `MarkdownGenerationResult`. `fit_markdown` is then generated by converting this `fit_html` to Markdown.

*   4.2. **Overview of `RelevantContentFilter` Strategies**
    *   4.2.1. **`PruningContentFilter`**:
        *   **How it works:** Applies heuristic rules to remove common boilerplate. For example, it might remove elements with very short text content, elements with a high link-to-text ratio, or elements matching common boilerplate CSS classes/IDs (like "footer", "nav", "sidebar").
        *   **When to use it:** A good first-pass filter for general-purpose cleaning. It's fast and doesn't require LLM calls or complex configuration.
        *   **Impact on `fit_markdown`:** Typically good at removing obvious non-content sections, resulting in a cleaner, more article-focused Markdown.
    *   4.2.2. **`BM25ContentFilter`**:
        *   **How it works:** This filter uses the BM25 algorithm, a classical information retrieval technique. It tokenizes the HTML content into chunks and scores each chunk's relevance against a `user_query`. Chunks exceeding a `bm25_threshold` are kept.
        *   **When to use it:** When you want to extract content specifically related to a user's query from a larger page. Excellent for targeted information retrieval.
        *   **Impact on `fit_markdown`:** The output will be highly tailored to the query. If the query is "Tell me about Crawl4AI's caching", `fit_markdown` should primarily contain sections discussing caching.
    *   4.2.3. **`LLMContentFilter`**:
        *   **How it works:** This is the most powerful and flexible filter. It chunks the input HTML and sends each chunk (or a summary) to an LLM with specific `instructions` (e.g., "Extract only the paragraphs discussing financial results"). The LLM decides which chunks are relevant.
        *   **When to use it:** For complex filtering criteria that are hard to express with rules or keywords, or when nuanced understanding of content is required.
        *   **Impact on `fit_markdown`:** Can produce very precise and contextually relevant Markdown. However, it's generally slower and can be more expensive due to LLM API calls.
*   4.3. **Decision Guide: Choosing the Right `RelevantContentFilter`**
    *   *Table:*
        | Filter                | Speed      | Cost (LLM API) | Accuracy/Nuance | Use Case Examples                                  | Configuration Complexity |
        |-----------------------|------------|----------------|-----------------|----------------------------------------------------|--------------------------|
        | `PruningContentFilter`| Very Fast  | None           | Low-Medium      | General boilerplate removal, quick cleaning.       | Low                      |
        | `BM25ContentFilter`   | Fast       | None           | Medium          | Query-focused extraction, finding relevant sections. | Medium (query, threshold)|
        | `LLMContentFilter`    | Slow       | Potentially High| High            | Complex criteria, nuanced extraction, summarization. | High (prompt engineering)  |
    *   Factors to consider:
        *   **Desired Output Quality:** For the highest semantic relevance, `LLMContentFilter` is often best, but at a cost.
        *   **Performance Constraints:** If speed is critical, `PruningContentFilter` or `BM25ContentFilter` are preferred.
        *   **Nature of the HTML Content:** For well-structured articles, `PruningContentFilter` might be sufficient. For diverse content or Q&A, `BM25ContentFilter` or `LLMContentFilter` might be better.
        *   **Specificity of Task:** If you have a clear query, `BM25ContentFilter` excels. If you have complex instructions, `LLMContentFilter` is suitable.
*   4.4. **Code Examples: Combining Filters with `DefaultMarkdownGenerator`**
    *   4.4.1. *Example:* [Using `PruningContentFilter` to generate `fit_markdown`].
        ```python
        from crawl4ai import DefaultMarkdownGenerator, CrawlerRunConfig, AsyncWebCrawler, CacheMode
        from crawl4ai.content_filter_strategy import PruningContentFilter

        async def pruning_filter_example():
            pruning_filter = PruningContentFilter(threshold=0.4, threshold_type="fixed") # Adjust threshold as needed
            md_generator = DefaultMarkdownGenerator(content_filter=pruning_filter)
            run_config = CrawlerRunConfig(markdown_generator=md_generator, cache_mode=CacheMode.BYPASS)

            async with AsyncWebCrawler() as crawler:
                result = await crawler.arun(url="https://en.wikipedia.org/wiki/Python_(programming_language)", config=run_config)
                if result.success:
                    print("--- Fit Markdown (Pruned) ---")
                    print(result.markdown.fit_markdown[:1000]) # Show first 1000 chars
                    # print("\n--- Original Raw Markdown (for comparison) ---")
                    # print(result.markdown.raw_markdown[:1000])
        ```
    *   4.4.2. *Example:* [Using `BM25ContentFilter` with a query to generate query-focused `fit_markdown`].
        ```python
        from crawl4ai import DefaultMarkdownGenerator, CrawlerRunConfig, AsyncWebCrawler, CacheMode
        from crawl4ai.content_filter_strategy import BM25ContentFilter

        async def bm25_filter_example():
            user_query = "Python syntax and semantics"
            bm25_filter = BM25ContentFilter(user_query=user_query, bm25_threshold=0.1)
            md_generator = DefaultMarkdownGenerator(content_filter=bm25_filter)
            run_config = CrawlerRunConfig(markdown_generator=md_generator, cache_mode=CacheMode.BYPASS)
            
            async with AsyncWebCrawler() as crawler:
                result = await crawler.arun(url="https://en.wikipedia.org/wiki/Python_(programming_language)", config=run_config)
                if result.success:
                    print(f"--- Fit Markdown (BM25 Filtered for query: '{user_query}') ---")
                    print(result.markdown.fit_markdown)
        ```
    *   4.4.3. *Example:* [Using `LLMContentFilter` for nuanced content selection before Markdown generation].
        ```python
        from crawl4ai import DefaultMarkdownGenerator, CrawlerRunConfig, AsyncWebCrawler, LLMConfig, CacheMode
        from crawl4ai.content_filter_strategy import LLMContentFilter
        import os

        async def llm_filter_example():
            # Ensure OPENAI_API_KEY is set in your environment
            if not os.getenv("OPENAI_API_KEY"):
                print("OPENAI_API_KEY not set. Skipping LLMContentFilter example.")
                return

            llm_config_obj = LLMConfig(provider="openai/gpt-4o-mini", api_key=os.getenv("OPENAI_API_KEY"))
            
            instruction = "Extract only the sections that discuss Python's history and its creator."
            llm_filter = LLMContentFilter(
                llm_config=llm_config_obj,
                instruction=instruction,
                # chunk_token_threshold=1000 # Adjust as needed
            )
            
            md_generator = DefaultMarkdownGenerator(content_filter=llm_filter, content_source="cleaned_html")
            
            run_config = CrawlerRunConfig(markdown_generator=md_generator, cache_mode=CacheMode.BYPASS)

            async with AsyncWebCrawler() as crawler:
                result = await crawler.arun(url="https://en.wikipedia.org/wiki/Python_(programming_language)", config=run_config)
                if result.success:
                    print(f"--- Fit Markdown (LLM Filtered with instruction: '{instruction}') ---")
                    print(result.markdown.fit_markdown)
                    llm_filter.show_usage() # Display token usage
                else:
                    print(f"Crawl failed: {result.error_message}")
        ```
*   4.5. **Best Practices for Content Filtering for Markdown**
    *   **Start Simple:** Begin with `PruningContentFilter` for general cleanup. It's fast and often effective for removing common boilerplate.
    *   **Query-Specific Tasks:** If your goal is to extract information relevant to a specific query, `BM25ContentFilter` is a great, cost-effective choice.
    *   **Nuanced Selection:** Reserve `LLMContentFilter` for tasks requiring deeper semantic understanding or complex filtering logic that rules-based or keyword-based approaches can't handle. Be mindful of its cost and latency.
    *   **Iterate and Test:** Content filtering is often an iterative process. Test your filter configurations on various pages to ensure they behave as expected. Inspect `fit_html` to understand what the filter is selecting/discarding.
    *   **Combine with `content_source`:** Remember that `fit_markdown` is derived from the output of the filter. If you also need Markdown from the pre-filtered content, ensure your `MarkdownGenerationStrategy`'s `content_source` is set appropriately (e.g., `"cleaned_html"`) so that `raw_markdown` reflects that, while `fit_markdown` reflects the filtered version.

## 5. Customizing Markdown Output via `CrawlerRunConfig`

*   5.1. **The Role of `CrawlerRunConfig.markdown_generator`**
    *   5.1.1. How it allows specifying a custom Markdown generation strategy for a crawl run.
        *   **Explanation:** The `markdown_generator` parameter within the `CrawlerRunConfig` object is the primary way to control how Markdown is generated for a specific crawl operation (i.e., a call to `crawler.arun()` or tasks within `crawler.arun_many()`). You can assign an instance of any class that adheres to the `MarkdownGenerationStrategy` interface to it.
    *   5.1.2. Overriding the default Markdown generation behavior.
        *   **Explanation:** If `CrawlerRunConfig.markdown_generator` is not set (i.e., it's `None`), Crawl4AI will use a default instance of `DefaultMarkdownGenerator` with its standard settings. By providing your own `MarkdownGenerationStrategy` instance (be it a configured `DefaultMarkdownGenerator` or a custom class), you override this default behavior for that particular run.

*   5.2. **Scenarios for Using `CrawlerRunConfig.markdown_generator`**
    *   5.2.1. Applying a pre-configured `DefaultMarkdownGenerator` with specific filters or options.
        *   **Why:** You might want different filtering logic or `html2text` options for different URLs or types of content you're crawling, even within the same `AsyncWebCrawler` instance.
    *   5.2.2. Plugging in a completely custom `MarkdownGenerationStrategy`.
        *   **Why:** As discussed in section 2.1.2, if you have unique Markdown requirements or want to use a different conversion library.
    *   5.2.3. Disabling Markdown generation entirely by setting it to `None` (if applicable, or by using a "NoOp" strategy).
        *   **Why:** If, for a specific crawl, you only need the HTML or extracted structured data and don't require Markdown output, you can pass `markdown_generator=None` (or a strategy that does nothing) to save processing time.
        *   *Note:* To truly disable Markdown generation and its associated `CustomHTML2Text` processing, you might need a "NoOpMarkdownGenerator". If `markdown_generator` is `None`, the crawler might still fall back to a default. A NoOp strategy would explicitly do nothing.
            ```python
            # class NoOpMarkdownGenerator(MarkdownGenerationStrategy):
            #     def generate_markdown(self, input_html: str, **kwargs) -> MarkdownGenerationResult:
            #         return MarkdownGenerationResult(raw_markdown="", markdown_with_citations="", references_markdown="")
            # run_config = CrawlerRunConfig(markdown_generator=NoOpMarkdownGenerator())
            ```

*   5.3. **Code Examples:**
    *   5.3.1. *Example:* [Setting a `DefaultMarkdownGenerator` with a `PruningContentFilter` in `CrawlerRunConfig`].
        ```python
        from crawl4ai import (
            AsyncWebCrawler, 
            CrawlerRunConfig, 
            DefaultMarkdownGenerator, 
            CacheMode
        )
        from crawl4ai.content_filter_strategy import PruningContentFilter

        async def run_with_specific_md_generator():
            # Configure a specific markdown generator
            pruning_filter = PruningContentFilter(threshold=0.6)
            specific_md_generator = DefaultMarkdownGenerator(
                content_filter=pruning_filter,
                options={"body_width": 0, "ignore_links": True} 
            )

            # Configure the crawl run to use this generator
            run_config = CrawlerRunConfig(
                markdown_generator=specific_md_generator,
                cache_mode=CacheMode.BYPASS
            )

            async with AsyncWebCrawler() as crawler:
                result = await crawler.arun(url="https://example.com/article1", config=run_config)
                if result.success:
                    print("--- Markdown from Article 1 (Pruned, No Links, No Wrap) ---")
                    print(result.markdown.fit_markdown[:500]) 
                    # raw_markdown would also reflect no-wrap and no-links from html2text_options

                # For another URL, you could use a different (or default) generator
                # default_run_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)
                # result2 = await crawler.arun(url="https://example.com/article2", config=default_run_config)

        # asyncio.run(run_with_specific_md_generator())
        ```
    *   5.3.2. *Example:* [Setting a custom `MyMarkdownStrategy` in `CrawlerRunConfig` (assuming `MyCustomMarkdownStrategy` from 2.1.3)].
        ```python
        # Assuming MyCustomMarkdownStrategy is defined as in section 2.1.3
        # from crawl4ai import CrawlerRunConfig, AsyncWebCrawler, CacheMode
        # from your_module import MyCustomMarkdownStrategy # If it's in another file

        # async def run_with_custom_md_strategy():
        #     custom_strategy = MyCustomMarkdownStrategy(content_source="raw_html")
        #     run_config_custom = CrawlerRunConfig(
        #         markdown_generator=custom_strategy,
        #         cache_mode=CacheMode.BYPASS
        #     )

        #     async with AsyncWebCrawler() as crawler:
        #         result = await crawler.arun(url="https://example.com", config=run_config_custom)
        #         if result.success:
        #             print("--- Markdown from Custom Strategy ---")
        #             print(result.markdown.raw_markdown) # Or other fields your strategy populates
        
        # asyncio.run(run_with_custom_md_strategy())
        ```
*   5.4. **Interaction with Global vs. Run-Specific Configurations**
    *   **Explanation:** `AsyncWebCrawler` itself does not have a global `markdown_generator` setting during its initialization. Markdown generation is configured *per run* via `CrawlerRunConfig`. This design choice provides maximum flexibility, allowing different Markdown strategies for different URLs or tasks within the same crawler instance lifecycle. If `CrawlerRunConfig.markdown_generator` is not provided, a default `DefaultMarkdownGenerator` instance is used for that specific run.

## 6. Mastering `CustomHTML2Text` for Fine-Grained Control

*   6.1. **Understanding `CustomHTML2Text`**
    *   6.1.1. **Purpose:** Why Crawl4AI includes its own `html2text` extension.
        *   **Enhanced control:** `CustomHTML2Text` is a subclass of the standard `html2text.HTML2Text` library. Crawl4AI uses this custom version to gain more precise control over the HTML-to-Markdown conversion process, particularly to make the output more suitable for LLMs.
        *   **Specific adaptations:** It includes logic for handling Crawl4AI's citation and reference generation (`convert_links_to_citations`), and potentially other tweaks that improve the quality and utility of the Markdown output for AI applications.
    *   6.1.2. **How it's used by `DefaultMarkdownGenerator`**.
        *   **Explanation:** `DefaultMarkdownGenerator` instantiates `CustomHTML2Text` internally. When you pass `options` to `DefaultMarkdownGenerator`, these are ultimately used to configure this `CustomHTML2Text` instance. The `handle()` method of `CustomHTML2Text` is what performs the core HTML to Markdown conversion.

*   6.2. **Key `html2text_options` and Their Impact**
    *   (These options are passed via `DefaultMarkdownGenerator(options=...)`)
    *   6.2.1. `body_width`:
        *   **What it does:** Controls the maximum width of lines in the generated Markdown before wrapping.
        *   **Why configure it:** For LLM consumption, it's often best to disable automatic line wrapping to allow the LLM to process text based on natural paragraph breaks. Setting `body_width=0` achieves this.
        *   *Example:*
            *   `body_width=80` (default-ish for some tools):
                ```markdown
                This is a longer sentence that will be wrapped by html2text if the body_width is
                set to a value like 80 characters.
                ```
            *   `body_width=0`:
                ```markdown
                This is a longer sentence that will not be wrapped by html2text if body_width is 0, allowing the LLM to handle line breaks.
                ```
    *   6.2.2. `ignore_links`:
        *   **What it does:** If `True`, all hyperlink information (`[text](url)`) is removed, leaving only the link text.
        *   **Why configure it:** Set to `True` if links are considered noise for your LLM task and you don't need source attribution. If `False` (default for Crawl4AI's `CustomHTML2Text` unless overridden), links are preserved and can then be converted to citations by `DefaultMarkdownGenerator`.
        *   *Example:*
            *   `ignore_links=False` (then processed for citations): `Visit [Crawl4AI](https://crawl4ai.com)` -> `Visit Crawl4AI [^1^]`
            *   `ignore_links=True`: `Visit [Crawl4AI](https://crawl4ai.com)` -> `Visit Crawl4AI`
    *   6.2.3. `ignore_images`:
        *   **What it does:** If `True`, image tags (`<img>`) are completely ignored, and no Markdown image syntax (`![alt](src)`) is generated.
        *   **Why configure it:** Useful if image information is irrelevant to your LLM task and you want cleaner, more text-focused Markdown.
        *   *Example:*
            *   HTML: `<img src="logo.png" alt="My Logo">`
            *   `ignore_images=False`: `![My Logo](logo.png)`
            *   `ignore_images=True`: (nothing is output for the image)
    *   6.2.4. `protect_links`:
        *   **What it does:** If `True`, surrounds link URLs with `<` and `>`. E.g., `[text](<url>)`.
        *   **Why configure it:** This can sometimes help Markdown parsers that might misinterpret URLs containing special characters. However, with Crawl4AI's citation handling, this is generally not needed, as the raw URLs are moved to the reference section.
    *   6.2.5. `mark_code`:
        *   **What it does:** Controls how `<pre>` and `<code>` tags are handled. If `True`, it attempts to use Markdown code block syntax (backticks).
        *   **Why configure it:** Essential for preserving code snippets correctly. Usually, you'd want this to be `True`.
    *   6.2.6. `default_image_alt`:
        *   **What it does:** Provides a default alt text string if an `<img>` tag is missing an `alt` attribute.
        *   **Why configure it:** Can make Markdown more consistent if you choose to include images.
    *   6.2.7. `bypass_tables`:
        *   **What it does:** If `True`, `<table>` elements are not converted into Markdown table syntax. Their content might be rendered as plain text or omitted, depending on other settings.
        *   **Why configure it:** Standard Markdown table syntax is limited and may not handle complex tables (with `colspan`, `rowspan`, nested tables) well. If you encounter mangled tables, setting this to `True` and processing the table HTML separately (e.g., by extracting the `<table>` HTML and using a specialized table-to-text or table-to-JSON library) might be a better approach.
    *   6.2.8. `pad_tables`:
        *   **What it does:** If `True`, adds padding spaces around cell content in Markdown tables for better visual alignment in raw Markdown.
        *   **Why configure it:** Mostly an aesthetic choice for human readability of the raw Markdown; LLMs typically don't care about this padding.
    *   *Other relevant options identified from `CustomHTML2Text` (or base `html2text`) source:*
        *   `escape_snob`: If `True`, escapes `>` and `&` characters. Default is `False`.
        *   `skip_internal_links`: If `True`, ignores links that start with `#`. Default is `False`.
        *   `links_each_paragraph`: If `True`, puts a link list after each paragraph. Default is `False`. Crawl4AI's citation system provides a better alternative.
        *   `unicode_snob`: If `True`, uses Unicode characters instead of ASCII approximations. Default is `False` in base `html2text`, but `CustomHTML2Text` might behave differently or Crawl4AI ensures UTF-8 handling.
*   6.3. **Best Practices for Configuring `CustomHTML2Text`**
    *   6.3.1. **General recommendations for LLM-friendly output:**
        *   Set `body_width=0` to disable line wrapping and let paragraphs flow naturally.
        *   Consider `ignore_images=True` if images are not relevant to the LLM's task.
        *   Usually, keep `ignore_links=False` (Crawl4AI default) to allow `DefaultMarkdownGenerator` to handle citations properly.
    *   6.3.2. **How to balance information preservation with conciseness:**
        *   Be selective with `ignore_*` options. Removing too much might discard useful context.
        *   Use content filters (Section 4) for semantic reduction rather than relying solely on `html2text` options to remove large irrelevant sections.
    *   6.3.3. **Experimenting with options to achieve desired Markdown style:**
        *   Create a small test HTML snippet.
        *   Instantiate `DefaultMarkdownGenerator` with different `options` dictionaries.
        *   Call its `generate_markdown` method directly (or `_html_to_markdown` on its internal `CustomHTML2Text` instance if you want to bypass citation logic for testing) and observe the output.
*   6.4. **Handling Citations and References (`convert_links_to_citations` method in `CustomHTML2Text`)**
    *   6.4.1. **How it works:**
        *   The `convert_links_to_citations` method (called by `DefaultMarkdownGenerator` if citations are enabled) iterates through the Markdown produced by `html2text.handle()`.
        *   It uses a regular expression (`LINK_PATTERN`) to find all Markdown links (`[text](url "optional title")`).
        *   For each unique URL, it assigns an incremental citation number.
        *   It replaces the original Markdown link with `text [^N^]` (or `![text][^N^]` for images if not ignored).
        *   It builds up a list of reference strings like `[^N^]: url "optional title - text if different from title"`.
    *   6.4.2. **When it's called:** This method is invoked by `DefaultMarkdownGenerator.generate_markdown()` *after* the initial HTML-to-Markdown conversion by `CustomHTML2Text.handle()` if the `citations` flag is `True`.
    *   6.4.3. **Impact on `MarkdownGenerationResult` fields:**
        *   The modified Markdown (with `[^N^]` markers) is stored in `markdown_with_citations`.
        *   The collected reference list is stored in `references_markdown`.
        *   `raw_markdown` remains the version *before* citation processing.
    *   6.4.4. **Customizing Citation Behavior (if possible through options or by subclassing)**.
        *   **Explanation:** Direct customization of the citation format (e.g., changing `[^N^]` to `(N)`) via options is not explicitly provided in `CustomHTML2Text`.
        *   To change this, you would need to:
            1.  Create your own class inheriting from `DefaultMarkdownGenerator`.
            2.  Override the `generate_markdown` method.
            3.  In your override, you could either:
                *   Call the parent's `generate_markdown`, get the `MarkdownGenerationResult`, and then post-process `markdown_with_citations` and `references_markdown` to your desired format.
                *   Or, more invasively, replicate the logic but modify the citation generation part. This might involve creating a custom version of `CustomHTML2Text` or its `convert_links_to_citations` method.
        *   For most users, the default citation format is standard and widely accepted.

## 7. Advanced Markdown Generation Techniques & Best Practices

*   7.1. **Achieving LLM-Friendly Markdown Output**
    *   7.1.1. Prioritizing semantic structure (headings, lists, paragraphs).
        *   **Why:** LLMs leverage structural cues to understand context and hierarchy. Ensure your `html2text_options` (e.g., for headings, list indentation) preserve this structure faithfully.
        *   **How:** Rely on `CustomHTML2Text`'s default handling of semantic HTML tags. If specific tags are problematic, consider pre-processing the HTML.
    *   7.1.2. Handling complex HTML structures (nested tables, complex layouts).
        *   **Strategies for simplifying or selectively extracting from them:**
            *   **Tables:** For very complex tables, consider `html2text_options={'bypass_tables': True}`. Then, extract the table HTML separately (e.g., using `CrawlResult.html` and a CSS selector for the table) and process it with a specialized table parsing library or even an LLM call focused just on table interpretation.
            *   **Layouts:** Aggressive `RelevantContentFilter` strategies can help. If parts of a complex layout are consistently noise, use `CrawlerRunConfig.excluded_selector` to remove them before they even reach the Markdown generator.
    *   7.1.3. When to prefer `fit_markdown` over `raw_markdown` (or `markdown_with_citations`).
        *   **Reasoning:**
            *   **`fit_markdown`:** Best for tasks requiring high relevance and conciseness (e.g., RAG context, focused summarization). It reflects the output of your content filtering.
            *   **`raw_markdown` / `markdown_with_citations`:** Better when you need a broader representation of the page's textual content, or when the filtering might be too aggressive and discard potentially useful context. Also, if your `content_source` is already very clean (e.g., from a targeted CSS selector), the difference might be minimal.
    *   7.1.4. Balancing detail vs. conciseness for different LLM tasks (e.g., summarization vs. Q&A).
        *   **Summarization:** `fit_markdown` from a well-configured `LLMContentFilter` or `BM25ContentFilter` is often ideal. You might also use more aggressive `html2text_options` to remove minor elements.
        *   **Q&A / RAG:** You might prefer a slightly less aggressive filter or even `raw_markdown` (if `content_source` is clean) to ensure all potentially relevant details are available. Citations (`markdown_with_citations` and `references_markdown`) are crucial here for source tracking.

*   7.2. **Pre-processing HTML for Better Markdown**
    *   7.2.1. Using `CrawlerRunConfig.excluded_tags` or `excluded_selector` to remove noise before Markdown generation.
        *   **How:** These parameters in `CrawlerRunConfig` are applied by the `ContentScrapingStrategy` *before* the HTML even reaches the `MarkdownGenerationStrategy`.
        *   **Why:** This is the most efficient way to remove large, consistently irrelevant sections (like global headers, footers, sidebars, ad blocks) across all outputs (HTML, Markdown, etc.).
        *   *Code Example:*
            ```python
            # In CrawlerRunConfig
            # config = CrawlerRunConfig(
            #     excluded_tags=["nav", "footer", "script", "style"],
            #     excluded_selector=".ads, #social-share-buttons"
            # )
            ```
    *   7.2.2. The role of `ContentScrapingStrategy` (e.g., `LXMLWebScrapingStrategy` or the default `WebScrapingStrategy` using BeautifulSoup) in preparing the HTML that `DefaultMarkdownGenerator` receives.
        *   **Explanation:** The `ContentScrapingStrategy` is responsible for the initial cleaning of the HTML. Its output (what becomes `cleaned_html`) is the direct input to `DefaultMarkdownGenerator` if `content_source` is `"cleaned_html"`. Understanding how your chosen scraping strategy cleans HTML is key to predicting the input for Markdown generation. `LXMLWebScrapingStrategy` is generally faster and can be more robust for heavily malformed HTML.

*   7.3. **Post-processing Generated Markdown**
    *   7.3.1. When and why you might need to further process Markdown from `MarkdownGenerationResult`.
        *   **Scenarios:**
            *   Custom formatting not achievable with `html2text` options (e.g., specific table styles, unique list markers).
            *   Domain-specific transformations (e.g., converting certain patterns to custom shortcodes).
            *   Further cleaning or condensing based on rules `html2text` or content filters don't cover.
    *   7.3.2. *Example:* [Python snippet for custom regex replacements or structural adjustments on `raw_markdown`].
        ```python
        import re

        def custom_post_process_markdown(markdown_text):
            # Example: Replace all occurrences of "Crawl4AI" with "**Crawl4AI**"
            markdown_text = re.sub(r"Crawl4AI", r"**Crawl4AI**", markdown_text)
            
            # Example: Add a horizontal rule after every H2 heading
            markdown_text = re.sub(r"(^## .*)", r"\1\n\n---", markdown_text, flags=re.MULTILINE)
            return markdown_text

        # result = await crawler.arun(...)
        # if result.success:
        #     final_markdown = custom_post_process_markdown(result.markdown.raw_markdown)
        #     print(final_markdown)
        ```

*   7.4. **Combining Different Strategies for Optimal Results**
    *   7.4.1. *Scenario:* Using a `RelevantContentFilter` to get `fit_html`, then passing `fit_html` to a custom Markdown generator that expects highly focused input.
        *   **How:**
            1.  Instantiate your filter (e.g., `LLMContentFilter`).
            2.  Instantiate your custom Markdown generator (`MyCustomMarkdownStrategy`).
            3.  In `CrawlerRunConfig`, set `markdown_generator` to your custom generator.
            4.  Crucially, within your custom generator's `generate_markdown` method, ensure you *first* apply the `content_filter` (passed as an argument) to the `input_html` to get the `fit_html`, and then process this `fit_html` with your custom logic. Or, configure your custom generator's `content_source="fit_html"` and pass the filter during its initialization.
    *   7.4.2. *Scenario:* Using one set of `html2text_options` for `raw_markdown` and another for generating an alternative Markdown representation (perhaps for a different LLM or purpose).
        *   **How:** This would typically require two separate calls to `crawler.arun()` with different `CrawlerRunConfig` objects, each specifying a `DefaultMarkdownGenerator` with different `options`. Alternatively, a custom `MarkdownGenerationStrategy` could internally generate multiple Markdown versions with different settings and include them in custom fields within `MarkdownGenerationResult` (though this would require modifying or extending `MarkdownGenerationResult`).

## 8. Troubleshooting Common Markdown Generation Issues

*   8.1. **Problem: Markdown is too noisy / includes boilerplate**
    *   8.1.1. **Solutions:**
        *   **Use a `RelevantContentFilter`**:
            *   Start with `PruningContentFilter`. It's fast and good for common boilerplate.
                ```python
                # from crawl4ai.content_filter_strategy import PruningContentFilter
                # from crawl4ai import DefaultMarkdownGenerator
                # md_generator = DefaultMarkdownGenerator(content_filter=PruningContentFilter(threshold=0.5))
                ```
            *   If more precision is needed, try `BM25ContentFilter` with a relevant query or `LLMContentFilter` with clear instructions.
        *   **Refine `excluded_tags` or `excluded_selector` in `CrawlerRunConfig`**: This removes elements *before* any Markdown strategy sees them.
            ```python
            # run_config = CrawlerRunConfig(
            #     excluded_tags=["nav", "footer", "aside", "script"],
            #     excluded_selector=".ad-banner, #social-links"
            # )
            ```
        *   **Adjust `html2text_options`**: Options like `ignore_links`, `ignore_images`, `skip_internal_links` can reduce clutter.
            ```python
            # from crawl4ai import DefaultMarkdownGenerator
            # md_generator = DefaultMarkdownGenerator(options={"ignore_images": True, "ignore_links": True})
            ```

*   8.2. **Problem: Important content is missing from Markdown**
    *   8.2.1. **Solutions:**
        *   **Check if `content_filter` is too aggressive**: If using a filter, try lowering its threshold (e.g., `bm25_threshold` for `BM25ContentFilter`) or simplifying instructions for `LLMContentFilter`. Temporarily disable the filter to see if the content appears in `raw_markdown`.
        *   **Ensure `word_count_threshold` in `CrawlerRunConfig` (or scraping strategy) is not too high**: The default `WebScrapingStrategy` might have its own cleaning. If `CrawlerRunConfig.word_count_threshold` is too high, it might remove short but important paragraphs.
        *   **Verify `html2text_options` are not inadvertently removing desired content**: For example, if `ignore_links=True` is set, link text itself might still be there, but the link URL will be gone.
        *   **Examine `cleaned_html` or `fit_html`**: Inspect `result.markdown.fit_html` (if a filter was used) or `result.cleaned_html` (if no filter and `content_source` was `cleaned_html`). If the content is missing here, the issue is with HTML cleaning or filtering, not the Markdown conversion itself. If it's present in these HTML versions but not in the final Markdown, the issue is likely with `html2text_options` or the conversion process.

*   8.3. **Problem: Tables are mangled or poorly formatted**
    *   8.3.1. **Solutions:**
        *   **Try `html2text_options={'bypass_tables': True}`**: This tells `html2text` to skip converting tables.
            ```python
            # from crawl4ai import DefaultMarkdownGenerator
            # md_generator = DefaultMarkdownGenerator(options={"bypass_tables": True})
            # run_config = CrawlerRunConfig(markdown_generator=md_generator)
            # result = await crawler.arun(...)
            # # Now result.markdown.raw_markdown will not have Markdown tables.
            # # You'd need to parse tables from result.cleaned_html or result.markdown.fit_html
            ```
            You can then extract the table HTML directly from `result.cleaned_html` (or `result.markdown.fit_html`) using BeautifulSoup or lxml and parse it with a library better suited for complex tables (e.g., pandas `read_html`, or a custom parser).
        *   **Experiment with other `html2text` table formatting options**: Options like `pad_tables` might slightly improve appearance, but won't fix fundamentally complex table structures.
        *   **Consider if the table is truly a data table or a layout table**: Layout tables are often problematic for Markdown conversion and should ideally be filtered out by `PruningContentFilter` or more aggressive cleaning.

*   8.4. **Problem: Citations or references are incorrect/missing**
    *   8.4.1. **Solutions:**
        *   **Ensure links are present in the HTML input to `DefaultMarkdownGenerator`**: If the links were removed during an earlier HTML cleaning stage (e.g., by an aggressive `ContentScrapingStrategy` or `excluded_tags`), they can't be converted to citations.
        *   **Verify `ignore_links` is not `True` in `html2text_options`**: `DefaultMarkdownGenerator` relies on `CustomHTML2Text` to see the links to convert them. If `ignore_links=True`, the links are stripped before citation processing can occur.
        *   **Check for unusual link structures in the HTML**: Very non-standard link formats (e.g., heavily JavaScript-driven links without `href` attributes) might not be picked up. `CustomHTML2Text` primarily looks for standard `<a href="...">` tags.

*   8.5. **Problem: Markdown formatting is not ideal for a specific LLM**
    *   8.5.1. **Solutions:**
        *   **Fine-tune `html2text_options` extensively**: This is the first line of defense. Experiment with all available options (see Section 6.2) to control aspects like heading styles, list formatting, code block rendering, etc.
        *   **Consider a custom `MarkdownGenerationStrategy`**: If `html2text` options are insufficient, you might need to build your own strategy, possibly using a different Markdown conversion library or implementing custom transformation logic (see Section 2.1.3).
        *   **Implement post-processing steps**: After getting the Markdown from `MarkdownGenerationResult`, apply your own Python scripts (e.g., using regex) to further refine the formatting (see Section 7.3.2).

*   8.6. **Debugging Workflow**
    *   8.6.1. **Start with `raw_html` from `CrawlResult`**: `print(result.html)` This is the very first HTML fetched, before any processing. Is your target content even here?
    *   8.6.2. **Examine `cleaned_html` (or `fit_html`)**:
        *   If no content filter is used in `MarkdownGenerationStrategy`, inspect `result.cleaned_html`. This is what `DefaultMarkdownGenerator` (with `content_source="cleaned_html"`) will use.
        *   If a content filter *is* used, inspect `result.markdown.fit_html`. This is what `DefaultMarkdownGenerator` will use to produce `fit_markdown`.
        *   Is your target content present in these intermediate HTML stages?
    *   8.6.3. **Isolate the issue**:
        *   **HTML Cleaning/Scraping:** If content is missing from `cleaned_html` (but present in `raw_html`), the issue lies with the `ContentScrapingStrategy` or `CrawlerRunConfig` parameters like `excluded_tags`, `css_selector`, `target_elements`.
        *   **Content Filtering:** If content is in `cleaned_html` but missing from `fit_html`, the issue is with your `RelevantContentFilter` configuration.
        *   **Markdown Conversion:** If content is in `cleaned_html`/`fit_html` but malformed or missing in the final Markdown fields (`raw_markdown`, `fit_markdown`), the issue is likely with `html2text_options` or the `CustomHTML2Text` conversion process.
    *   8.6.4. **Use `verbose=True` in relevant configs**: Set `verbose=True` in `BrowserConfig` and `CrawlerRunConfig` for more detailed logging output from Crawl4AI, which can provide clues.

## 9. Conclusion and Next Steps

*   9.1. Recap of key strategies for effective Markdown generation.
    *   **Summary:** Crawl4AI provides a flexible Markdown generation pipeline. Start with `DefaultMarkdownGenerator`. Use `html2text_options` for stylistic control. Employ `RelevantContentFilter` strategies (`PruningContentFilter`, `BM25ContentFilter`, `LLMContentFilter`) to create focused `fit_markdown` for LLMs. Choose the appropriate `content_source` based on your needs. For highly custom requirements, implement your own `MarkdownGenerationStrategy`.
*   9.2. Pointers to other relevant documentation sections (e.g., `RelevantContentFilter` deep dive, `CustomHTML2Text` options in API reference).
    *   **Suggestion:** For a detailed breakdown of each `RelevantContentFilter`, see the "Content Filtering Strategies" guide. For an exhaustive list of `html2text` options, refer to the `CustomHTML2Text` API documentation or the original `html2text` library's documentation.
*   9.3. Encouragement for experimentation and community contributions.
    *   **Call to Action:** The best way to master Markdown generation is to experiment with different configurations and content types. If you develop useful custom strategies or identify improvements, consider contributing them back to the Crawl4AI community!

---
```

---


## Vibe Coding - Memory
Source: crawl4ai_vibe_memory_content.llm.md

Okay, I have read the "vibe" description for `crawl4ai`. Based on this, and adhering to the "memory" document type requirements, here is the detailed Markdown outline:

```markdown
# Detailed Outline for crawl4ai - vibe Component

**Target Document Type:** memory
**Target Output Filename Suggestion:** `llm_memory_vibe_coding.md`
**Library Version Context:** 0.6.3
**Outline Generation Date:** 2025-05-24
---

## 1. Vibe Coding with Crawl4AI: Core Concept

*   1.1. Purpose:
    *   Provides a conceptual framework for interacting with the `crawl4ai` library, particularly when using AI coding assistants.
    *   Aims to simplify the process of building web data applications by focusing on high-level capabilities and key building blocks, enabling users to guide AI assistants effectively even with limited direct `crawl4ai` API knowledge.
*   1.2. Principle:
    *   Describes how users can communicate their web scraping and data extraction goals to an AI assistant, which then translates these "vibes" or high-level intentions into `crawl4ai` Python code by leveraging knowledge of the library's core components and configurations.

## 2. `crawl4ai` High-Level Capabilities (for Vibe Prompts)

*   2.1. Fetching Webpages
    *   2.1.1. Description: The library can retrieve content from specified web URLs.
*   2.2. Converting Web Content to Clean Markdown
    *   2.2.1. Description: The library can process raw HTML content and convert it into a cleaned, structured Markdown format.
    *   2.2.2. Applications: Suitable for content summarization, input for Question & Answering systems, and as a pre-processing step for other LLMs.
*   2.3. Extracting Specific Information (JSON)
    *   2.3.1. Description: The library can extract targeted data elements from webpages and organize them into a JSON structure.
    *   2.3.2. Examples: Can be used to extract product names, prices from e-commerce sites, article headlines, author names, etc.
*   2.4. Crawling Multiple Pages
    *   2.4.1. Description: The library supports concurrent fetching and processing of a list of URLs.
*   2.5. Taking Screenshots and Generating PDFs
    *   2.5.1. Description: The library can capture visual representations of webpages as PNG screenshots or generate PDF documents.
*   2.6. Handling Simple Page Interactions
    *   2.6.1. Description: The library can execute JavaScript to simulate basic user interactions on a webpage, such as clicking buttons (e.g., "load more") or scrolling.

## 3. Key `crawl4ai` Building Blocks (API Reference for Vibe Coding Context)

*   3.1. Class `AsyncWebCrawler`
    *   3.1.1. Purpose: The primary entry point and main tool within `crawl4ai` for orchestrating web crawling and data extraction tasks.
    *   3.1.2. Initialization (`__init__`):
        *   Signature: `AsyncWebCrawler(self, crawler_strategy: Optional[AsyncCrawlerStrategy] = None, config: Optional[BrowserConfig] = None, base_directory: str = ..., thread_safe: bool = False, logger: Optional[AsyncLoggerBase] = None, **kwargs)`
        *   Parameters:
            *   `crawler_strategy (Optional[AsyncCrawlerStrategy])`: The underlying strategy for web crawling (e.g., `AsyncPlaywrightCrawlerStrategy`). Defaults to `AsyncPlaywrightCrawlerStrategy`.
            *   `config (Optional[BrowserConfig])`: Configuration for the browser instance. See section 3.5 for details.
            *   Other parameters are generally handled by defaults for vibe coding.
*   3.2. Method `AsyncWebCrawler.arun()`
    *   3.2.1. Purpose: Executes a crawl operation on a single URL or resource.
    *   3.2.2. Signature: `async def arun(self, url: str, config: Optional[CrawlerRunConfig] = None, **kwargs) -> RunManyReturn`
    *   3.2.3. Parameters:
        *   `url (str)`: The target resource.
            *   Description: Can be a standard web URL (e.g., "https://example.com"), a local file path (e.g., "file:///path/to/file.html"), or raw HTML content (e.g., "raw:<html>...</html>").
        *   `config (Optional[CrawlerRunConfig])`: An instance of `CrawlerRunConfig` specifying how this particular crawl run should be executed. See section 3.4 for details.
*   3.3. Method `AsyncWebCrawler.arun_many()`
    *   3.3.1. Purpose: Executes crawl operations on a list of URLs or resources, often concurrently.
    *   3.3.2. Signature: `async def arun_many(self, urls: List[str], config: Optional[CrawlerRunConfig] = None, dispatcher: Optional[BaseDispatcher] = None, **kwargs) -> RunManyReturn`
    *   3.3.3. Parameters:
        *   `urls (List[str])`: A list of target resources (URLs, file paths, raw HTML strings).
        *   `config (Optional[CrawlerRunConfig])`: An instance of `CrawlerRunConfig` applied to all URLs in the list. See section 3.4 for details.
*   3.4. Class `CrawlerRunConfig`
    *   3.4.1. Purpose: Configuration object for individual crawl runs, controlling aspects like content extraction, page interaction, and output formats.
    *   3.4.2. Key Parameters for Vibe Coding Context:
        *   `markdown_generator (Optional[MarkdownGenerationStrategy])`:
            *   Description: Specifies the strategy for generating Markdown.
            *   Default: An instance of `DefaultMarkdownGenerator`.
            *   Note for Vibe Coding: Can be `DefaultMarkdownGenerator(content_filter=PruningContentFilter())` for cleaner output.
        *   `extraction_strategy (Optional[ExtractionStrategy])`:
            *   Description: Specifies the strategy for extracting structured data.
            *   Supported Strategies (for Vibe Coding):
                *   `JsonCssExtractionStrategy`: For extracting data based on CSS selectors from structured HTML. Requires a `schema` dictionary.
                *   `LLMExtractionStrategy`: For extracting data using an LLM, often for complex or unstructured HTML. Requires an `LLMConfig` and an `instruction` or Pydantic model defining the desired output.
        *   `js_code (Optional[Union[str, List[str]]])`:
            *   Description: JavaScript code (or a list of code snippets) to be executed on the page after it loads.
        *   `wait_for (Optional[str])`:
            *   Description: A CSS selector or JavaScript expression. The crawler will wait for this condition to be met after `js_code` execution before proceeding.
        *   `session_id (Optional[str])`:
            *   Description: An identifier used to maintain the state of a browser page across multiple `arun` calls. Essential for multi-step interactions on the same page.
        *   `js_only (bool)`:
            *   Description: If `True` (and `session_id` is used), only executes `js_code` on the existing page without a full navigation/reload. Default is `False`.
        *   `screenshot (bool)`:
            *   Description: If `True`, captures a screenshot of the page. Result in `CrawlResult.screenshot`. Default is `False`.
        *   `pdf (bool)`:
            *   Description: If `True`, generates a PDF of the page. Result in `CrawlResult.pdf`. Default is `False`.
        *   `cache_mode (Optional[CacheMode])`:
            *   Description: Controls caching behavior.
            *   Type: `crawl4ai.cache_context.CacheMode` (Enum).
            *   Common Values: `CacheMode.ENABLED`, `CacheMode.BYPASS`.
*   3.5. Class `BrowserConfig`
    *   3.5.1. Purpose: Configures persistent browser-level settings for an `AsyncWebCrawler` instance.
    *   3.5.2. Key Parameters for Vibe Coding Context:
        *   `headless (bool)`:
            *   Description: If `True`, the browser runs without a visible UI. If `False`, the browser UI is shown.
            *   Default: `True`.
        *   `proxy_config (Optional[Union[ProxyConfig, Dict[str, str]]])`:
            *   Description: Configuration for using a proxy server.
            *   Structure (if dict): `{"server": "http://<host>:<port>", "username": "<user>", "password": "<pass>"}`.
        *   `user_agent (Optional[str])`:
            *   Description: Custom User-Agent string to be used by the browser.
*   3.6. Class `LLMConfig`
    *   3.6.1. Purpose: Configures settings for interacting with Large Language Models, used by `LLMExtractionStrategy`.
    *   3.6.2. Key Parameters:
        *   `provider (str)`:
            *   Description: Specifies the LLM provider and model identifier.
            *   Examples: "openai/gpt-4o-mini", "ollama/llama3", "anthropic/claude-3-opus-20240229".
        *   `api_token (Optional[str])`:
            *   Description: API key for the LLM provider. Can be the actual key or an environment variable reference (e.g., "env:OPENAI_API_KEY").
*   3.7. Class `CrawlResult`
    *   3.7.1. Purpose: The data object returned by `crawl4ai` operations, containing the results and metadata of a crawl.
    *   3.7.2. Key Attributes:
        *   `success (bool)`: `True` if the crawl was successful, `False` otherwise.
        *   `markdown (MarkdownGenerationResult)`: Object containing Markdown representations.
            *   `markdown.raw_markdown (str)`: Markdown generated directly from the cleaned HTML.
            *   `markdown.fit_markdown (str)`: Markdown potentially further processed by content filters.
        *   `extracted_content (Optional[str])`: JSON string of structured data if an `ExtractionStrategy` was used and successful.
        *   `links (Links)`: Object containing `internal` and `external` lists of `Link` objects. Each `Link` object has `href`, `text`, `title`.
        *   `media (Media)`: Object containing lists of `MediaItem` for `images`, `videos`, `audios`, and `tables`. Each `MediaItem` has `src`, `alt`, `score`, etc.
        *   `screenshot (Optional[str])`: Base64 encoded string of the PNG screenshot, if `screenshot=True`.
        *   `pdf (Optional[bytes])`: Raw bytes of the PDF document, if `pdf=True`.
        *   `error_message (Optional[str])`: Description of the error if `success` is `False`.

## 4. Common `crawl4ai` Usage Patterns (Vibe Recipes Mapped to Components)

*   4.1. Task: Get Clean Markdown from a Page
    *   4.1.1. Description: Fetch a single webpage and convert its main content into clean Markdown.
    *   4.1.2. Key `crawl4ai` elements:
        *   `AsyncWebCrawler`
        *   `arun()` method.
        *   `CrawlerRunConfig`:
            *   `markdown_generator`: Typically `DefaultMarkdownGenerator()`. For very clean output, `DefaultMarkdownGenerator(content_filter=PruningContentFilter())`.
*   4.2. Task: Extract All Product Names and Prices from an E-commerce Category Page
    *   4.2.1. Description: Scrape structured data (e.g., product names, prices) from a page with repeating elements.
    *   4.2.2. Key `crawl4ai` elements:
        *   `AsyncWebCrawler`
        *   `arun()` method.
        *   `CrawlerRunConfig`:
            *   `extraction_strategy`: `JsonCssExtractionStrategy(schema={"name_field": "h2.product-title", "price_field": "span.price"})`. The schema's CSS selectors identify where to find the data.
*   4.3. Task: Extract Key Information from an Article using an LLM
    *   4.3.1. Description: Use an LLM to parse an article and extract specific fields like author, date, and a summary into a JSON format.
    *   4.3.2. Key `crawl4ai` elements:
        *   `AsyncWebCrawler`
        *   `arun()` method.
        *   `CrawlerRunConfig`:
            *   `extraction_strategy`: `LLMExtractionStrategy(llm_config=..., instruction=..., schema=...)`.
        *   `LLMConfig`: Instance specifying `provider` (e.g., "openai/gpt-4o-mini") and `api_token`.
        *   Schema for `LLMExtractionStrategy`: Can be a Pydantic model definition or a dictionary describing the target JSON structure.
*   4.4. Task: Crawl Multiple Pages of a Blog (Clicking "Next Page")
    *   4.4.1. Description: Navigate through paginated content by simulating clicks on "Next Page" or similar links, collecting data from each page.
    *   4.4.2. Key `crawl4ai` elements:
        *   `AsyncWebCrawler`
        *   Multiple sequential calls to `arun()` (typically in a loop).
        *   `CrawlerRunConfig` (reused or cloned for each step):
            *   `session_id`: A consistent identifier (e.g., "blog_pagination_session") to maintain the browser state across `arun` calls.
            *   `js_code`: JavaScript to trigger the "Next Page" action (e.g., `document.querySelector('a.next-page-link').click();`).
            *   `wait_for`: A CSS selector or JavaScript condition to ensure the new page content has loaded before proceeding.
            *   `js_only=True`: For subsequent `arun` calls after the initial page load to indicate only JS interaction without full navigation.
*   4.5. Task: Get Screenshots of a List of URLs
    *   4.5.1. Description: Capture screenshots for a batch of URLs.
    *   4.5.2. Key `crawl4ai` elements:
        *   `AsyncWebCrawler`
        *   `arun_many()` method.
        *   `CrawlerRunConfig`:
            *   `screenshot=True`.

## 5. Key Input Considerations for `crawl4ai` Operations (Inferred from Vibe Prompting Tips)

*   5.1. Clear Objective: `crawl4ai` operations are guided by the configuration. The configuration should reflect the user's goal (e.g., Markdown generation, specific data extraction, media capture).
*   5.2. URL Input: The `arun` method requires a single `url` string. `arun_many` requires a `List[str]` of URLs.
*   5.3. Structured Data Extraction Guidance:
    *   For `JsonCssExtractionStrategy`, the `schema` parameter (a dictionary mapping desired field names to CSS selectors) is essential.
    *   For `LLMExtractionStrategy`, the `instruction` parameter (natural language description of desired data) and/or a `schema` (Pydantic model or dictionary) are crucial, along with a configured `LLMConfig`.
*   5.4. LLM Configuration: When `LLMExtractionStrategy` is used, an `LLMConfig` instance specifying `provider` and `api_token` (if applicable) must be provided.
*   5.5. Dynamic Page Handling: For pages requiring interaction, `CrawlerRunConfig` parameters like `js_code`, `wait_for`, `session_id`, and `js_only` are used.

## 6. Expected Output Data from `crawl4ai` Operations (Accessing `CrawlResult`)

*   6.1. Generated Python Code: When using an AI assistant with `crawl4ai` context, the AI is expected to generate Python code that utilizes `crawl4ai` classes and methods.
*   6.2. `CrawlResult` Object: The primary output of `arun()` and `arun_many()` calls.
    *   `result.success (bool)`: Indicates if the individual crawl operation was successful.
    *   `result.markdown.raw_markdown (str)` / `result.markdown.fit_markdown (str)`: Contains the generated Markdown content.
    *   `result.extracted_content (Optional[str])`: Contains the JSON string of structured data if an extraction strategy was successful.
    *   `result.links (Links)`: Provides access to lists of internal and external links.
    *   `result.media (Media)`: Provides access to lists of images, videos, audio files, and tables.
    *   `result.screenshot (Optional[str])`: Base64 encoded screenshot data.
    *   `result.pdf (Optional[bytes])`: Raw PDF data.
    *   `result.error_message (Optional[str])`: Error details if `success` is `False`.
*   6.3. Files on Disk: Operations like screenshot or PDF generation, or custom code within an AI-generated script, might save files to the local disk (e.g., PNGs, PDFs, JSON files). The paths depend on the configuration or the custom code.

```

---


## Vibe Coding - Reasoning
Source: crawl4ai_vibe_reasoning_content.llm.md

```markdown
# Detailed Outline for crawl4ai - vibe Component

**Target Document Type:** reasoning
**Target Output Filename Suggestion:** `llm_reasoning_vibe.md`
**Library Version Context:** 0.6.3
**Outline Generation Date:** 2025-05-24
---

# Vibe Coding with Crawl4AI: A Guide for AI Coding Assistants

## 1. Introduction: What is "Vibe Coding" with Crawl4AI?

*   **1.1. Explaining the Core Idea:**
    "Vibe Coding" with Crawl4AI is about empowering users, even those with limited direct coding experience, to build powerful web data applications. Instead of meticulously writing every line of Python, you focus on clearly describing your data extraction or web interaction goals to an AI coding assistant. This guide teaches you how to provide that AI assistant with the *right context* about Crawl4AI, so it can generate effective and efficient code for you. The goal is to translate your "vibe"  your high-level intent  into working Crawl4AI solutions quickly.

*   **1.2. Who is this Guide For?**
    This guide is designed for:
    *   **Users new to web scraping or `crawl4ai`:** If you prefer to articulate your needs in natural language and have an AI assistant handle the code generation, this guide is for you.
    *   **Data analysts, researchers, and product managers:** Anyone who needs web data but doesn't want to get bogged down in the intricacies of web scraping libraries.
    *   **Developers looking for rapid prototyping:** Even experienced developers can use "vibe coding" to quickly generate boilerplate or test ideas with `crawl4ai` before refining the code.
    *   **AI Coding Assistant Users:** This guide helps you understand what information to feed your AI to get the best `crawl4ai` code.

*   **1.3. How this Guide Helps You (and Your AI Assistant):**
    By understanding the concepts in this guide, you (and by extension, your AI assistant) will:
    *   Grasp the high-level capabilities of `crawl4ai` that are most relevant for prompting an AI.
    *   Learn the key terminology and building blocks of `crawl4ai` to include in your prompts for precise code generation.
    *   Discover common "vibe recipes"  typical data extraction tasks and how to prompt an AI to solve them using `crawl4ai`.
    *   Pick up effective prompting patterns to maximize the quality of AI-generated `crawl4ai` code.

## 2. High-Level Capabilities of Crawl4AI (What to Tell Your AI Assistant Crawl4AI Can Do)

When you're "vibe coding" with your AI assistant, you don't need to explain every nuance of `crawl4ai`. Instead, focus on what it *can do* for you. Here's a high-level overview of capabilities you can confidently tell your AI assistant about:

*   **2.1. Fetching Any Webpage:**
    *   **How to tell your AI:** "Crawl4AI can fetch the content of any webpage, whether it's a simple static page or a complex JavaScript-heavy application."
    *   **Why it's important:** This establishes the fundamental capability  getting the raw HTML from a target URL.

*   **2.2. Converting Web Content into Clean Markdown:**
    *   **How to tell your AI:** "Crawl4AI is great at turning messy web pages into clean, readable Markdown. This is perfect if I need to summarize an article, feed content into another LLM for Q&A, or just get the main text."
    *   **Why it's important:** Markdown is often the desired end-format for LLM-based tasks, and `crawl4ai` simplifies this conversion.

*   **2.3. Extracting Specific Pieces of Information (Structured Data/JSON):**
    *   **How to tell your AI:** "If I need specific data from a page, like all the product names and prices from an e-commerce site, or all the headlines from a news page, Crawl4AI can extract that and give it to me as structured JSON."
    *   **Why it's important:** This highlights `crawl4ai`'s ability to go beyond simple text extraction and pull out specific, targeted information.

*   **2.4. Crawling Multiple Pages at Once:**
    *   **How to tell your AI:** "If I have a list of URLs, Crawl4AI can process them all efficiently, often in parallel, instead of me having to do them one by one."
    *   **Why it's important:** This points to the library's capability for batch processing, saving significant time for larger tasks.

*   **2.5. Taking Screenshots or Making PDFs of Pages:**
    *   **How to tell your AI:** "Crawl4AI can take a screenshot of a webpage or save it as a PDF document. This is useful for archiving, visual checks, or sharing."
    *   **Why it's important:** Visual capture and PDF generation are common requirements that `crawl4ai` handles easily.

*   **2.6. Handling Simple Page Interactions:**
    *   **How to tell your AI:** "For pages that need a bit of interaction, like clicking a 'load more' button to reveal more content or scrolling down, Crawl4AI can execute JavaScript to do that."
    *   **Why it'S important:** This shows that `crawl4ai` isn't limited to static content and can handle basic dynamic page elements to access more data.

## 3. Key Building Blocks: Communicating Crawl4AI Concepts to Your AI Assistant

To get the best results from your AI assistant, you'll want to mention the key components of `crawl4ai` it should use. Think of these as the essential vocabulary for your "vibe" prompts.

*   **3.1. `AsyncWebCrawler`**: The Main Tool
    *   **3.1.1. Explaining its Role:**
        *   **What to tell your AI:** "The main tool we'll use in `crawl4ai` is called `AsyncWebCrawler`. We always need to create an instance of this first. It's best to use it with an `async with` statement to make sure resources are managed correctly."
        *   **Why it's important for the AI:** This directs the AI to the central class for all crawling operations and hints at the asynchronous nature and proper resource management.
    *   **3.1.2. Basic Usage Pattern:**
        *   **Vibe Prompt:** "Let's start a `crawl4ai` script. We'll need an `AsyncWebCrawler`."
        *   **Expected AI action:** The AI should generate the basic setup:
            ```python
            import asyncio
            from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig # Common imports

            async def main():
                # browser_config = BrowserConfig(...) # Optional, if needed
                async with AsyncWebCrawler(# config=browser_config
                                          ) as crawler:
                    # ... your crawl logic will go here ...
                    pass

            if __name__ == "__main__":
                asyncio.run(main())
            ```

*   **3.2. `arun(url, config)`**: Crawling a Single Page
    *   **3.2.1. Explaining its Purpose:**
        *   **What to tell your AI:** "To crawl a single page, we'll use the `arun` method of the `AsyncWebCrawler` instance. It takes the URL and an optional configuration object."
        *   **Why it's important for the AI:** This points to the primary method for individual page crawls.
    *   **3.2.2. Guiding the AI on `url` Specification:**
        *   **What to tell your AI:** "The `url` for `arun` can be a regular web address (like 'https://example.com'), a local file path (if it starts with `file://`), or even raw HTML content (if it starts with `raw:`)."
        *   **Vibe Prompts & AI Interpretation:**
            *   "Crawl this URL: https://my-target-site.com" -> AI uses `url="https://my-target-site.com"`
            *   "Process this local HTML file: /path/to/my/page.html" -> AI uses `url="file:///path/to/my/page.html"`
            *   "I have some raw HTML I want to parse: `<html><body><h1>Test</h1></body></html>`" -> AI uses `url="raw:<html><body><h1>Test</h1></body></html>"`
    *   **3.2.3. Emphasizing `config` (`CrawlerRunConfig`):**
        *   **What to tell your AI:** "The most important part for customizing a crawl is the `config` parameter of `arun`. This is where we'll use `CrawlerRunConfig` to tell `crawl4ai` *how* to crawl, what to extract, and what to do with the page."
        *   **Vibe Prompt:** "...and when you call `arun`, pass in a `CrawlerRunConfig` object with these settings: [describe settings]."

*   **3.3. `arun_many(urls, config)`**: Crawling Multiple Pages
    *   **3.3.1. Explaining its Purpose:**
        *   **What to tell your AI:** "If I have a list of URLs, we should use `arun_many` instead of calling `arun` in a loop. It's more efficient for batch crawling."
        *   **Why it's important for the AI:** Guides the AI to use the optimized method for multiple URLs, which handles concurrency internally.
    *   **3.3.2. Vibe Prompt:**
        *   "Crawl all these pages: https://example.com/page1, https://example.com/page2, https://example.com/page3."
        *   **Expected AI action:**
            ```python
            # ... inside async def main() and async with AsyncWebCrawler ...
            list_of_urls = [
                "https://example.com/page1",
                "https://example.com/page2",
                "https://example.com/page3"
            ]
            # run_config = CrawlerRunConfig(...) # Define config for all URLs
            results = await crawler.arun_many(urls=list_of_urls, config=run_config)
            for result in results:
                # process each result
                pass
            ```

*   **3.4. `CrawlerRunConfig`**: Tailoring the Crawl (Simplified for Vibe Coding)
    *   **3.4.1. Getting Markdown:**
        *   **Basic Request Vibe:** "Get the Markdown for this page [URL]."
            *   **AI's understanding:** The AI should know that `crawl4ai` produces Markdown by default, so a very simple `CrawlerRunConfig()` or even `None` might suffice if no other settings are needed.
        *   **For Cleaner Markdown Vibe:** "I need very clean Markdown from [URL], suitable for feeding into another LLM. Remove boilerplate like headers, footers, and navigation."
            *   **AI's understanding:** "Ah, the user wants cleaner output. I should suggest or use `DefaultMarkdownGenerator` combined with `PruningContentFilter`. The `PruningContentFilter` heuristically tries to remove common non-content sections."
            *   **Code Snippet AI might generate:**
                ```python
                from crawl4ai import CrawlerRunConfig, DefaultMarkdownGenerator
                from crawl4ai.content_filter_strategy import PruningContentFilter

                markdown_generator = DefaultMarkdownGenerator(
                    content_filter=PruningContentFilter()
                )
                run_config = CrawlerRunConfig(
                    markdown_generator=markdown_generator
                )
                # result = await crawler.arun(url="...", config=run_config)
                # print(result.markdown.fit_markdown) # Access the filtered markdown
                ```
            *   **Reasoning to convey (implicitly or explicitly):** `PruningContentFilter` is a good first step for general boilerplate removal. `DefaultMarkdownGenerator` then converts the filtered HTML to Markdown. Accessing `result.markdown.fit_markdown` gives this cleaner version.
    *   **3.4.2. Extracting Specific Data (JSON):**
        *   **3.4.2.1. Using `JsonCssExtractionStrategy` for Structured Pages:**
            *   **When to suggest it (Vibe):** "The page I want to crawl ([URL]) has a list of items, like products on an e-commerce category page, and each item has a similar layout. I want to extract these repeating items into a JSON list."
            *   **Information to provide the AI (Vibe):** "For each item, I want to get the 'product_name', which is usually in an `<h2>` tag, and the 'price', which seems to be in a `<span>` tag with a class like 'price-tag' or 'current-price'."
            *   **AI's Role & Reasoning:** The AI should recognize this pattern and suggest `JsonCssExtractionStrategy`. It understands that the user is describing a schema. The AI's job is to translate "name from h2" into `{"name": "product_name", "selector": "h2", "type": "text"}` within the `fields` list of a schema dictionary, and the overall repeating item selector into `baseSelector`. The AI should also know to set `extraction_type="schema"` on `LLMExtractionStrategy` if it were using that for schema generation, but here it's direct CSS.
            *   **Code Snippet AI might generate:**
                ```python
                from crawl4ai import CrawlerRunConfig
                from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

                # AI would help construct this schema based on user's description
                schema = {
                    "name": "ProductList",
                    "baseSelector": "div.product-item", # Example selector for each product block
                    "fields": [
                        {"name": "product_name", "selector": "h2.product-title", "type": "text"},
                        {"name": "price", "selector": "span.price-tag", "type": "text"}
                    ]
                }
                extraction_strategy = JsonCssExtractionStrategy(schema=schema)
                run_config = CrawlerRunConfig(extraction_strategy=extraction_strategy)
                # result = await crawler.arun(url="...", config=run_config)
                # if result.success and result.extracted_content:
                #     products = json.loads(result.extracted_content)
                #     for product in products:
                #         print(f"Name: {product.get('product_name')}, Price: {product.get('price')}")
                ```
        *   **3.4.2.2. Using `LLMExtractionStrategy` for Complex/Unclear Structures:**
            *   **When to suggest it (Vibe):** "The page ([URL]) has the information I want, but it's not in a clear, repeating list, or it's mixed in with a lot of text. I need the AI to understand the content to pull out specific details." Or, "I want to extract information that requires some interpretation, like summarizing a paragraph."
            *   **Information to provide the AI (Vibe):**
                *   "Use `LLMExtractionStrategy` for this."
                *   "The LLM I want to use is [LLM provider/model, e.g., 'openai/gpt-4o-mini'] and my API key is [YOUR_API_KEY_OR_ENV_VAR_NAME] (or tell it to look for an env var)."
                *   **Option A (Describing fields):** "I need a JSON object with the following fields: 'author_name', 'article_publish_date', and a 'short_summary' (about 2 sentences)."
                *   **Option B (Example JSON):** "The JSON output should look something like this: `{\"author\": \"Jane Doe\", \"published_on\": \"2024-05-23\", \"summary\": \"This article discusses...\"}`."
                *   **Option C (Pydantic Model - more advanced but best for AI):** "Here's a Pydantic model that defines the structure I want: [Pydantic Class Code Snippet]. Use this for the schema."
            *   **AI's Role & Reasoning:** The AI needs to construct an `LLMConfig` and an `LLMExtractionStrategy`. If the user provides field descriptions or an example JSON, the AI can generate a simple schema dictionary. If a Pydantic model is provided, the AI should use `MyPydanticModel.model_json_schema()` to create the schema for `LLMExtractionStrategy`. This strategy is powerful because it leverages the LLM's understanding.
            *   **Code Snippet AI might generate (with Pydantic example):**
                ```python
                from crawl4ai import CrawlerRunConfig, LLMConfig
                from crawl4ai.extraction_strategy import LLMExtractionStrategy
                from pydantic import BaseModel, Field # Assuming user might provide this

                # User might provide this, or AI generates it from description
                class ArticleInfo(BaseModel):
                    author_name: str = Field(description="The main author of the article")
                    publication_date: str = Field(description="The date the article was published, e.g., YYYY-MM-DD")
                    short_summary: str = Field(description="A concise 2-3 sentence summary of the article")

                llm_config = LLMConfig(
                    provider="openai/gpt-4o-mini", # Or user's choice
                    api_token="env:OPENAI_API_KEY" # Or direct key if user insists and understands risk
                )
                extraction_strategy = LLMExtractionStrategy(
                    llm_config=llm_config,
                    schema=ArticleInfo.model_json_schema(),
                    # instruction="Extract author, publication date, and a summary." # Could also be used
                    extraction_type="schema" # Important for Pydantic/JSON schema
                )
                run_config = CrawlerRunConfig(extraction_strategy=extraction_strategy)
                # result = await crawler.arun(url="...", config=run_config)
                # if result.success and result.extracted_content:
                #     article_data = json.loads(result.extracted_content) # Or ArticleInfo.model_validate_json(result.extracted_content)
                #     print(article_data)
                ```
    *   **3.4.3. Interacting with Pages (Dynamic Content):**
        *   **How to tell your AI (Vibe):** "This page ([URL]) loads more content when you scroll down, or when you click a 'Show More' button. `crawl4ai` needs to perform this interaction."
        *   **For clicking (Vibe):** "To get all the data, we need to click the button with text 'Load All Comments'."
            *   **AI's understanding:** This requires `js_code` to find and click the button. The AI should be guided that finding elements by text might involve more complex JS like `Array.from(document.querySelectorAll('button')).find(btn => btn.textContent.includes('Load All Comments')).click();`.
        *   **For scrolling (Vibe):** "Scroll to the bottom of the page to make sure everything loads."
            *   **AI's understanding:** `js_code` like `window.scrollTo(0, document.body.scrollHeight);`
        *   **Ensuring actions complete (Vibe):** "After clicking 'Load More', wait for the new items to appear. They usually show up in a `div` with class `comment-list` and we expect more than 10 comments."
            *   **AI's understanding:** Use `wait_for`. This can be a CSS selector (e.g., `wait_for="css:.comment-list .comment-item:nth-child(11)"`) or a JS condition (e.g., `wait_for="js:() => document.querySelectorAll('.comment-item').length > 10"`).
        *   **For multi-step interactions on the same page (Vibe):** "I need to first click 'Agree to Cookies', then click 'Show Details', then extract the text. Make sure these happen on the same page view."
            *   **AI's understanding:** "This requires a `session_id` to persist the page state across multiple `arun` calls. Each `arun` call will perform one step of the interaction."
        *   **If only JS interaction is needed (Vibe):** "After the first page load, the next actions (like clicking 'Next Page') only update part of the page with JavaScript, they don't reload everything."
            *   **AI's understanding:** "For these subsequent `arun` calls within the same session, set `js_only=True` in `CrawlerRunConfig` to prevent unnecessary full page navigations, making it faster."
    *   **3.4.4. Taking Screenshots or PDFs:**
        *   **Simple request (Vibe):** "Get me a screenshot of [URL]."
            *   **AI's understanding:** Set `screenshot=True` in `CrawlerRunConfig`.
        *   **Or (Vibe):** "I need a PDF version of [URL]."
            *   **AI's understanding:** Set `pdf=True` in `CrawlerRunConfig`.
    *   **3.4.5. Caching for Speed and Freshness:**
        *   **For faster testing/development (Vibe):** "I'm testing my script for [URL]. Can you make it use the cache so it's faster after the first run?"
            *   **AI's understanding:** Use `cache_mode=CacheMode.ENABLED`.
            *   **Reasoning:** This saves time during development by re-using previously fetched content.
        *   **To get the latest data (Vibe):** "I need the absolute latest version of [URL], don't use any cached data."
            *   **AI's understanding:** Use `cache_mode=CacheMode.BYPASS`.
            *   **Reasoning:** Ensures `crawl4ai` fetches a fresh copy from the web, ignoring any local cache. This is crucial for production or when data changes frequently.

*   **3.5. `BrowserConfig`**: Browser-Level Settings (Simplified for Vibe Coding)
    *   **3.5.1. When it's usually not needed:**
        *   **What to tell your AI (or what it should infer):** "For most simple crawls where you just need to fetch a page and get its content, you probably don't need to worry about `BrowserConfig`. `crawl4ai` uses sensible defaults."
    *   **3.5.2. Headless Mode:**
        *   **Default behavior (Vibe):** "By default, `crawl4ai` runs 'headless', which means you won't see a browser window pop up. This is usually what we want for automated scripts."
        *   **To see the browser (Vibe):** "I'm having trouble with [URL], and I want to see what the browser is doing. Can you make it run in non-headless mode?"
            *   **AI's understanding:** The user wants to visually debug. Set `headless=False` in `BrowserConfig`.
    *   **3.5.3. Using a Proxy:**
        *   **How to ask (Vibe):** "I need to make `crawl4ai` use a proxy server for this crawl."
        *   **Information to provide (Vibe):** "The proxy server address is `http://myproxy.server.com:8080`. If it needs a username and password, they are `myuser` and `mypass`. Make sure to use the `proxy_config` setting in `BrowserConfig`."
            *   **AI's understanding:** The AI should construct a `ProxyConfig` object (or dictionary that `BrowserConfig` can handle) and pass it to `BrowserConfig`.
    *   **3.5.4. Changing User Agent:**
        *   **How to ask (Vibe):** "The website [URL] might be blocking default user agents. Can we make `crawl4ai` look like it's Firefox on a Mac?"
        *   **Information to provide (Vibe):** "You can set a custom `user_agent` string in `BrowserConfig`. For example, 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/115.0'."
            *   **AI's understanding:** The AI should pass the provided string to the `user_agent` parameter of `BrowserConfig`.

*   **3.6. `LLMConfig`**: Configuring Language Models (Simplified for Vibe Coding)
    *   **3.6.1. When it's needed:**
        *   **What to tell your AI:** "If we're using `LLMExtractionStrategy` to extract structured data or `LLMContentFilter` to clean up content, we need to tell `crawl4ai` which language model to use. This is done with an `LLMConfig` object."
    *   **3.6.2. Information to provide the AI (Vibe):**
        *   **Model choice:** "For this task, let's use the `provider` called 'openai/gpt-4o-mini'." (Other examples: 'ollama/llama3', 'anthropic/claude-3-opus-20240229').
        *   **API Key:** "My `api_token` for this provider is [YOUR_API_KEY_PLACEHOLDER]. (Best practice is to tell the AI to get it from an environment variable, e.g., 'env:OPENAI_API_KEY')."
            *   **AI's understanding:** The AI will create an `LLMConfig(provider="...", api_token="...")` and pass it to the relevant strategy.
            *   **Code Snippet AI might generate:**
                ```python
                from crawl4ai import LLMConfig
                # For OpenAI
                llm_conf = LLMConfig(provider="openai/gpt-4o-mini", api_token="env:OPENAI_API_KEY")
                # For Ollama (locally running Llama3)
                # llm_conf = LLMConfig(provider="ollama/llama3") # api_token often not needed for local Ollama
                ```

*   **3.7. The `CrawlResult`**: Understanding What You Get Back
    *   **3.7.1. Checking for Success:**
        *   **What to tell your AI (Crucial Vibe):** "When `crawl4ai` finishes an `arun` or `arun_many` call, the most important first step is to check if it was successful. Tell the AI to always generate code that checks `result.success`. This will be `True` or `False`."
        *   **If `False` (Vibe):** "If `result.success` is `False`, the AI should print or log `result.error_message` to tell us what went wrong."
    *   **3.7.2. Accessing Markdown Content:**
        *   **Raw Markdown (Vibe):** "The main text content of the page, converted to Markdown, is usually in `result.markdown.raw_markdown`."
        *   **Filtered Markdown (Vibe):** "If we used a content filter (like `PruningContentFilter`), the cleaner, more focused Markdown will be in `result.markdown.fit_markdown`."
    *   **3.7.3. Accessing Extracted Structured Data (JSON):**
        *   **Where to find it (Vibe):** "If we asked `crawl4ai` to extract specific structured data (using `JsonCssExtractionStrategy` or `LLMExtractionStrategy`), that data will be in `result.extracted_content`."
        *   **How to use it (Vibe):** "The `result.extracted_content` is a JSON string. To use it in Python, tell the AI to parse it using `json.loads()`."
    *   **3.7.4. Accessing Links:**
        *   **What it contains (Vibe):** "`result.links` is a dictionary. It usually has two keys: 'internal' and 'external'. Each of these is a list of links found on the page, with details like the link text and URL."
    *   **3.7.5. Accessing Media Information:**
        *   **What it contains (Vibe):** "`result.media` is another dictionary. It can contain lists of 'images', 'videos', or 'audios' found on the page, along with their URLs and sometimes alt text or descriptions."
    *   **3.7.6. Accessing Screenshots and PDFs:**
        *   **Screenshots (Vibe):** "If we set `screenshot=True` in the config, the screenshot image data (which is a base64 encoded string) will be in `result.screenshot`."
        *   **PDFs (Vibe):** "If we set `pdf=True`, the PDF data (as raw bytes) will be in `result.pdf`."

## 4. Common "Vibe" Recipes & Prompting Patterns for AI Assistants

Here are some common scenarios and how you can prompt your AI assistant to use `crawl4ai` to solve them.

*   **4.1. Recipe 1: Get Clean Markdown from a Page**
    *   **4.1.1. User Vibe Prompt to AI:**
        *   *Example 1 (Basic):* "Hey AI, can you use `crawl4ai` to get me the clean Markdown content of `https://blog.example.com/my-latest-post`?"
        *   *Example 2 (Requesting more cleaning):* "AI, I need *very* clean Markdown from `https://some.news.site/article123`. It's for an LLM, so please try to remove ads, navigation, and footers."
    *   **4.1.2. Key `crawl4ai` elements the AI should use (and why):**
        *   **`AsyncWebCrawler`:** The starting point for any crawl.
        *   **`arun`:** Since it's a single URL.
        *   **`CrawlerRunConfig`:**
            *   For Example 1, the default `DefaultMarkdownGenerator` might be sufficient.
            *   For Example 2 ("very clean"), the AI should be guided to use `DefaultMarkdownGenerator(content_filter=PruningContentFilter())`.
            *   **Reasoning:** `PruningContentFilter` is designed to heuristically remove common boilerplate like headers, footers, and navigation bars before Markdown conversion, leading to cleaner text suitable for LLMs.
    *   **4.1.3. Expected Output from AI-generated code:**
        *   A Python script that initializes `AsyncWebCrawler`, calls `arun` with the appropriate URL and config.
        *   The script should then access and print (or save) `result.markdown.raw_markdown` (for basic) or `result.markdown.fit_markdown` (if `PruningContentFilter` was used).
        *   **Code Example (for "very clean"):**
            ```python
            import asyncio
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator
            from crawl4ai.content_filter_strategy import PruningContentFilter

            async def get_clean_markdown(url_to_crawl):
                markdown_generator = DefaultMarkdownGenerator(
                    content_filter=PruningContentFilter()
                )
                run_config = CrawlerRunConfig(
                    markdown_generator=markdown_generator,
                    cache_mode="BYPASS" # Ensure fresh crawl for demo
                )
                async with AsyncWebCrawler() as crawler:
                    result = await crawler.arun(url=url_to_crawl, config=run_config)
                    if result.success:
                        print(f"--- Fit Markdown for {url_to_crawl} ---")
                        print(result.markdown.fit_markdown)
                        # You might also want to see raw_markdown to compare
                        # print(f"--- Raw Markdown for {url_to_crawl} ---")
                        # print(result.markdown.raw_markdown)
                    else:
                        print(f"Failed to crawl {url_to_crawl}: {result.error_message}")

            # asyncio.run(get_clean_markdown("https://en.wikipedia.org/wiki/Python_(programming_language)"))
            ```

*   **4.2. Recipe 2: Extract All Product Names and Prices from an E-commerce Category Page**
    *   **4.2.1. User Vibe Prompt to AI:**
        *   *Example:* "AI, I need to use `crawl4ai` to get all product names and their prices from `https://www.example-store.com/laptops`. On that page, product names look like they are in `<h3>` tags with a class `product-title`, and prices are in `<span>` elements with the class `final-price`."
    *   **4.2.2. Key `crawl4ai` elements AI should use (and why):**
        *   **`AsyncWebCrawler`**, **`arun`**.
        *   **`CrawlerRunConfig`** with **`JsonCssExtractionStrategy`**.
            *   **Reasoning:** The user described a page with repeating structured items. `JsonCssExtractionStrategy` is ideal for this as it uses CSS selectors to pinpoint the data. The AI's task is to translate the user's description of element locations into a valid schema for the strategy.
            *   The AI needs to understand that `baseSelector` in the schema should target the container for each product, and `fields` will target individual pieces of data within that container.
    *   **4.2.3. Expected Output from AI-generated code:**
        *   A Python script that defines the schema dictionary.
        *   Initializes `JsonCssExtractionStrategy` with this schema.
        *   Passes the strategy to `CrawlerRunConfig`.
        *   After `arun`, it parses `result.extracted_content` using `json.loads()` and likely iterates through the list of extracted product dictionaries.
        *   **Code Example:**
            ```python
            import asyncio
            import json
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
            from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

            async def extract_products(url_to_crawl):
                # AI helps create this schema based on user's description
                product_schema = {
                    "name": "LaptopList",
                    "baseSelector": "div.product-listing-item", # Hypothetical selector for each product's container
                    "fields": [
                        {"name": "product_name", "selector": "h3.product-title", "type": "text"},
                        {"name": "price", "selector": "span.final-price", "type": "text"}
                    ]
                }
                extraction_strategy = JsonCssExtractionStrategy(schema=product_schema)
                run_config = CrawlerRunConfig(
                    extraction_strategy=extraction_strategy,
                    cache_mode="BYPASS"
                )
                async with AsyncWebCrawler() as crawler:
                    result = await crawler.arun(url=url_to_crawl, config=run_config)
                    if result.success and result.extracted_content:
                        products = json.loads(result.extracted_content)
                        print(f"Found {len(products)} products:")
                        for i, product in enumerate(products[:3]): # Print first 3
                            print(f"  Product {i+1}: Name='{product.get('product_name')}', Price='{product.get('price')}'")
                    else:
                        print(f"Failed to extract products from {url_to_crawl}: {result.error_message}")

            # asyncio.run(extract_products("https://www.example-store.com/laptops")) # Replace with a real URL for testing
            ```

*   **4.3. Recipe 3: Extract Key Information from an Article using an LLM**
    *   **4.3.1. User Vibe Prompt to AI:**
        *   *Example:* "AI, I want `crawl4ai` to read this article: `https://example.com/news/ai-breakthrough`. Use `openai/gpt-4o-mini` to extract the author's name, the publication date, and a short (2-3 sentence) summary. The output should be JSON. My OpenAI API key is in the `OPENAI_API_KEY` environment variable."
    *   **4.3.2. Key `crawl4ai` elements AI should use (and why):**
        *   **`AsyncWebCrawler`**, **`arun`**.
        *   **`CrawlerRunConfig`** with **`LLMExtractionStrategy`**.
        *   **`LLMConfig`**: To specify the `provider` ("openai/gpt-4o-mini") and `api_token` ("env:OPENAI_API_KEY").
            *   **Reasoning:** The task requires understanding and summarization, making `LLMExtractionStrategy` suitable. The AI needs to construct a schema (either a simple dictionary or a Pydantic model `model_json_schema()`) that tells the LLM what fields to populate. The instruction to the LLM will be implicitly derived from the schema field descriptions or can be explicitly provided.
    *   **4.3.3. Expected Output from AI-generated code:**
        *   Python script that defines a Pydantic model (or a dictionary schema).
        *   Initializes `LLMConfig` and `LLMExtractionStrategy`.
        *   Parses `result.extracted_content`.
        *   **Code Example (using Pydantic):**
            ```python
            import asyncio
            import json
            import os
            from pydantic import BaseModel, Field
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig
            from crawl4ai.extraction_strategy import LLMExtractionStrategy

            class ArticleDetails(BaseModel):
                author_name: str = Field(..., description="The main author of the article.")
                publication_date: str = Field(..., description="The date the article was published (e.g., YYYY-MM-DD).")
                summary: str = Field(..., description="A concise 2-3 sentence summary of the article.")

            async def extract_article_info_llm(url_to_crawl):
                if not os.getenv("OPENAI_API_KEY"): # Or your specific key variable
                    print("API key environment variable not set. Skipping LLM extraction.")
                    return

                llm_config = LLMConfig(
                    provider="openai/gpt-4o-mini", # Use a cost-effective model for demos
                    api_token="env:OPENAI_API_KEY"
                )
                extraction_strategy = LLMExtractionStrategy(
                    llm_config=llm_config,
                    schema=ArticleDetails.model_json_schema(),
                    extraction_type="schema" # Crucial for Pydantic/JSON schema
                )
                run_config = CrawlerRunConfig(
                    extraction_strategy=extraction_strategy,
                    cache_mode="BYPASS"
                )
                async with AsyncWebCrawler() as crawler:
                    result = await crawler.arun(url=url_to_crawl, config=run_config)
                    if result.success and result.extracted_content:
                        try:
                            article_data = ArticleDetails.model_validate_json(result.extracted_content)
                            print(f"Extracted Article Info for {url_to_crawl}:")
                            print(json.dumps(article_data.model_dump(), indent=2))
                        except Exception as e:
                            print(f"Error parsing LLM output: {e}")
                            print(f"Raw LLM output: {result.extracted_content}")
                    else:
                        print(f"Failed to extract article info from {url_to_crawl}: {result.error_message}")

            # asyncio.run(extract_article_info_llm("https://www.example.com/news/ai-breakthrough")) # Replace with real article
            ```

*   **4.4. Recipe 4: Crawl the first 3 pages of a blog (clicking "Next Page")**
    *   **4.4.1. User Vibe Prompt to AI:**
        *   *Example:* "AI, can you use `crawl4ai` to get the Markdown from the first 3 pages of `https://myblog.example.com/archive`? To get to the next page, I think you need to click a link that says 'Older Posts'."
    *   **4.4.2. Key `crawl4ai` elements AI should use (and why):**
        *   **`AsyncWebCrawler`**.
        *   **Multiple `arun` calls** in a loop (3 iterations).
        *   **`CrawlerRunConfig`** with:
            *   `session_id="blog_session"`: **Crucial** for maintaining the browser state (cookies, current page) across the multiple clicks.
            *   `js_code`: JavaScript to find and click the "Older Posts" link. The AI might need to generate robust JS like:
                `Array.from(document.querySelectorAll('a')).find(a => a.textContent.trim() === 'Older Posts')?.click();`
            *   `wait_for`: After clicking, wait for a condition that indicates the next page has loaded (e.g., a specific element on the new page, or a change in an existing element). This can be tricky and might require some iteration. A simple `wait_for` for a few seconds could also be a starting point, like `wait_for=3000` (milliseconds).
            *   `js_only=True`: For the second and third `arun` calls, after the initial page load. This tells `crawl4ai` to only execute the JS and not perform a full new navigation to the original URL.
    *   **4.4.3. Expected Output from AI-generated code:**
        *   A Python script with a loop that calls `arun` three times.
        *   The script should collect and potentially print or save the Markdown from each page.
        *   **Code Example:**
            ```python
            import asyncio
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode

            async def crawl_blog_pages(start_url, num_pages=3):
                session_id = "my_blog_crawl_session"
                all_markdowns = []

                # JavaScript to find and click "Older Posts" (example)
                js_click_older_posts = """
                (() => {
                    const links = Array.from(document.querySelectorAll('a'));
                    const olderPostsLink = links.find(a => a.textContent.trim().toLowerCase() === 'older posts');
                    if (olderPostsLink) {
                        olderPostsLink.click();
                        return true; // Indicate click was attempted
                    }
                    return false; // Indicate link not found
                })();
                """

                async with AsyncWebCrawler() as crawler:
                    current_url = start_url
                    for i in range(num_pages):
                        print(f"Crawling page {i+1}...")
                        run_config_dict = {
                            "session_id": session_id,
                            "cache_mode": CacheMode.BYPASS,
                            "wait_for": 2000 # Wait 2s for content to potentially load after click
                        }
                        if i > 0: # For subsequent pages, click and don't re-navigate
                            run_config_dict["js_code"] = js_click_older_posts
                            run_config_dict["js_only"] = True
                        
                        run_config = CrawlerRunConfig(**run_config_dict)
                        
                        result = await crawler.arun(url=current_url, config=run_config) # URL is mainly for context in js_only
                        
                        if result.success:
                            print(f"  Page {i+1} ({result.url}) - Markdown length: {len(result.markdown.raw_markdown)}")
                            all_markdowns.append({"url": result.url, "markdown": result.markdown.raw_markdown})
                            if i < num_pages - 1 and i > 0 and not run_config_dict.get("js_code_executed_successfully", True): # Hypothetical flag
                                print(f"  'Older Posts' link might not have been found or clicked on page {i+1}. Stopping.")
                                break
                        else:
                            print(f"  Failed to crawl page {i+1}: {result.error_message}")
                            break
                    
                    # Important: Clean up the session
                    await crawler.crawler_strategy.kill_session(session_id) 
                
                print(f"\nCollected markdown for {len(all_markdowns)} pages.")
                # For demo, print first 100 chars of each
                # for i, md_data in enumerate(all_markdowns):
                #     print(f"\n--- Page {i+1} URL: {md_data['url']} ---")
                #     print(md_data['markdown'][:100] + "...")

            # asyncio.run(crawl_blog_pages("YOUR_BLOG_START_URL_HERE"))
            ```

*   **4.5. Recipe 5: Get Screenshots of a List of URLs**
    *   **4.5.1. User Vibe Prompt to AI:**
        *   *Example:* "AI, use `crawl4ai` to take a screenshot of each of these pages: `https://example.com`, `https://crawl4ai.com`, `https://github.com`. Save them as `example_com.png`, `crawl4ai_com.png`, and `github_com.png`."
    *   **4.5.2. Key `crawl4ai` elements AI should use (and why):**
        *   **`AsyncWebCrawler`**.
        *   **`arun_many`**: Efficient for processing a list of URLs.
        *   **`CrawlerRunConfig`** with `screenshot=True`.
            *   **Reasoning:** `arun_many` will process each URL with the same config. The AI needs to add logic to iterate through the results and save each `result.screenshot` (which is base64 data) to a uniquely named file.
    *   **4.5.3. Expected Output from AI-generated code:**
        *   Python script.
        *   PNG files saved to the current directory or a specified output directory.
        *   **Code Example:**
            ```python
            import asyncio
            import base64
            import os
            from urllib.parse import urlparse
            from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode

            async def take_screenshots(urls_to_screenshot):
                run_config = CrawlerRunConfig(
                    screenshot=True,
                    cache_mode=CacheMode.BYPASS # Get fresh screenshots
                )
                output_dir = "screenshots_output"
                os.makedirs(output_dir, exist_ok=True)

                async with AsyncWebCrawler() as crawler:
                    results = await crawler.arun_many(urls=urls_to_screenshot, config=run_config)
                    
                    for result in results:
                        if result.success and result.screenshot:
                            # Create a filename from the URL
                            parsed_url = urlparse(result.url)
                            filename = "".join(c if c.isalnum() else '_' for c in parsed_url.netloc + parsed_url.path)
                            if not filename or filename == "_": # Handle root path or empty paths
                                filename = "homepage"
                            filepath = os.path.join(output_dir, f"{filename}.png")
                            
                            try:
                                screenshot_data = base64.b64decode(result.screenshot)
                                with open(filepath, "wb") as f:
                                    f.write(screenshot_data)
                                print(f"Screenshot saved to {filepath}")
                            except Exception as e:
                                print(f"Error saving screenshot for {result.url}: {e}")
                        elif not result.success:
                            print(f"Failed to crawl {result.url}: {result.error_message}")
                        elif not result.screenshot:
                            print(f"Crawled {result.url} but no screenshot data was returned.")
            
            # urls = ["https://example.com", "https://crawl4ai.com", "https://github.com"]
            # asyncio.run(take_screenshots(urls))
            ```

## 5. Tips for Effective Prompting Your AI Assistant for Crawl4AI Tasks

To get the best code from your AI assistant when working with `crawl4ai`, consider these prompting tips:

*   **5.1. Be Clear About Your Goal:**
    *   Start with a high-level objective. Instead of just "Crawl a page," say "I need to extract all article titles from the homepage of this news site," or "Get the main content of this blog post as clean Markdown," or "Take full-page screenshots of these product pages." This helps the AI choose the right strategies and configurations.

*   **5.2. Always Provide the URL(s):**
    *   This seems obvious, but be precise. If it's a list, provide the list.
    *   Remember to use the `file:///` prefix for local files (e.g., `file:///Users/me/Documents/mypage.html`) and `raw:` for inline HTML (e.g., `raw:<html><body>...</body></html>`). The AI might not always infer this correctly without a hint.

*   **5.3. Describe Data for Extraction (Especially for `JsonCssExtractionStrategy` or `LLMExtractionStrategy`):**
    *   **What you want:** List the specific pieces of information you need (e.g., "product name," "price," "author," "publication_date," "article summary").
    *   **Where to find it (for CSS/XPath):** If you have an idea of the HTML structure, share it. "Product names seem to be in `<h2>` tags with class `item-title`." "The price is always in a `<span>` element right after a `<strong>` tag that says 'Price:'." This helps the AI generate accurate CSS selectors or XPath expressions for `JsonCssExtractionStrategy`.
    *   **Desired structure (for LLM):** For `LLMExtractionStrategy`, tell the AI the desired JSON structure. "I want a list of objects, where each object has a 'title' and a 'link'." Or even better, "Can you define a Pydantic model for me that has 'title' as a string and 'link' as a string, and then use that for extraction?"

*   **5.4. Specify LLM Details for LLM Extraction or Filtering:**
    *   **Model/Provider:** "Use `openai/gpt-4o-mini` for this extraction." or "I want to use my local Ollama model, `ollama/llama3`."
    *   **API Key:** Clearly state where the API key should come from. "My API key is in the environment variable `OPENAI_API_KEY`." (This is safer than putting the key directly in the prompt). If you must provide it directly, be aware of the security implications.

*   **5.5. Mention Page Dynamics and Interactions:**
    *   "This page loads more items when you scroll down."
    *   "You need to click the 'View All Reviews' button to see all the reviews."
    *   "The data I want only appears after selecting 'Category X' from a dropdown."
    *   This signals to the AI that `js_code`, `wait_for`, and possibly `session_id` will be necessary. You might need to guide it on *how* to identify the elements to interact with (e.g., "The 'Load More' button has the ID `load-more-btn`").

*   **5.6. Iterative Refinement is Key:**
    *   Your first prompt might not yield perfect code. That's okay!
    *   Treat it as a conversation. If the AI-generated code misses something or makes a mistake:
        *   "That was close, but it missed extracting the product ratings. Ratings seem to be in a `div` with class `star-rating` inside each product item."
        *   "The script timed out. Can we increase the `page_timeout` in `CrawlerRunConfig` to 90 seconds?"
        *   "It didn't click the 'Next' button correctly. The button actually has the text '>>' instead of 'Next Page'."
    *   Provide the error messages or incorrect output back to the AI for context.

## 6. What to Expect as Output (From AI-Generated Code)

When you use "Vibe Coding" with an AI assistant for `crawl4ai`, you should generally expect the following:

*   **6.1. Python Code:**
    *   The primary output will be a Python script that uses the `crawl4ai` library.
    *   It should include necessary imports like `asyncio`, `AsyncWebCrawler`, `CrawlerRunConfig`, etc.
    *   It will typically define an `async def main():` function and run it with `asyncio.run(main())`.

*   **6.2. Accessing the `CrawlResult`:**
    *   The core of the script will involve one or more calls to `crawler.arun(...)` or `crawler.arun_many(...)`.
    *   These calls return `CrawlResult` objects (or a list of them for `arun_many`).
    *   The AI-generated code should then show you how to access the specific data you asked for from these `CrawlResult` objects. For example:
        *   `print(result.markdown.raw_markdown)` or `print(result.markdown.fit_markdown)`
        *   `data = json.loads(result.extracted_content)`
        *   `screenshot_data = base64.b64decode(result.screenshot)`
        *   `if not result.success: print(result.error_message)`

*   **6.3. Files Saved to Disk (if requested):**
    *   If your vibe prompt included saving data (e.g., "save the screenshots as PNG files," "write the extracted JSON to `output.json`"), the AI-generated code should include the Python logic to perform these file operations.
    *   **Example for saving a screenshot:**
        ```python
        import base64
        # ... inside your async function, after getting 'result' ...
        if result.success and result.screenshot:
            with open("myscreenshot.png", "wb") as f:
                f.write(base64.b64decode(result.screenshot))
            print("Screenshot saved to myscreenshot.png")
        ```

## 7. Conclusion: Vibe Your Way to Web Data!

*   **7.1. Recap of "Vibe Coding" Benefits with `crawl4ai`:**
    "Vibe Coding" empowers you to leverage the full capabilities of `crawl4ai` without needing to memorize every API detail. By understanding the high-level concepts and key building blocks outlined in this guide, you can effectively communicate your data extraction and web interaction needs to an AI coding assistant. This leads to faster prototyping, easier access to web data for non-programmers, and a more intuitive way to build data-driven applications.

*   **7.2. Encouragement to experiment with different prompts and `crawl4ai` features:**
    The key to successful "Vibe Coding" is experimentation. Try different ways of describing your goals to your AI assistant. If the first attempt doesn't yield the perfect `crawl4ai` code, refine your prompt with more specific details or hints. Don't be afraid to mention `crawl4ai` specific terms like `CrawlerRunConfig`, `js_code`, or `LLMExtractionStrategy`  this guide has equipped you with the essential vocabulary. The more context you provide, the better the AI can assist you.

*   **7.3. Pointers to more detailed `crawl4ai` documentation for users who want to learn direct coding or advanced configurations:**
    While "Vibe Coding" is a great way to get started and be productive quickly, you might eventually want to dive deeper into `crawl4ai`'s capabilities or fine-tune the generated code yourself. For that, refer to:
    *   **The Official Crawl4AI API Reference:** (Assuming this exists or will exist - replace with actual link if available, e.g., `https://docs.crawl4ai.com/api/`) For detailed information on all classes, methods, and parameters.
    *   **Specific "Reasoning & Problem-Solving" Guides:** Check the `crawl4ai` documentation for other guides that delve into specific components like advanced `CrawlerRunConfig` options, deep crawling strategies, or custom extraction techniques.

Happy Vibe Coding, and may your web data adventures be fruitful!
```

---

